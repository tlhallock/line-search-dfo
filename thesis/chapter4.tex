\sbnote{Questions:
\begin{enumerate}
\item The Ideal Solution section doesn't actually spell out an algorithm for finding the maximum volume ellipsoid within $\capcones \cap \outertrkpo$.   You really only describe how to check whether an ellipsoid is contained within a cone.    So I think it would be more honest (and actually would be a more sensible flow of logic) to rename the section as ``Checking if an ellipsoid is contained in a cone''.
\item Can we change $\sampletrk$ to $T^{(k)}_{\text{sample}}$?  Whenever we talk about it, we always describe it as the sample trust region rather than the interpolation trust region, so it would be nice if the notation was consistent with that.
\end{enumerate}
}
%
\sbnote{To Do: 
\begin{enumerate}
\item Change $B_{\infty}(x^{(k)},\Delta_k)$ to $\outertrk$ throughout chapter 4.  Also make sure that $\outertrk$ and $\outertrkpo$ are being used correctly.
\item Clean up the alignment of equations.  
\end{enumerate}
}
\label{infeasible_point_strategies}
This chapter considers the problem
\begin{align}
\begin{array}{ccl} \min_{x \in \Rn} & f(x) \\
\mbox{subject to} \quad & c_i(x) \le 0, & 1 \le i \le m,
\end{array}
\label{nonlinear_dfo_problem}
\end{align}
where $f$ and $c_i$, $i=1,\ldots,m$ are real-valued, black-box functions on $\Rn$.   We assume $f$ is twice continuously differentiable and each $c_i$ is continuously differentiable.   Denote the feasible region for this problem by
\begin{align}\label{define_feasible}
\feasible = \{x \in \Rn | c_i(x) \le 0, \; \forall 1 \le i \le m \}. 
\end{align}

To define a trust-region method for solving this problem,  we approximate $f$ at iteration $k$ by a quadratic interpolation model $\mfk$ and each constraint function $c_i,  i=1,\ldots,m$ by a linear interpolation model $\mcik$.     

%
%Specifically,  these functions have the form
%\sbnote{You might prefer other notation in the following:}
%\begin{align} \mfk(x) & = f(\xk)+(g_f^{(k)})^T(x-\xk) + \frac{1}{2}(x-\xk)^T H_f (x-\xk) \\
%\mcik(x) & = c_i(\xk) + (g_{c_i}^{(k)})^T(x-\xk), \quad i=1,\ldots, m \label{mcik_definition}
%\end{align}


Using these model functions, we can define the {\em model feasible region} by
\begin{align}
\feasiblek = \left\{x \in \Rn \big| \mcik(x) \le 0, \; \forall 1 \le i \le m \right\}. \label{define_feasiblek}
\end{align}
The model functions $\mfk$ and $\mcik$ are constructed by interpolating (as described in \cref{interpolation}) over a sample set as 
that includes $\xk$.   All of the sample points,  with the possible exception of $\xk$, are required to lie within a  {\em sample trust region} $\sampletrk$, which is defined prior to the start of the $k$th iteration.  

Ideally,  we would like $\sampletrk$  to lie within $\feasible \cap \outertrk$, where $\outertrk := B_\infty(\xk,\Delta_k)$,  is the outer trust region, and $\Delta_k>0$ is the trust region radius at iteration $k$.     However, since we do not know $\feasible$,  or even $\feasiblek$, when choosing the sample points for iteration $k$,  we can ensure only that $\sampletrk \subset \mathcal{F}^{(k-1)}_m \cap \outertrk$, where we define $\mathcal{F}^{(0)} := \Rn$ for completeness.

As discussed in \cref{chap:background}, we need 
 $p+1 = \frac{(n+1)(n+2)}{2}$ sample points in order to uniquely determine the quadratic model of the objective function, $\mfk$.      For the constraints, we first use the same sample points to fit quadratic models $q_{c_i}^{(k)}$ of the constraint functions and then define  linear models $\mcik$ so that $\mcik(\xk)=f(\xk)$ and $\nabla \mcik(\xk) = \nabla q_{c_i}^{(k)}(x^k)$.   
 
The trust region subproblems have the form
\begin{align*} \label{trust_problem_nlp}
\begin{array}{ccl} \min_{s}
 & \mfk \left(\xk+s\right) \\
 & \xk+s \in \searchtrk,
 \end{array}
%\mbox{subject to} \quad & \mcik\left(\xk + s\right) \le 0 & 1 \le i \le m, \\
%& \|s\|_\infty \le \Delta_k
%\end{array}
\end{align*}
where $\searchtrk$ is a {\em search trust region} constructed so that 
\[ \searchtrk \subset \feasiblek \cap \outertrk.\] 

Developing an always-feasible algorithm for \cref{nonlinear_dfo_problem}  is complicated by the fact that we do not know $\feasible$ explicitly,  so we have to work with models $\feasiblek$ of the feasible region instead.   Because of errors in these models,  we cannot guarantee that we never attempt to evaluate the black-box functions at infeasible points.  Instead, our goal is to make such infeasible evaluations rare.    Toward that end,  we propose strategies for constructing $\sampletrk$ and $\searchtrk$ with the property that,  for sufficiently small $\Delta_k$,  both of these trust regions will be guaranteed to lie within  the feasible region.   Thus,  under reasonable assumptions, the algorithm will attempt infeasible function evaluations only finitely many times.

%Errors in these models mean that we may attempt to evaluate the black-box functions at  a point that is not feasible in two ways:
%\begin{itemize}
%\item A sample point chosen by the \replace{geometry ensuring }{model improvement } algorithm may be infeasible.
%\item The trial point solving the trust region subproblem may be infeasible.
%\end{itemize}



%We use the same algorithmic framework as was presented in \cref{chap:linear}.    As we did there, we define three trust regions:   the outer trust region $\outertrk = B_2(\xk,\Delta_k)$, the sample trust region $\sampletrk$ from which the sample points are selected, and the search trust region $\searchtrk$, which defines the feasible region for the trust region subproblem.    
%
%Ideally,  we would like $\sampletrk$ and $\searchtrk$ to lie within $\outertrk \cap \feasible$.    However, since we do not know $\feasible$, we can ensure only that $\sampletrk$ and $\searchtrk$ lie within $\outertrk \cap \feasiblek$.   

%
%\begin{boxedcomment}
%Consildate this discussion with the background.
%\end{boxedcomment}

% What remains in throughout \cref{infeasible_point_strategies} is a discussion of various strategies
% for handling infeasible sample points.

Our strategies for constructing $\sampletrk$ and $\searchtrk$ make use of second order buffering cones defined for each nearly active constraint.    These cones are constructed in such a way that for sufficiently small $\Delta_k$, the intersection of these cones with $\outertrk$ is guaranteed to be contained in the feasible region.  Thus, by requiring $\sampletrk$ and $\searchtrk$ to lie within this intersection, we ensure that all sample points and trial points are feasible for small $\Delta_k$.



%These trust regions are a box-shaped outer trust region $\outertrk$,  an ellipsoidal inner trust-region $\sampletrk$ from which the sample points are chosen, 
%and a search trust-region, $\searchtrk$, which constrains the search for the next iterate.
%
%Note that the outer trust region may include infeasible points.
%To ensure feasibility of all sample points, we construct an inner trust region  $ \sampletrk $  satisfying 
%$\sampletrk \subset \outertrk \cap \feasiblek$.
%Since $\feasiblek$ is only an approximation of $\feasible$,  we place additional restrictions on $\sampletrk$
%to ensure that it is entirely contained within the feasible region $\feasible$.
%These restrictions will be described in \cref{ellipsoid_requirements}, 
%and we show that these restrictions ensure $\sampletrk \subset \feasible$ in \cref{ellipsoid_is_feasible} and \cref{searchtrk_is_feasible}.

%\cref{ellipsoid_is_feasible}

% However, we do not want to limit the search for a new iterate to the same trust region we use to construct the model.
%The third trust region $\searchtrk$ is the search trust region, which could either be $\sampletrk$ itself, 
%or a larger region satisfying $ \sampletrk \subset \searchtrk \subset \outertrk \cap \feasible$.
%The search trust region is used in defining the trust region subproblem.

% \replace{
% Within our algorithm, if $ \outertrk \subseteq \feasible$ we can set $ \sampletrk $ to be a sphere of radius $\dk$.
% This saves the computation of $ \sampletrk $ when it is not needed, as there are no nearby constraints.}{}  \sbnote{I don't think this needs to be said here}

% In the last paper, we showed convergence of our algorithm for linear constraints.
% We follow the same pattern as in that paper, but we must deal with some problems introduced by general convex constraints.
% To avoid evaluating infeasible points with general convex constraints, we construct models of the constraints in addition to the objective.
The remainder of this chapter is organized as follows:   In the next section we define the concept of a nearly active constraint and define buffering cones associated with each nearly active constraints.  
In \cref{possible_ellipsoids}, we propose several strategies for constructing an ellipsoidal sample trust region that lies with the intersection of the buffering cones and an outer trust region.
\cref{tr_subproblem} describes the trust region subproblem.   
\sbnote{come back to this}.



%What remains in \cref{infeasible_point_strategies} is devoted to search strategies for ensuring feasible sample points:
%\cref{possible_ellipsoids} and \cref{handling_linear_constraints_within_ellipsoid_programs} could be considered background, while the others discuss algorithms.
%The version we call the ``conservative ellipsoid'' is lengthier and discussed within \cref{constructing_and_analysizing_conservative_ellipsoid}.
%Within \cref{convex_model_reduction} we examine two strategies for handling infeasible trial points.
%Finally, in \cref{the_algroithm_section}, we state the convergent algorithm, which is based on the strategies found in 
%\cref{constructing_and_analysizing_conservative_ellipsoid} and \cref{decreasing_the_trust_region_for_infeasible_trial}.



% We will show feasibility and convergence of the algorithm assuming certain of these 


% Although an approach for avoiding one of these two function evaluations can work for the other in thoery, we stick to one simplified version.

% \subsection{Implemented Ideas not used in this paper}
% 
% 
% \subsubsection{Using higher order models}
% I have implemented an algorithm that uses higher order models to approximate the constraints.
% 
% In this algorithm, we add additional degrees of freedom to the constraints to cut off points that have been evaluated and are infeasible.
% The models are forced to be a given negative value at these points.
% 
% One question for this method is how many constraints to model.
% 
% 
% I implemented Kriging, as this can use as many points as are available.
% However, this requires using some artificial value of the constraints at infeasible points.



\section{Buffering Cones}
\label{buffering_cones}
As discussed above, both the sample trust region and the search trust will be defined based on a set of buffering cones associated with the nearly active constraints.   
  
Let $\zik$ denote the projection of the current iterate $\xk$ onto the hyperplane defined by the $i$-th constraint model.
Specifically, define
% vector of infinities
\begin{align}
\label{define_z}
\zik =
\begin{cases}
\xk &  \text{if } c_i\left(\xk\right) = 0 \\
\xk - \frac{c_i\left(\xk\right)}{\left\|\gmcik\right\|^2} \gmcik&  \text{if } c_i\left(\xk\right) \ne 0, \; \left\|\gmcik\right\| \ne 0 \\
\mathbf{\infty}\cdot e & \text{if } c_i\left(\xk\right) \ne 0,  \;  \left\|\gmcik\right\| = 0 \\
\end{cases}
\end{align}

\begin{boxedcomment}
Note that $m^{(k)}_{c_i}\left(\xk\right) = c_i\left(\xk\right)$.
\end{boxedcomment}

%
%To ensure this ellipsoid is feasible, we construct cones that buffer the ellipsoid from the constraint boundaries.
%These cones have a vertex between the current iterate and the nearest point along the linearization of the constraints, and open towards the current iterate.
%We define these cones within this section.

If $\zik$ lies sufficiently near the boundary of the outer trust region, we say that the $i$-th constraint is \emph{nearly active}.
Specifically, the set of {\em nearly active constraints} is defined by
\begin{align}
\activeconstraintsk := \left\{i \in [m] \bigg| \zik \in B_{\infty}\left(\xk, (1+\zikthresh)\dk\right)\right\}, \label{define_activeconstraints}
\end{align}
where $\zikthresh$ is a user defined parameter that must satisfy the following identity involving $\omegainc$ (which is defined by \cref{define_the_omegas}):  \sbnote{I don't think we want to refer the reader back to chapter 2 here.    It would be better to just say, ``where $\omegainc \ge 1$ is a user-defined parameter.''}
\begin{align}
(2 + \omegainc)\sqrt{n} < \zikthresh. \label{define_zikthresh}
\end{align}
For each nearly active constraint, we define a buffering cone with user defined parameters
\begin{align}
\alpha, \beta, p_{\alpha}, p_{\beta} \in (0, 1). \label{define_abpab}
\end{align}
The vertex $\wik$ of this cone is located along the line segment connecting $\xk$ to $\zik$:
\begin{align}
\wik = \xk + \left(1 - \alpha\dk^{p_{\alpha}}\right)\left(\zik - \xk\right) \forall i \in \activeconstraintsk. \label{define_w}
\end{align}
The cone for constraint $i$ is then defined by
\begin{align}
\fik = \left\{x \in \Rn | x = \wik + t s,t > 0, \|s\| = 1, -s^T\hgik \ge \beta \dk^{p_{\beta}} \right\}. \label{define_fik}
\end{align}
This cone is illustrated in \cref{explanation_2}.  
\begin{figure}[ht]
    \centering
    \includegraphics[width=150px]{images/explanation_2.png}
    \includegraphics[width=150px]{images/explanation_3.png}
    \caption[An example of a buffering cone.]{
    	As the trust region goes to zero the buffering cone better approximates the linearization of the constraint and becomes locally feasible.
    	The current iterate is the green star;
    	the true constraint is black;
    	the vertex, $\wik$, is the red star;
    	the linearization of the constraint boundary is blue;
    	the cone buffering cone $\fik$ is in red.
	}
    \label{explanation_2}
\end{figure}

The intersection of these cones is the set
\begin{align}
\capcones = \{x\in\Rn | \; x \in \fik \quad \forall i \in \activeconstraintsk \} \label{define_capcones}
\end{align}
which is depicted in \cref{completed_2}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=150px]{images/completed_2.png}
    \caption
    	[An example ellipsoidal, buffered sample region.]{
    	After constructing 
    	the linearization of the constraints in black;
    	the buffering cones in blue;
    	the trusted region $\capcones$ in yellow;
    	we seek to find an ellipsoid within $\capcones$ such as the one in red/pink.
	}
    \label{completed_2}
\end{figure}

Observe that, by construction,  if $x \in \capcones$, then it is feasible with respect to all of the nearly active model constraints.  Additionally,  from the definition of $\activeconstraintsk$, any point that is infeasible with respect to a model constraint not in $\activeconstraintsk$ must be too far away from $\xk$ to lie within  $B_\infty(\xk,\Delta_k)$.  Thus, 
$\capcones \cap B_\infty(\xk,\Delta_k) \subset \feasiblek$.     Moreover,   the multiplier $(1+K_a)$ used in the definition of $\activeconstraintsk$ will also allow us to prove that
$\capcones \cap B_\infty(\xkpo,\Delta_{k+1}) \subset \feasiblek$.     \sbnote{verify that this last statement is correct.}



\sbnote{Move the following somewhere else}

In \cref{cone_and_tr_are_feasible}, we show that for sufficiently small $\dk$, $\capcones \cap \outertrkpo \subseteq \feasible$.  \sbnote{Do we also show that $\capcones \cap \outertrk \subseteq \feasible$?}
Therefore, to obtain feasible sample points, we require that 
\begin{align*}
\sampletrkpo \subset \capcones \cap \outertrkpo.
\end{align*}


\sbnote{Maybe we need to be consistent about writing $\outertrk$ versus $B_{\infty}(\xk,\Delta_k)$.}















\section{The Sample Trust Region}
\label{possible_ellipsoids}
The sample trust region $\sampletrk$ is an ellipsoidal region defined by an $n\times n$ symmetric, positive definite matrix $\qk$, center $\ck \in \Rn$, and size determined by a parameter $\delta_{k} > 0$:
\begin{align}
\sampletrk = \left \{x \in \Rn \bigg | \left(x - \ck\right)^T \qk\left(x - \ck\right) \le \frac 1 2 \sdk^2 \right \}. \label{define_sampletrk}
\end{align}
The starting ellipsoid,  $T_{\text{interp}}^{(1)}$,  is specified by the user.  Thereafter,  $\sampletrkpo$ is constructed at the end of iteration $k$ using the cones defined in \cref{buffering_cones}
In particular,  $\qkpo,\ckpo$, and $\delta_{k+1}$ are chosen so that
  $\sampletrkpo  \subset \capcones \cap \outertrkpo$.   To accomplish this, we first need to understand how to verify that an ellipsoid is contained within a cone.   This is discussed in \cref{ellipsoid_in_cone}.    We then present two methods for constructing $\sampletrkpo$.   Finally, we show that the conservative ellipsoid method discussed in \cref{conservative_ellipsoid} satisfies certain properties that will be needed in our convergence proof.
   
%  Our goal is to also ensure that the resulting ellipsoid also satisfies
%certain properties that will be needed in order to prove convergence of our algorithm.  These properties are summarized in the following definitions:
% \begin{definition}
%\label[definition]{ellipsoids_notation_definitions}
%For each $k \in \naturals$, suppose we are given $n\times n$, symmetric, positive-definite matrices $\qk$, vectors $\ck \in \Rn$, scalars $\sdk > 0$.
%We have the following definitions.
%\begin{itemize}
%\item The sequence of tuples $((\qk, \ck, \sdk))$ is \emph{conditioned} if the condition numbers of $\qk$ are bounded for small enough $\dk$.
%That is, there exists a $\sigmamax \ge 1$ and $\dacc > 0$ such that if $\dk \le \dacc$, then
%\begin{align}
%\condition \left(\qk\right) \le \sigmamax. \label{define_suitable_condition_numbers}
%\end{align}
%\item The tuple $(\qkpo, \ckpo, \sdkpo)$ is \emph{trusted} for $\xkpo, \dkpo$ if an ellipsoid defined by $\qkpo, \ckpo, \sdkpo$ lies within $ \capcones \cap \trkpo $.
%That is,
%\begin{align}
%\unshiftedellipsoidkpo \subseteq \capcones \cap \trkpo  \label{define_suitable_in_tr}
%\end{align}
%where
%\begin{align}
%\unshiftedellipsoid = \left\{x \in \Rn | \left(x - \ck \right)^T \qk \left(x - \ck\right) \le \frac 1 2 {\sdk}^2 \right\} \label{define_unshifed_ellipsoid}
%\end{align}
%\item The tuple $(\qk, \ck, \sdk)$ is \emph{feasible} if an ellipsoid defined by $\qk, \ck, \sdk$ lies within the feasible region:
%\begin{align}
%\unshiftedellipsoid \subseteq \feasible.
%\end{align}
%where $\unshiftedellipsoid$ is defined by \cref{define_unshifed_ellipsoid}
%\item The tuple $(\qk, \ck, \sdk)$ is \emph{adjacent} to $\xk$ if an ellipsoid defined by $\qk, \ck, \sdk$ is near to the current iterate:
%\begin{align}
%\xk \in \scaledunshiftedellipsoid \label{define_suitable_close_to_iterate}
%\end{align}
%where
%\begin{align}
%\scaledunshiftedellipsoid = \left\{x \in \Rn | \left(x - \ck\right)^T \qk \left(x - \ck\right) \le {\sdk}^2 \right\} \label{define_scaledunshiftedellipsoid}
%\end{align}
%\item The tuple $(\qk, \ck, \sdk)$ is \emph{non-empty} if
%\begin{align}
%\unshiftedellipsoid \ne \emptyset
%\end{align}
%where $\unshiftedellipsoid$ is defined by \cref{define_unshifted_ellipsoid}.
%\end{itemize}
%\end{definition}
% 


%
%the property that 
%for small enough $\Delta_k$,  $\sampletrk$ is feasible.    





%
%  
%We ensure that sample points are feasible by constructing an inner trust region $\sampletrk$ that we show is feasible for small enough $\dk$.
%\begin{boxedcomment}
%Steve wants me to delete most of this...
%\end{boxedcomment}
%This means that if $\sampletrk$ includes infeasible points-and we attempt to evaluate at an infeasible point- we can reduced the trust region radius.
%Of course, the method of constructing this ellipsoid impacts the algorithm's efficiency because constructing a $\sampletrk$ too small means a poor sample set,
%while using a $\sampletrk$ too large implies reducing the trust region quickly so that only small steps can be made towards a critical point.
%
%There is a well known algorithm for constructing poised sets over spheres, and this algorithm can be easily generalized to an ellipsoid.
%Therefore, we construct an ellipsoidal inner trust region to avoid the constraints.








\subsection{Ideal Solution}
\label{ideal_ellipsoid_in_polyhedron}
\label{ellipsoid_in_cone}

\sbnote{Consider renaming this section to ``checking if an ellipsoid is contained in a cone''}

\sbnote{If you like the name change idea, here is an alternative introductory paragraph:
Ideally, we would like the sample trust region to be the largest possible ellipsoid contained within $\capcones \cap B_{\infty}(x^{(k)},\Delta_k)$.   To accomplish this, we first need to understand how to check whether an ellipsoid is contained in a cone.}



Ideally, we would like the sample trust region to be the largest possible ellipsoid contained within $\capcones \cap B_{\infty}(x^{(k)},\Delta_k)$.   
%Although this sounds appealing, we were only able to formulate this problem as a multilevel optimization problem.    
This problem can be formulated as a multilevel optimization problem.  In particular, we first describe how to check whether a given ellipsoid is contained within any particular cone.    This requires solving a lower-level optimization problem.   We can then search over all possible ellipsoids using the lower level optimization problem as a constraint.

\sbnote{Maybe state the multi-level optimization problem here.}

%  the problem of checking whether an ellipsoid is contained within any particular cone.    This is    We can then use this to search over possible ellipsoids
%However, it is possible to formulate the problem of checking whether an ellipsoid is contained within any particular cone.
%This admits a search over possible ellipsoids, with an algorithmic check that is efficient.  
%We discuss this here.

\sbnote{The following sentence doesn't fit here, but I don't want to loose track of the idea.}
Throughout, we will assume that the constraints found in \cref{ellipsoids_trust_region_constraints} are included, 
to ensure the ellipsoid is within the outer trust region $\outertrk$.


% \begin{align*}
% \max_{\qk, \ck, \sk} \det\left(\qk^{-1}\right) \\
% \end{align*}

Notice that because the cone for each constraint opens toward the current iterate,
and each cone uses the same threshold $\beta \dk^{p_\beta}$ for feasible directions from the vertex,
the cones can be uniquely defined by their vertices.
This means that for an iteration $k$, after a change of variables
\begin{align}
x \gets x - \xk \\
v^{(i)}  \gets \wik - \xk \\
\beta \gets \beta \dk^{p_{\beta}},
\end{align}
each cone in \cref{define_fik} is equivalent to
\begin{align*}
A_i = \fik - \xk = \left\{x\in\Rn\bigg|\;-\frac{(x - v^{(i)})^T}{\|x - v^{(i)}\|} \frac{v^{(i)}}{\|v^{(i)}\|} \ge \beta \right\}.
\end{align*}

\sbnote{I really don't like the notation $A_i$ here, since it is hard to think of $A_i$ as anything other than the $i$th row of the matrix $A$.}

The ellipsoid $\sampletrk$ is contained in each a cone $A_i$ if and only if the \emph{perspective projection} from $v^{(i)}$ of the ellipsoid onto the hyper-plane 
\begin{align*}
H_i = \left\{x \in \Rn \bigg|\left(v^{(i)}\right)^Tx = 0\right\}
\end{align*}
is contained in the projection of the cone.   \sbnote{Do you have a reference to justify this statement? }
The perspective projection onto $v^{(i)}$ is the shape drawn onto a hyperplane as if a viewer were positioned at $v^{(i)}$.
That is, imagine a viewer is positioned at $v^{(j)}$, and viewing the ellipsoid as in \cref{perspective_projection_depiction}.
They create a line from $v^{(j)}$ to the hyperplane by looking past the outer edge of the ellipsoid onto the hyper plane.
If they were to trace all possible rays through the boundary of the ellipsoid onto the hyperplane, 
these rays intersect the hyperplane on the ellipsoid's perspective projection.  \sbnote{This is a nice intuitive description of the perspective projection, but do you have a formal definition?}


\begin{figure}[ht]
    \centering
    \includegraphics[width=300px]{images/perspective_projection.png}
    \caption[A depiction of the perspective projection]{
    		The perspective projection is the intersection of the plane and directions from $V$ through the boundary of the ellipsoid.
	}
    \label{perspective_projection_depiction}
\end{figure}

First, we compute the projection of the cone, which is simply its intersection with the hyperplane:
\begin{align*}
H_i \cap A_i 
= \left\{x \in \Rn \bigg| \left(v^{(i)}\right)^Tx = 0, -\frac{\left(x - v^{(i)}\right)^T}{\|x - v^{(i)}\|}\frac{v^{(i)}}{\|v^{(i)}\|}\ge \beta \right\} \\
= \left\{x \in \Rn \bigg| \left(v^{(i)}\right)^Tx = 0, \|v^{(i)}\|\ge \beta\|x - v^{(i)}\| \right\} \\
= \left\{x \in \Rn \bigg| \left(v^{(i)}\right)^Tx = 0, \|x - v^{(i)}\|^2 \le \frac 1 {\beta^2}\|v^{(i)}\|^2 \right\} \\
= \left\{x \in \Rn \bigg| \left(v^{(i)}\right)^Tx = 0, \|x\|^2 \le \left(\frac 1 {\beta^2} - 1\right)\|v^{(i)}\|^2 \right\}.
\end{align*}

Then, we can compute the perspective projection of the ellipsoid,  in fashion similar to \cite{eberly_2013}.
Consider the point $x = v^{(i)} + td$ for some unit direction $d$ and distance $t$ from $v^{(i)}$.
If $x$ is on the boundary of the ellipsoid, then:
\begin{align*}
0 = (v^{(i)} + t d - \ck )^T \qk  (v^{(i)} + t d - \ck ) - 1\\
= (v^{(i)} + t d - \ck )^T \qk  v^{(i)} + t (v^{(i)} + t d - \ck )^T \qk d \\ - (v^{(i)} + t d - \ck )^T \qk \ck - 1\\
= \left(v^{(i)}\right)^T \qk  v^{(i)} + t d^T \qk  v^{(i)} - \ck ^T \qk  v^{(i)} + t \left(v^{(i)}\right)^T \qk  d \\ + t^2 d^T \qk  d - t \ck ^T \qk  d - \left(v^{(i)}\right)^T \qk  \ck  - t d^T \qk  \ck + \ck ^T \qk \ck - 1\\
= \left(d^T\qk d
\right) t^2 + \left(
d^T \qk  v^{(i)} +  \left(v^{(i)}\right)^T \qk  d - \ck ^T \qk  d - d^T \qk  \ck 
\right) t \\ +  \left(
\left(v^{(i)}\right)^T \qk  v^{(i)} - \ck ^T \qk  v^{(i)} - \left(v^{(i)}\right)^T \qk  \ck   + \ck ^T \qk  \ck  - 1
\right) \\
= \left[d^T\qk d
\right] t^2 + 2\left[
\left(v^{(i)}\right)^T \qk  d - \ck ^T\qk d
\right] t + \\ \left[
\left(v^{(i)}\right)^T \qk  v^{(i)} + \ck ^T \qk  \ck  - 2 \ck ^T \qk  v^{(i)} - 1
\right]
\end{align*}

There are either $0$, $1$, or $2$ values of $t$ for which this equation will have solutions.
The directions from $v^{(i)}$ with exactly $1$ intersection with the ellipsoid are those whose projection will be on the boundary of the perspective projection of the ellipsoid.
The discriminant for these is $0$.
If we let
\begin{align*}
M_i = 
\qk (v^{(i)} - \ck )\left[\qk (v^{(i)} - \ck )\right]^T \\
- \qk  \left(\left(v^{(i)}\right)^T \qk  v^{(i)} + \ck ^T \qk  \ck  - 2 \ck ^T \qk  v^{(i)} - 1\right),
\end{align*}
we find that that these directions satisfy
\begin{align*}
4\left(\left(v^{(i)}\right)^T \qk  d - \ck ^T\qk d\right)^2 \\
- 4 \left(d^T\qk d\right) \left(\left(v^{(i)}\right)^T \qk  v^{(i)} + \ck ^T \qk  \ck  - 2 \ck ^T \qk  v^{(i)} - 1\right) \\
= d^TM_id = 0.
\end{align*}

% This used to be a line, but it didn't fit...
% d^T\left[
% \left(\qk (v^{(i)} - \ck )\right)\left(\qk (v^{(i)} - \ck )\right)^T
% - \qk  \left(\left(v^{(i)}\right)^T \qk  v^{(i)} + \ck ^T \qk  \ck  - 2 \ck ^T \qk  v^{(i)} - 1\right)\right]d = 0 \\

Next, let
\begin{align*}
t = -\frac {\left(v^{(i)}\right)^T v^{(i)}}{\left(v^{(i)}\right)^T d } \Longrightarrow
0 = \left(v^{(i)}\right)^T v^{(i)} + t {v^{(i)}}^T d \Longrightarrow
0 = \left(v^{(i)}\right)^T \left(v^{(i)} + t d\right)
\end{align*}
so that $x \in H_i$.
We already know that
\begin{align*}
x = v^{(i)} + t d \Longrightarrow
d = \frac 1 t \left(x - v^{(i)}\right)
\Longrightarrow d^TM_id = \frac 1 {t^2} \left(x - v^{(i)}\right)^TM_i\left(x - v^{(i)}\right) = 0
\end{align*}
so the perspective projection of the boundary of the ellipsoid on the hyper-plane $H_i$ is described by
\begin{align*}
\{x \in \Rn | {v^{(i)}}^Tx = 0, \left(x - v^{(i)}\right)^TM_i\left(x - v^{(i)}\right) = 0\}
\end{align*}

Thus, the ellipsoid is contained in cone $A_i$ if and only if
\begin{align*}
\{x \in \Rn | {v^{(i)}}^Tx = 0, \left(x - v^{(i)}\right)^TM\left(x - v^{(i)}\right) = 0\} \\
\subseteq \{x \in \Rn | {v^{(i)}}^Tx = 0, \|x - v^{(i)}\|^2 \le \frac 1 {\beta_i^2}\|v^{(i)}\|^2 \} \\
= \{x \in \Rn | {v^{(i)}}^Tx = 0, \|x\|^2 \le \left(\frac 1 {\beta_i^2} - 1\right)\|v^{(i)}\|^2 \}.
\end{align*}

For each $i \in [m]$, define
\begin{align*}
\hat v^{(i)} & = \frac{v^{(i)}}{\|v^{(i)}\|} \\
R^i & = 2\frac{(\hat v^{(i)} + e_1)(\hat v^{(i)} + e_1)^T}{(\hat v^{(i)} + e_1)^T(\hat v^{(i)} + e_1)} - I
\end{align*}
so that
\begin{align*}
& {R^i}v^{(i)} &= \|v^{(i)}\|e_1 & {R^i}^T{R^i} = I & \quad \det({R^i}) \in \{-1, 1\}
\end{align*}
and let $W_i$, $y$, $w_{1,1}$, $w_1$ be defined so that
\[
M_i = {R^i}^T M_i' {R^i} = {R^i}^T\left( \begin{array}{cc}
{w_{1,1}^i} & {w_1^i} \\
{w_1^i}^T	& {W_i}  \\
\end{array} \right){R^i},  \; \text{ and }
{R^i}x = \left(\begin{array}{c}
0 \\
y
\end{array}\right)
\]
for all $x$ with $x^Tv = 0$.
Then
\begin{align*}
0 &= \left(x - {v^{(i)}}\right)^TM_i\left(x - {v^{(i)}}\right) \\
&= x^TM_ix - 2x^TM_i{v^{(i)}} + {v^{(i)}}^TM_i{v^{(i)}} \\
&= x^T{R^i}^TM_i'{R^i}x - 2x^T{R^i}^TM_i'{R^i}{v^{(i)}} + {v^{(i)}}^T{R^i}^TM_i'{R^i}{v^{(i)}} \\
&= y^T{W_i}y - 2\|v^{(i)}\|y^Tw_1^i + {w_{1,1}^i}\|v^{(i)}\|^2 \\
\Longleftrightarrow \quad & y^T{W_i}y - 2\|v^{(i)}\|y^T{W_i}{W_i}^{-1}{w_1^i} + \|v^{(i)}\|^2{{w_1^i}}^T{W_i}^{-1}{W_i}{W_i}^{-1}{w_1^i} \\
&= - \|v^{(i)}\|^2{w_{1,1}^i} + \|v^{(i)}\|^2{{w_1^i}}^T{W_i}^{-1}{W_i}{W_i}^{-1}{w_1^i} \\
\Longleftrightarrow \quad & \left(y - \|v^{(i)}\|{W_i}^{-1}{w_1^i}\right)^T{W_i}\left(y - \|v^{(i)}\|{W_i}^{-1}{w_1^i}\right) \\
&= - \|v^{(i)}\|^2{w_{1,1}^i} + \|v^{(i)}\|^2{{{w_1^i}}}^T{W_i}^{-1}{{w_1^i}}. 
\end{align*}
Thus, to find the maximum norm $x$ with $\left(v^{(i)}\right)^Tx = 0$, we wish to compute:
\begin{align*}
\max_{y}  \quad \|y\|^2 &  \\
 \st \quad & \left(y - \|v^{(i)}\|{W_i}^{-1}{w_1^i}\right)^T{W_i}\left(y - \|v^{(i)}\|{W_i}^{-1}{w_1^i}\right) = - \|v^{(i)}\|^2{w_{1,1}^i} + \|v^{(i)}\|^2{{w_1^i}}^T{W_i}^{-1}{w_1^i}.
\end{align*}

After a change of variables
\begin{align*}
w_c \gets \|v^{(i)}\|{W_i}^{-1}w_1^i \\
w_r \gets  - \|v^{(i)}\|^2{w_{1,1}^i} + \|v^{(i)}\|{{w_1^i}}^Tw_c \\
s \gets y - w_c
\end{align*}
this becomes:

\begin{align}
\label{cone_feasibility_check}
\begin{array}{ccc}
\max_{y} & \quad \|s - \left(-w_c\right)\|^2  \\
 & s^T\left(\frac {W_i}{w_r}\right)s = 1
 \end{array}
\end{align}

% This is the well known optimization problem of projecting a point onto the surface of an ellipsoid.
To our knowledge, no explicit solution exists, 
which leaves us to believe that there is no explicit formulation of finding the maximum volume ellipsoid contained within the intersection of these cones.  
However, there is an efficient, binary-search algorithm to solve this.
% \cite{projecttoellipsoid}.


Namely, given 
$p \in \Rn$ and a matrix $Q$,
we can consider how to compute  the point farthest from $p$ that is also in the set $\left\{x \in \Rn \bigg | x^TQx = 1 \right\}$:
\begin{align}
\begin{array}{ccc}
x^{\star} = &\argmax_{x \in \Rn} & \|x - p\|^2 \\
& \textrm{s.t.} & x^TQx = 1.
\end{array} \label{antiprojection_problem}
\end{align}
% If $p^TQp \le 1$, then clearly $x^{\star} = p$.
The first order optimality conditions imply the existence of a $\lambda \in \reals$ such that
% x^{\star} - p = \lambda Qx^{\star} \Longleftrightarrow \left(Q - \frac 1 {\lambda} I\right)x^{\star} = -\frac 1 {\lambda}p \\
\begin{align*}
x^{\star} - p = \lambda Qx^{\star} %\Longleftrightarrow \left(\lambda Q - I\right)x^{\star} = -p
\Longleftrightarrow x^{\star} = -\left(\lambda Q - I\right)^{-1}p
\end{align*}
provided the inverse exists.
Using $\left(x^{\star}\right)^TQx^{\star} = 1$, the problem reduces to finding zeros of the function $f : \reals_+ \to \reals$ defined by
\begin{align*}
f(\lambda) = p^T\left(\lambda Q - I\right)^{-1}Q\left(\lambda Q - I\right)^{-1}p - 1.
\end{align*}
Notice 
\begin{itemize}
\item $f(0) = p^TQp - 1 > 0$
\item $\lim_{\lambda \to \frac 1 {\lambda_i}} f(\lambda) = \infty$ for any eigenvalue $\lambda_i$
\item the presence of $\lambda$ within inverse expressions suggests $\lim_{\lambda \to \pm \infty}f(\lambda) = -1$
\end{itemize}
A binary search on $\lambda$ can find the zeros of $f$.


\begin{figure}[ht]
    \centering
    \includegraphics[width=300px]{images/antiprojection.png}
    \caption[
    		An example plot of the one-dimensional function whose zeros provide the largest ellipsoid within the buffering cones.
		]{
    		By computing the zeros of this function, we find values of $\lambda$ for each critical point of \cref{antiprojection_problem}.
    		The function $f$ is in blue, and the eigenvalues of $Q$ are red vertical lines.
    		A binary seach can be used to find one zero (plotted as green vertical lines) to the left of the smallest eigenvalue and one to the right of the largest.
    		Between any two eigenvalues, we first find the minimum.
    		If the minimum is negative, we can then use a binary search to compute two more zeros.
	}
    \label{feasible_direction}
\end{figure}





% \subsubsection{Numerical Solution}
% 
% Although the algorithm presented in \cref{ideal_ellipsoid_in_polyhedron} yields a solution that cannot be solved explicitly, it is easy to check if an ellipsoid is feasible using \cref{cone_feasibility_check}.
% One very simple random-search algorithm can be used to show this.
% 
% \begin{boxedcomment}
% Dramatically simplify this algorithm...
% \end{boxedcomment}
% 
% \begin{algorithm}[H]
%     \caption{Search for feasible ellipsoid}
%     \label{numerical_ellipsoid_algorithm}
%     \begin{itemize}
%         \item[\textbf{Step 0}] \textbf{(Initialization)} \\
%                 Set $E_{max}$ to any feasible ellipsoid, 
%                 an initial sample variance $\sigma^2$, 
%                 a threshold $M$ of iterations per sample variance, 
%                 a decrease ratio $\gamma \in (0, 1)$, 
%                 a variance tolerance $\delta_{\sigma} > 0$
%         
%         \item[\textbf{Step 1}] \textbf{(Random Perturbation)} \\
%             Evaluate the next iterate \begin{itemize}
%                 \item[] Sample a rotation matrix $R$ with variance $\sigma^2$, $\qk \gets R \qk$
%                 \item[] Sample a positive diagonal matrix $D$ with variance $\sigma^2$, $\qk \gets D \qk$
%                 \item[] Sample a translation $c$ bounded by with variance $\sigma^2$, $\ck \gets c + \ck$
%             \end{itemize}
%         
%         \item[\textbf{Step 2}] \textbf{(Check Feasibility)} \\
%             Run feasibility check \cref{cone_feasibility_check} for each constraint.
%             If the new ellipsoid is feasible and larger than $E_{max}$, then 
%             	set $E_{max}$ to this ellipsoid,
%             	set counter $j \gets 0$, and
%             	go to Step 1
%         
%         \item[\textbf{Step 3}] \textbf{(Decrease Sample Variance)} \\
%             If the counter $j \ge M$, then
% 	    	decrease $\sigma^2 \gets \gamma \sigma^2$,
% 	    	set the counter $j\gets 0$, and
% 	    	go to Step 1
%             
%         \item[\textbf{Step 4}] \textbf{(Check for Convergence)} \\
% 	    If $\sigma^2 < \delta_{\sigma}$ \textbf{Return} $E_{max}$.
% 	    Otherwise, 
%         		set counter $j \gets j + 1$, and
%         		go to Step 1
%     \end{itemize}
% \end{algorithm}
% 
% Although not efficient, this can be used to find a large ellipsoid within the buffering cones.


\subsection{Spherical Solution}

One simplification that allows an explicit formulation is to restrict the ellipsoid to be a sphere.
Although this may produce an ellipsoid with less volume, it may be used as a hot start while searching over all ellipsoids.

To compute the maximum volume sphere, we can maximize the minimum distance from the center of the sphere to any cone.
We once again perform a change of variables, this time subtracting off $\xk$ and rotating $v^{(i)}$ onto the unit vector $e_1$.
This reduces the problem of computing the projection of an arbitrary point $(t, y) \in \Rn$ and the second order cone: $\left\{ x \in \mathbb R^n | \quad x^Te_1 = \beta \|x\| \right\}$ for some $\beta \in \reals$.  \sbnote{The preceding sentence seems to be missing something.   What does it reduce the problem to?}
We find
\begin{align*}
 \\
x^Te_1 = \beta \|x\| 
 \Longleftrightarrow t^2 = \beta^2 \|te_1  + x - t e_1\|^2 
  = \beta^2 \left(\|te_1\|^2  + \|x - t e_1\|^2\right) \\
= \beta^2 \left(t^2  + \|y\|^2 \right) 
 \Longleftrightarrow (1 - \beta^2)t^2 = \beta^2 \|y\|^2 
 \Longleftrightarrow \sqrt{\frac{1 - \beta^2}{ \beta^2}} t = \|y\|.
\end{align*}

%  \Longleftrightarrow t^2 = \beta^2 \left(\|te_1\|^2  + \|x - t e_1\|^2\right) \\

This means, that if we let $\beta' = \sqrt{\frac{1 - \beta^2}{\beta^2}}$, we have
\begin{align*}
\left\{ x \in \mathbb R^n | \quad x^Te_1 = \beta \|x\| \right\} = \left \{(s, x)\in \mathbb R^n | \quad\|x\| \le \beta' s \right\}.
\end{align*}

The projection gives the following optimization problem:
\begin{align*}
\min_{x \in \mathbb R^{n-1}, s \in \mathbb R} & \quad \frac 1 2 \|x - y\|^2 + \frac 1 2 (s - t)^2 \\
	\textrm{s.t.}		& \quad \frac 1 2 \|x\|^2 = \frac 1 2 {\beta'}^2 s^2.
\end{align*}

\sbnote{You don't need to define the Lagrangian.  Just state the first order optimality condition.}
%with Lagrangian:
%\begin{align*}
%l(x, s, \lambda) = \frac 1 2 \|x - y \|^2 + \frac 1 2 \left(s - t\right)^2 - \lambda \frac 1 2 \left(\|x\|^2 - {\beta'}^2 s^2\right).
%\end{align*}
%
%After taking the gradient of the Lagrangian, 

The first order optimality conditions for this problem imply that for some $\lambda \in \reals$:
\begin{align*}
x - y - \lambda x = 0, & \quad s - t + \lambda {\beta'}^2 s = 0 \\
x = \frac {y}{1 - \lambda}, & \quad s = \frac {t}{1 + \lambda {\beta'}^2 }.
\end{align*}

We can substitute this into the constraint to solve for $\lambda$:
\begin{align*}
\|x\| = {\beta'} s \\
\left\|\frac {y}{1 - \lambda}\right\| = {\beta'} \frac {t}{1 + \lambda {\beta'}^2 } \\
\left(1 + \lambda {\beta'}^2\right) \left\|y\right\| = {\beta'}  {t} \left|1 - \lambda\right|
\end{align*}

\begin{align*}
\left\|y\right\| + {\beta'}^2\left\|y\right\|\lambda = t {\beta'} - t {\beta'} \lambda          &   \quad
\left\|y\right\| + {\beta'}^2\left\|y\right\|\lambda = t {\beta'} \lambda - t {\beta'}					\\
\left\|y\right\|-t {\beta'} +\left( {\beta'}^2\left\|y\right\| + t {\beta'} \right)\lambda = 0  &		\\
\lambda = \frac{t {\beta'} - \|y\|}{{\beta'}^2\|y\| + t {\beta'}}                               &		
\end{align*}


Substituting $\lambda$ to find $x$ and $s$:
\begin{align*}
x = \frac {y}{1 - \lambda} 																		
= \frac {y}{1 - \frac{t{\beta'} - \|y\|}{{\beta'}^2\|y\| + t{\beta'}}} 									
= \frac {y\left({\beta'}^2\|y\| + t{\beta'}\right)}{{\beta'}^2\|y\| + t{\beta'} - t{\beta'} + \|y\|} 			
= \frac {{\beta'}^2 + \frac{t{\beta'}}{\|y\|}}{1 + {\beta'}^2}y 											
\end{align*}

\begin{align*}
s = \frac {t}{1 + \lambda{\beta'}^2 } 
= \frac {t}{1 +\frac{t{\beta'} - \|y\|}{{\beta'}^2\|y\| + t{\beta'}}{\beta'}^2 } 
= \frac {t\left({\beta'}^2\|y\| + t{\beta'}\right)}{{\beta'}^2\|y\| + t{\beta'} + \left(t{\beta'} - \|y\|\right){\beta'}^2 } 
= \frac {{\beta'}\|y\| + t}{1 + {\beta'}^2 } 
\end{align*}


Thus, the projected point is
\begin{align*}
\left(\frac{{\beta'} \|y\| + t}{1 + {\beta'} ^ 2}, \frac{{\beta'} ^ 2 + t \frac {{\beta'}}{\|y\|}}{1 + {\beta'} ^ 2}y\right)
\end{align*}
with squared distance
\begin{align*}
\left(\frac{{\beta'} \|y\| + t}{1 + {\beta'} ^ 2} - t\right)^2 + \left\|\frac{{\beta'} ^ 2 + t \frac {{\beta'}}{\|y\|}}{1 + {\beta'} ^ 2}y - y\right\|^2 \\
= \left(\frac{{\beta'} \|y\| + t}{1 + {\beta'} ^ 2} - \frac{t + t{\beta'} ^ 2}{1 + {\beta'} ^ 2}\right)^2 + \left(\frac{{\beta'} ^ 2 + t \frac {{\beta'}}{\|y\|}}{1 + {\beta'} ^ 2} - 1\right)^2\left\|y\right\|^2 \\
= \left(\frac{{\beta'} \|y\| - t{\beta'}^2}{1 + {\beta'} ^ 2}\right)^2 + \left(\frac{{\beta'} ^ 2 + t \frac {{\beta'}}{\|y\|} - 1 - {\beta'} ^ 2}{1 + {\beta'} ^ 2}\right)^2\left\|y\right\|^2 \\
= \left(\frac{{\beta'} \|y\| - t{\beta'}^2}{1 + {\beta'} ^ 2}\right)^2 + \left(\frac{t {\beta'} - \left\|y\right\|}{1 + {\beta'} ^ 2}\right)^2 \\
= \left(1 + {\beta'}^2\right)^{-2}\left[\left({\beta'}^2 \|y\|^2 - 2\beta' \|y\| t {\beta'}^2  + t^2 {\beta'}^4\right) + \left(t^2{\beta'}^2 - 2 t {\beta'} \|y\| + \|y\|^2\right) \right] \\
= \left(1 + {\beta'}^2\right)^{-2}\left[
\left(1 + {\beta'}^2\right)t^2{\beta'}^2 - 2t{\beta'}\left(1 + {\beta'}^2\right) \|y\| + \left(1 + {\beta'}^2\right)\|y\|^2
\right] \\
= \frac{\left(t \beta' - \|y\|\right)^2}{1 + {\beta'}^2}
\end{align*}

We summarize the previous statements here:
\begin{lemma}
Let $t \in \reals$, $y \in \mathbb R^{n-1}$, and $\beta' > 0$.
Then the point
\begin{align*}
z = \left(1 + {\beta'} ^ 2\right)^{-1} \left({\beta'} \|y\| + t, \left({\beta'} ^ 2 + t \frac {{\beta'}}{\|y\|}\right) y\right)
\end{align*}
satisfies
\begin{align*}
z = \argmin_{\left\|x\right\| \le \beta' s} \left\|x - y\right\|^2 + \left\|s - t\right\|^2 \\
\left\|z - (t, y)\right\|^2 = \left(1 + {\beta'}^2\right)^{-1}\left(t \beta' - \left\|y\right\| \right)^2.
\end{align*}
\end{lemma}

After a shift and rotation from the vertex point $\wik$ and going in the direction $-\wik$:

\begin{align*}
\hat w^{(i,k)} = \frac {\wik} {\|\wik\|} & \quad \forall i \in [m] \\
R^j = 2 \frac{(e_1 + \hat w^{(i,k)})(e_1 + \hat w^{(i,k)})^T}{(e_1 + \hat w^{(i,k)})^T(e_1 + \hat w^{(i,k)})} - I  & \quad \forall j \in [m],
\end{align*}
we have the following optimization problem:
\begin{align*}
\begin{array} {ccc}
\max_{r \ge 0, c, t^i}	& r & \\
					& t^i = R^j\left(\hat w^{(i,k)} - c\right) 									& \quad \forall i \in [m] \\
					& \beta^2 \left(\beta' e_1^T t^i - \left\|t^i - e_1^T t^j\right\|\right)^2 \ge r			& \quad \forall i \in [m] \\
					& \left(\left(c - w^j\right)^T\hat w^{(i,k)}\right)^2 \ge \beta^2 \|c - \wik\|^2		& \quad \forall i \in [m] \\
					& c \in \tr &
\end{array}
\end{align*}

% 
% \begin{boxedcomment}
% What did the following few lines do, again?
% \end{boxedcomment}
% 
% \begin{align*}
% \beta^2 \left(\beta' e_1^T t^j - \left\|t^j - e_1^T t^j\right\|\right)^2 \ge r \\
% \beta \left(\beta' e_1^T t^j - \left\|t^j - e_1^T t^j\right\|\right) \ge \sqrt{r} \\
% \beta \beta' e_1^T t^j - \beta \left\|t^j - e_1^T t^j\right\| \ge \sqrt{r} \\
% \beta' e_1^T t^j - \frac 1 {\beta} \sqrt{r} \ge \left\|t^j - e_1^T t^j\right\| \\
% \left( \beta' e_1^T t^j - \frac 1 {\beta} \sqrt{r} \right) ^ 2\ge \left\|t^j - e_1^T t^j\right\|^2 \\
% \end{align*}
% 
% 
% 
% 
% \begin{align*}
%  \left(\beta' e_1^T t^j - \left\|t^j - e_1^T t^j\right\|\right)^2 \ge \frac 1 {\beta^2} r \\
%  \left(\beta' e_1^T t^j\right)^2 - 2\beta' e_1^T t^j \left\|t^j - e_1^T t^j\right\| + \left\|t^j - e_1^T t^j\right\|^2 \ge \frac 1 {\beta^2} r \\
%  \left(\beta' e_1^T t^j\right)^2 - \frac 1 {\beta^2} r + \left\|t^j - e_1^T t^j\right\|^2 \ge 2\beta' e_1^T t^j \left\|t^j - e_1^T t^j\right\|\\
%  \left[\left(\beta' e_1^T t^j\right)^2 - \frac 1 {\beta^2} r + \left\|t^j - e_1^T t^j\right\|^2 \right] ^2\ge 4\left(\beta' e_1^T t^j\right)^2 \left\|t^j - e_1^T t^j\right\|^2\\
% \end{align*}
% 

Notice that this optimization problem is not convex.
Although more efficient formulations may exist, this leads us to believe that it may sometimes be difficult to even compute the maximum volume sphere within the buffering cones.

\subsection{Conservative Ellipsoid}
\sbnote{I think it would be better to reorganize this section by defining the method for constructing the ellipsoid first, and then discuss its properties.  In other words, move \cref{ellipsoids_notation_definitions} to \cref{feasible_ellipsoid_analysis}}

Throughout \cref{possible_ellipsoids} we have considered several inner trust regions.
However, we are not able to show that these formulations satisfy the requirements necessary for convergence.  \sbnote{You haven't said what these requirements are yet, set this statement is a bit vague.  Maybe it would be better to move \cref{ellipsoids_notation_definitions} to earlier in the chapter.}
% not been able to show that these satisfied the requirements stated in \cref{ellipsoids_notation_definitions}.
We were able to find an ellipsoid contained within each \cref{define_fik} that may, in general, be smaller than the largest possible ellipsoid.
Because we analyse this in more depth, we dedicate \cref{constructing_and_analysizing_conservative_ellipsoid} to this.

\section{Conservative Ellipsoid Derivation}
\label{constructing_and_analysizing_conservative_ellipsoid}

We state the ellipsoid requirements required for convergence in \cref{ellipsoids_notation_definitions}, and then show the conservative ellipsoid satisfies them.
Note the criteria in \cref{ellipsoids_notation_definitions} only summarize one of many possible sets of criteria sufficient for convergence.

\sbnote{double check the iterate superscripts in the following definitions  (i.e. $k$ or $k-1$ or $k+1$).}

\begin{definition}
\label[definition]{ellipsoids_notation_definitions}
For each $k \in \naturals$, suppose we are given $n\times n$, symmetric, positive-definite matrices $\qk$, vectors $\ck \in \Rn$, scalars $\sdk > 0$.
We have the following definitions.
\begin{itemize}
\item The sequence of tuples $((\qk, \ck, \sdk))$ is \emph{conditioned} if the condition numbers of $\qk$ are bounded for small enough $\dk$.
That is, there exists a $\sigmamax \ge 1$ and $\dacc > 0$ such that if $\dk \le \dacc$, then
\begin{align}
\condition \left(\qk\right) \le \sigmamax. \label{define_suitable_condition_numbers}
\end{align}
\item The tuple $(\qkpo, \ckpo, \sdkpo)$ is \emph{trusted} for $\xkpo, \dkpo$ if an ellipsoid defined by $\qkpo, \ckpo, \sdkpo$ lies within $ \capcones \cap \trkpo $.
That is,
\begin{align}
\unshiftedellipsoidkpo \subseteq \capcones \cap \trkpo  \label{define_suitable_in_tr}
\end{align}
where
\begin{align}
\unshiftedellipsoid = \left\{x \in \Rn | \left(x - \ck \right)^T \qk \left(x - \ck\right) \le \frac 1 2 {\sdk}^2 \right\} \label{define_unshifed_ellipsoid}
\end{align}
\item The tuple $(\qk, \ck, \sdk)$ is \emph{feasible} if an ellipsoid defined by $\qk, \ck, \sdk$ lies within the feasible region:
\begin{align}
\unshiftedellipsoid \subseteq \feasible.
\end{align}
where $\unshiftedellipsoid$ is defined by \cref{define_unshifed_ellipsoid}
\item The tuple $(\qk, \ck, \sdk)$ is \emph{adjacent} to $\xk$ if an ellipsoid defined by $\qk, \ck, \sdk$ is near to the current iterate:
\begin{align}
\xk \in \scaledunshiftedellipsoid \label{define_suitable_close_to_iterate}
\end{align}
where
\begin{align}
\scaledunshiftedellipsoid = \left\{x \in \Rn | \left(x - \ck\right)^T \qk \left(x - \ck\right) \le {\sdk}^2 \right\} \label{define_scaledunshiftedellipsoid}
\end{align}
\item The tuple $(\qk, \ck, \sdk)$ is \emph{non-empty} if
\begin{align}
\unshiftedellipsoid \ne \emptyset
\end{align}
where $\unshiftedellipsoid$ is defined by \cref{define_unshifed_ellipsoid}.
\end{itemize}
\end{definition}

\subsection{Ellipsoid Definition}

% However, we are able to show that this ellipsoid satisfies each of the requirements stated in \cref{ellipsoids_notation_definitions}.
In this section,  we show how to construct an ellipsoid that satisfies the conditions needed to prove convergence of our algorithm.  Specifically, we show that this ellipsoid is
trusted and adjacent
according to \cref{ellipsoids_notation_definitions}.
Later, in \cref{ellipsoid_is_feasible_section}, we show that it is feasible for small enough $\dk$.
Note that during iteration $k$, we construct the ellipsoid for the next iteration $k+1$.

% Until the $k$-th models are computed, this will initially be an approximation from the previous iteration:
% \begin{align}
% \approxactiveconstraintskpo = \left\{i \in [m] | \zik \in B_{\infty}(\xkpo, \dkpo)\right\}. \label{define_active_approximation}
% \end{align}

First, we define the set of nearly active constraints $\activeconstraintsk$ by \cref{define_activeconstraints}.
If there are no active constraints, $\activeconstraintsk = \emptyset$, then we simply let
\begin{align}
\qkpo = I, \quad \ckpo = \xkpo, \quad \sdkpo = \dkpo. \label{define_trivial_ellipsek}
\end{align}

However, if $\activeconstraintsk \ne \emptyset$, we compute a feasible direction for these nearly active constraints:
\begin{align}
\huk = -\argmin_{\|u\| = 1} \max_{i \in \activeconstraintsk} u^T \frac{\gmcik}{\left\|\gmcik\right\|}. \label{define_u}
\end{align}
We are interested in constructing a cone of directions that make a negative dot product with each nearly active constraint.
We measure the distance between the direction $\huk$ and the nearest direction not in this cone by computing
\begin{align}
\thetamink = \min_{i \in \activeconstraintsk} \left(-\hgik\right)^T \huk \label{define_thetamink}.
\end{align}
Intuitively, larger values of $\thetamink$ imply $\huk$ is farther from the boundary of the cone.
For simplicity, define $\thetamink = 1$ if $\activeconstraintsk = \emptyset$.
% The vector $\huk$ and and value $\thetamink$ can be computed with the following program:
% \begin{align*}
% \begin{array}{ccc}
% \huk \in \argmax_{u\in\Rn, \pi \in\reals} & \pi \\
% & -u^T \frac{\gmcik}{\left\|\gmcik\right\|} \ge \pi & \forall i \in \activeconstraintsk \\
% & \|u \| = 1& 
% \end{array}.
% \end{align*}
We then buffer the directions about $\huk$ by computing
\begin{align}
\bsk = \beta\dk^{p_{\beta}} + \sqrt{\left[1 - \left(\beta\dk^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink\right) ^2\right]} \label{define_bsk}
\end{align}
and defining the cone
\begin{align}
\fcki = \left\{x \in \Rn \; \bigg| \; x = \xkpo + ts, t > 0, \|s\| = 1, s^T\huk \ge \bsk \right\}. \label{define_inner_cone}
\end{align}
This cone is feasible with respect to all nearly active constraints.
This cone is depicted in \cref{feasible_direction}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=300px]{images/feasible_direction.png}
    \caption
    	[An example of the conservative ellipsoid being overly conservative]
    	{
    		At times, the conservative ellipsoid may be much smaller than desired.
    		Here, each cone $\fik$ is in blue, and $\fcki$ is in green.
    		$\fcki$ is contained within the intersection of the $\fik$ cones.
	}
    \label{feasible_direction}
\end{figure}

We then define an ellipsoid within this cone with a helper function
\begin{align}
f_e(\epsilon, \delta, \theta; x) = (x - \epsilon e_1)^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix}(x - \epsilon e_1) - \frac 1 2 \delta^2 \label{define_ellipse_function}
\end{align}
and rotation matrix
\begin{align}
\rotk = 2\frac{(e_1 + \huk)(e_1 + \huk)^T}{(e_1 + \huk)^T(e_1 + \huk)} - \boldsymbol I \label{define_rotation}
\end{align}
by defining
\begin{align}
\gamma &= 1 + \frac 1 {\sqrt{2}} \label{define_the_constant_gamma} \\
\bs &= \max\left\{\frac 1 2 , \bsk\right\} \label{define_bs} \\
\sampletrkpo &= \left\{x \in \Rn | f_e\left(\frac 1 {2\gamma} \dkpo, \frac 1 {2\gamma} \dkpo,\bs; \rotk(x - \xkpo)\right) \le 0\right\}. \label{define_ellipsek}
\end{align}
Note that within \cref{define_ellipsek} we have defined the components $\qkpo$, $\ckpo$, $\sdkpo$ as
\begin{align}
\begin{array}{ccc}
\qkpo &=& \left(\rotk\right)^T \begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\bs^2}{1 - \bs^2} \boldsymbol I \\
\end{pmatrix} \rotk, \\
\ckpo &=& \xkpo + \frac 1 {2\gamma} \dkpo \huk, \\
\sdkpo &=& \frac 1 {2\gamma} \dkpo. 
\end{array}\label{conservative_ellipsoid_details}
\end{align}

% \cref{ellsoid_is_suitable_theorem_p2} tells us that this ellipsoid satisfies \cref{ellipsoids_notation_definitions}.
% Which part? Trusted and adjacent

\subsection{Properties}
\label{feasible_ellipsoid_analysis}

In this section, we show that our construction of the set $\sampletrk$ as given in 
\cref{define_ellipsek} and \cref{define_trivial_ellipsek}
is trusted and adjacent according to definition \cref{ellipsoids_notation_definitions}.
This is shown within \cref{ellsoid_is_suitable_theorem_p1} and \cref{ellsoid_is_suitable_theorem_p2}.

\begin{lemma}
\label[lemma]{ellipse_in_cone}
Define $f_e$ by \cref{define_ellipse_function}.
Let $0 < \delta \le \epsilon$ and $\theta > 0$.

We have
\begin{align*}
\left\{x \in \Rn \bigg| f_e\left(\epsilon, \delta, \theta; x\right) \le 0\right\} \subseteq \left\{tx\in\Rn\bigg| e_1^T x \ge \theta,\|x\|=1, t>0\right\}.
\end{align*}
\end{lemma}

\begin{proof}
Let $x$ be such that $f_e(\epsilon, \delta, \theta, x) \le 0$.
First, note that a square is non-negative, so
\begin{align*}
0 \le 2(e_1^Tx - \frac 1 2 \epsilon )^2
= 2(e_1^Tx)^2 - 2e_1^Tx\epsilon + \frac 1 2 \epsilon^2.
\end{align*}
By moving the second and third terms to the right hand side, we find
\begin{align}
(e_1^Tx)^2 \ge \frac 1 2 \epsilon^2 - \left((e_1^Tx)^2 - 2e_1^Tx\epsilon + \epsilon^2\right) 
= \frac 1 2 \epsilon^2 - (e_1^Tx - \epsilon)^2. \label{ellipse_in_cone_eqn1}
\end{align}
If we substitute \cref{define_ellipse_function} into $f_e(\epsilon, \delta, \theta, x) \le 0$, we find that
\begin{align*}
(x - \epsilon e_1)^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix}(x - \epsilon e_1) \le \frac 1 2 \delta^2
\end{align*}
This can be simplified to
\begin{align*}
(x - \epsilon e_1)^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix}(x - \epsilon e_1) \\
 = (e_1^Txe_1 + (x - e_1^Txe_1) - \epsilon e_1)^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix}(e_1^Txe_1 + (x - e_1^Txe_1) - \epsilon e_1)  \\
=
(e_1^Tx - \epsilon)^2 + \frac{\theta^2}{1 - \theta^2}\|x - e_1^Tx e_1\|^2 \le \frac 1 2 \delta^2.
\end{align*}
For a contradiction, suppose that $e_1^Tx < 0$, then $(e_1^Tx - \epsilon)^2 \ge \epsilon^2 \ge \delta^2$.
However, the previous line shows $(e_1^Tx - \epsilon)^2 \le \frac 1 2 \delta^2$.
Thus, $e_1^Tx \ge 0$.
Therefore,
\begin{align*}
(e_1^Tx - \epsilon)^2 + \frac{\theta^2}{1 - \theta^2}\left(\|x\|^2 - (e_1^Tx)^2\right) 
\le (e_1^Tx - \epsilon)^2 + \frac{\theta^2}{1 - \theta^2}\|x - e_1^Tx e_1\|^2 \le \frac 1 2 \delta^2 \le \frac 1 2 \epsilon^2.
\end{align*}
Using \cref{ellipse_in_cone_eqn1}, 
\begin{align*}
\frac{\theta^2}{1 - \theta^2}(\|x\|^2 - (e_1^Tx)^2) \le \frac 1 2 \epsilon^2 - (e_1^Tx - \epsilon)^2 \le (e_1^Tx)^2
\Longrightarrow \|x\|^2 - (e_1^Tx)^2 \le \frac{1 - \theta^2}{\theta^2}(e_1^Tx)^2 \\
\Longrightarrow \|x\|^2 \le \frac 1 {\theta^2}(e_1^Tx)^2
\Longrightarrow e_1^T\frac{x}{\|x\|} \ge \theta
\end{align*}
where we can take the square root because $e_1^Tx \ge 0$.
\end{proof}

\begin{lemma}
\label[lemma]{ellipse_fits}
Define $f_e$ by \cref{define_ellipse_function}.
We have that $f_e(\delta, \sqrt{2}\delta, \theta; 0) = 0$ for any $\delta, \theta > 0$.
\end{lemma}
\begin{proof}
We compute
\begin{align*}
f_e(\delta, \sqrt{2}\delta, \theta; 0) =(0 - \delta e_1)^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix}(0 - \delta e_1) - \frac 1 2 (\sqrt 2 \delta)^2
=\delta^2 - \delta^2 = 0.
\end{align*}
% and
% \begin{align*}
% f_e(\delta, \delta, \theta; (1 + \frac{1}{\sqrt{2}}) \delta e_1) =\frac {\delta}{\sqrt{2}}e_1^T\bigg(\begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
% \end{pmatrix}\bigg)\frac {\delta}{\sqrt{2}}e_1 - \frac 1 2 \delta^2
% =\frac 1 2 \delta^2 - \frac 1 2 \delta^2 = 0.\\
% \end{align*}
\end{proof}


\begin{lemma}
\label[lemma]{ellipse_fits_part_2}
Define $f_e$ by \cref{define_ellipse_function}.

Suppose that for some $s \in \Rn$ with $\|s\| = 1$, $s^Te_1 \in \left[\frac 1 2, 1\right]$, $\delta \in [0, \frac 1 2]$ and $f_e(\delta, \delta, \theta; ts) \le 0$.

Then $t \le \left(2 + \sqrt{2}\right) \delta$.
\end{lemma}
\begin{proof}
Notice that $s^Te_1 - \delta \in \left[0, 1\right]$.
and $\left[\left(ts^Te_1 - \delta\right) e_1 + t\left(s - \left(s^Te_1\right) e_1\right)\right] = ts - \delta e_1$
so that
\begin{align*}
f_e(\delta, \delta, \theta; ts) \le 0 \\
\Longrightarrow 
\left[ts - \delta e_1\right]^T\begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
\end{pmatrix} \left[ts - \delta e_1\right] \le \frac 1 2 \delta^2 \\
\Longrightarrow
\left(ts^Te_1 - \delta\right)^2 + t^2\left(s - \left(s^Te_1\right) e_1\right)^2  \frac{\theta^2}{1 - \theta^2} \le \frac 1 2 \delta^2 \\
\Longrightarrow 
\left(t s^Te_1 - \delta\right)^2 \le \frac 1 2 \delta^2 
\Longrightarrow t s^Te_1 - \delta \le \frac 1 {\sqrt{2}} \delta.
\end{align*}
However,
\begin{align*}
\frac 1 2 t \le t s^Te_1 \le \left(1 + \frac 1 {\sqrt{2}}\right) \delta
\Longrightarrow t \le \left(2 + \sqrt{2}\right) \delta.
\end{align*}
\end{proof}

% \Longrightarrow 
% \left[\left(ts^Te_1 - \delta\right) e_1 + t\left(s - \left(s^Te_1\right) e_1\right)\right]^T\begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\theta^2}{1 - \theta^2} \boldsymbol I \\
% \end{pmatrix} \\ \left[\left(ts^Te_1 - \delta\right) e_1 + t\left(s - \left(s^Te_1\right) e_1\right)\right] \le \frac 1 2 \delta^2 \\




\begin{lemma}
\label[lemma]{boundbsk}
Let $\beta$, $\bsk$, $\thetamink$, and $p_{\beta}$ be defined by \cref{define_abpab}, \cref{define_bsk}, \cref{define_thetamink}, and \cref{define_abpab}.

If $\dk^{p_{\beta}} \le \frac {1} {4\beta}\left(\thetamink\right)^2$, then $\bsk \le 1 - \frac 1 4 \left(\thetamink\right)^2$.
\end{lemma}


\begin{proof}
First note that $\thetamink \le 1$ and $p_{\beta} \in (0, 1)$, so that $\beta \dk \le \frac 1 4$.
Because $\left(\thetamink\right)^2 \ge 4\beta\dk^{p_{\beta}}$,
\begin{align*}
    \left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink \right)^2\right]
\le \left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - 4\beta \dk^{p_{\beta}}\right] \\
= 1 - 4 \beta \dk^{p_{\beta}} - \left(\beta \dk^{p_{\beta}}\right)^2 + 4 \left(\beta \dk^{p_{\beta}}\right)^3 \\
= \left(1 - 2 \beta \dk^{p_{\beta}}\right)^2 - 5 \left(\beta\dk^{p_{\beta}}\right)^2 + 4 \left(\beta \dk^{p_{\beta}}\right)^3 \\
\le \left(1 - 2\beta\dk^{p_{\beta}}\right)^2.
\end{align*}
Thus, 
\begin{align}
\label{steves_rewrite_eqn1}
\sqrt{\left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink \right)^2\right]} 
\le 1 - 2 \beta \dk^{p_{\beta}}.
\end{align}
Note also that $1 - \left(\beta  \dk ^{p_{\beta}}\right)^2 \le 1$, so
\begin{align}
\label{steves_rewrite_eqn2}
\sqrt{\left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink \right)^2\right]} 
\le \sqrt{1 - \left(\thetamink \right)^2} \le 1 - \frac 1 2 \left(\thetamink \right)^2.
\end{align}
Adding \cref{steves_rewrite_eqn1} and \cref{steves_rewrite_eqn2}, we find
\begin{align*}
2 \sqrt{\left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink \right)^2\right]} 
\le 2 - 2 \beta \dk ^{p_{\beta}} - \frac 1 2 \left(\thetamink \right)^2
\end{align*}
and
\begin{align*}
\bsk = \beta \dk ^{p_{\beta}} + \sqrt{\left[1 - \left(\beta \dk ^{p_{\beta}}\right)^2\right]\left[1 - \left(\thetamink \right)^2\right]}
\le \beta \dk ^{p_{\beta}} + 1 - \beta \dk ^{p_{\beta}} - \frac 1 4 \left(\thetamink \right)^2 \\
= 1 - \frac 1 4 \left(\thetamink \right)^2.
\end{align*}
\end{proof}

% \begin{proof}
% For any $0 < \epsilon < 1$ there exists $\dacco(\epsilon) > 0$, such that for any $k \in \naturals$ with
% $\thetamink \ge \epsilon$ and $\dk \le \dacco(\epsilon)$, we have
% $\bsk \le 1 - \frac 1 4 \epsilon^2$.
% Let $\alpha$, $\beta$, $p_{\alpha}$, and $p_{\beta}$ be defined by \cref{define_abpab}.
% % Also, let $\minangledelta$ be defined as in \cref{minangleassumption}.
% Then we can let $\dacco(\epsilon)$ be defined by
% % \minangledelta,
% % 1,
% \begin{align}
% \dacco(\epsilon) < \min\left\{
% \left(\frac {\epsilon ^2} {4\beta} \right)^{\frac 1 {p_{\beta}}},
% \left(\frac 1 {2\beta}\right)^{\frac 1 {p_{\beta}}}
% \right\}\label{define_delta_accuracy_old}.
% \end{align}
% If $\dk \le \dacco(\epsilon)$, then
% \begin{align}
% \beta\dk^{p_{\beta}} \le \frac 1 {4} \epsilon^2 \quad \textrm{and}\label{boundedbeta_deltasmall_2} \\
% 2\beta\dk^{p_{\beta}} \le 1. \label{boundedbeta_deltasmall_3}
% \end{align}
% Rearranging \cref{boundedbeta_deltasmall_2}, provides $-\epsilon^2 \le -4\beta\dk^{p_{\beta}}$,
% which combined with $\epsilon ^2 \le 1 \le 5$ shows
% \begin{align*}
% -\left[5- \epsilon^2\right]\left(\beta\dk^{p_{\beta}}\right)^2  - \epsilon^2 \le -4\beta\dk^{p_{\beta}}.
% \end{align*}
% This means
% \begin{align*}
% \left(1 - \epsilon^2\right)\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right) 
% = 1 - \epsilon^2 - \left[5 - \epsilon^2\right]\left(\beta\dk^{p_{\beta}}\right)^2 + 4\left(\beta\dk^{p_{\beta}}\right)^2 \\
% \le 1 - 4\beta\dk^{p_{\beta}} + 4\left(\beta\dk^{p_{\beta}}\right)^2 = \left(1 - 2\beta\dk^{p_{\beta}}\right)^2.
% \end{align*}
% Dividing by $1 - \left(\beta\dk^{p_{\beta}}\right)^2$, taking the square root, and then dividing by $2$ we see
% % 1 - \epsilon^2 \le \frac{\left(1 - 2\beta\dk^{p_{\beta}}\right)^2}{1 - \left(\beta\dk^{p_{\beta}}\right)^2}
% % \Longrightarrow
% \begin{align}
% \frac 1 2 \sqrt{1 - \epsilon^2} \le \frac 1 2 \frac{1 -2\beta\dk^{p_{\beta}}}{\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}}
% = \frac{\frac 1 2 -\beta\dk^{p_{\beta}}}{\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}}. \label{boundedbeta_eqn1}
% \end{align}
% Here, \cref{boundedbeta_deltasmall_3} ensures that the denominator is positive.
% Also,
% \begin{align*}
% 1 - \epsilon^2 \le 1 - \epsilon^2 + \frac 1 4 \epsilon^4 
% = \left(1 - \frac 1 2 \epsilon^2 \right)^2 
% \Longrightarrow 1 \le \frac{\left(1 - \frac 1 2 \epsilon^2\right)^2}{1 - \epsilon^2}
% \end{align*}
% so that
% \begin{align*}
% 1 - \left(\beta\dk^{p_{\beta}}\right)^2 \le 1 \le \frac{\left(1 - \frac 1 2 \epsilon^2\right)^2}{1 - \epsilon^2} 
% \Longrightarrow \sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}\le \frac{1 - \frac 1 2 \epsilon^2}{\sqrt{1 - \epsilon^2} } 
% \Longrightarrow \sqrt{1 - \epsilon^2} \le \frac{1 - \frac 1 2 \epsilon^2}{\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}}.
% \end{align*}
% Dividing by $2$ yields:
% \begin{align}
% \frac 1 2 \sqrt{1 - \epsilon^2} \le \frac{\frac 1 2 - \frac 1 4 \epsilon^2}{\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}}
% \label{boundedbeta_eqn2}.
% \end{align}
% 
% We then add \cref{boundedbeta_eqn1} and \cref{boundedbeta_eqn2} to find
% \begin{align*}
% \sqrt{1 - \epsilon^2} \le \frac{1 -  \frac 1 4 \epsilon^2 - \beta\dk^{p_{\beta}}}{\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2}}
% \Longrightarrow \beta\dk^{p_{\beta}} + \left(\sqrt{1 - \epsilon^2}\right)\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2} \le 1 -  \frac 1 4 \epsilon^2.
% \end{align*}
% Because $\thetamink \ge \epsilon$, we have $\sqrt{1 - \epsilon^2} \ge \sqrt{1 - \left(\thetamink\right)^2}$
% so that
% \begin{align*}
% \bsk 
% = \beta\dk^{p_{\beta}} + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \left(\thetamink\right)^2\right)} 
% \le \beta\dk^{p_{\beta}} + \left(\sqrt{1 - \epsilon^2}\right)\sqrt{1 - \left(\beta\dk^{p_{\beta}}\right)^2} \\
% \le 1 -  \frac 1 4 \epsilon^2.
% \end{align*}
% \end{proof}



\begin{lemma}
\label[lemma]{boundbeta}
Let $\beta$, $\thetamink$, and $\qkpo$ be defined by \cref{define_abpab}, \cref{define_thetamink}, and \cref{define_ellipsek} respectively.

If $\dk \le \frac 1 {4 \beta} \left(\thetamink\right)^2$, then $\condition \left(\qkpo\right) \le 12 \left(\thetamink\right)^{-2}$.

% For any $0 < \epsilon < 1$ there exists $\dacco(\epsilon) > 0$, such that for any $k \in \naturals$ with
% $\thetamink \ge \epsilon$ and $\dk \le \dacco(\epsilon)$, we have $\condition\left(\qkpo\right) \le \frac{12}{\epsilon^2}$.
\end{lemma}

\begin{proof}
Let $\rotk$, and $\gamma$ be defined as in \cref{define_rotation}, and \cref{define_the_constant_gamma} respectively.
Also, during iteration $k$, let $\bs$ be defined by \cref{define_bs}.
% Definition \cref{define_ellipsek} states
% \begin{align*}
% \qk = \rotk^T \begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\bs^2}{1 - \bs^2} \boldsymbol I \\
% \end{pmatrix} \rotk, \quad
% \ck = \xk  + \frac 1 {2\gamma} \dk\huk, \quad
% \sdk = \frac 1 {2\gamma} \dk.
% \end{align*}
First note that by \cref{define_bs} and \cref{boundbsk}, we have
$\frac 1 2 \bsk \le 1 - \frac 1 4 \left(\thetamink\right)^2$.
Squaring this yields
\begin{align}
\frac {1} 4 \le \left(\bs\right)^2 \le \left(1 - \frac 1 4 \left(\thetamink\right)^2\right)^2 \le 1 - \frac 1 4 \left(\thetamink\right)^2 \label{p2_numerator}
\end{align}
so that
\begin{align}
\frac 1 4  \left(\thetamink\right)^2 \le 1 - \left(\bs\right)^2 \le \frac 3 4. \label{p2_denominator}
\end{align}
Dividing \cref{p2_numerator} by \cref{p2_denominator}, we see that
\begin{align*}
\frac 1 3
\le \frac{\left(\bs\right)^2}{1 - \left(\bs\right)^2}
\le \frac {4 - \left(\thetamink\right)^2}{\left(\thetamink\right)^2} \le \frac {4}{\left(\thetamink\right)^2}
\end{align*}
Now, using \cref{conservative_ellipsoid_details}, we can compute 
\begin{align*}
\condition \left(\qkpo\right) 
= \frac{\max\left\{1, \frac{\left(\bs\right)^2}{1 - \left(\bs\right)^2}\right\}}{\min\left\{1, \frac{\left(\bs\right)^2}{1 - \left(\bs\right)^2}\right\}} 
\le \frac {12}{\left(\thetamink\right)^2}
\end{align*}
as $\det(\rotk) = 1$.
\end{proof}




\begin{lemma}
\label[lemma]{cone_subset_cone}
Given $u, v \in \Rn$, and $\gamma \in (0, 1]$, $\beta \in [0, \gamma)$ that satisfy $\|u\| = \|v\|= 1$, $u^Tv = \gamma$, define
\begin{align*}
B = \{x\in\Rn | {v}^Tx \ge \beta\|x\|\}, \quad
S = \left\{x\in\Rn \bigg| u^Tx \ge \left(\beta\gamma + \sqrt{(1 - \beta^2)\left(1 - \gamma^2\right)}\right)\|x\| \right\}. 
\end{align*}
Then, $S \subseteq B$.
\end{lemma}

\begin{proof}
For a contradiction, let $y \in \Rn$ be such that $y \not \in B$ and $y \in S$ and define $\hat y = \frac{y}{\|y\|}$.
That is,
\begin{align}
v^T\hat y < \beta \label{csc_vy} \\
u^T\hat y \ge \beta\gamma + \sqrt{\left(1 - \beta^2\right)\left(1 - \gamma^2\right)}. \label{csc_uy}
\end{align}

Define
\begin{align*}
x^{\star} = \beta v + \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} (u - \gamma v )
\end{align*} and notice
\begin{align}
\begin{array}{ccccc}
{u}^Tx^{\star} &=& \beta\gamma + \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} (1 - \gamma^2) &=&  \beta\gamma + \sqrt{\left(1 - \beta^2\right)\left(1 - \gamma^2\right)} \\
{v}^Tx^{\star} &=& \beta + \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}}(\gamma - \gamma) &=& \beta \\
{x^{\star}}^Tx^{\star} &=& \beta^2 + 2\beta\sqrt{\frac{1 - \beta^2}{1 - \gamma^2}}(\gamma - \gamma) + \frac{1 - \beta^2}{1 - \gamma^2} (1- 2\gamma^2 + \gamma^2)&=& 1.
\end{array}. \label{csc_vx_ux}
\end{align}

Using \cref{csc_vx_ux} and \cref{csc_uy}, we see
\begin{align*}
\beta\gamma + \sqrt{\left(1 - \beta^2\right)\left(1 - \gamma^2\right)} \le {u}^T\hat y = {u}^T\left(x^{\star} + \hat y - x^{\star}\right) \\
= \beta \gamma + \sqrt{(1 - \beta^2)\left(1 - \gamma^2\right)} + {u}^T\left(\hat y - x^{\star}\right).
\end{align*}
Subtracting $\beta\gamma + \sqrt{\left(1 - \beta^2\right)\left(1 - \gamma^2\right)}$ from both sides, we find
\begin{align}
{u}^T\left(\hat y - x^{\star}\right) \ge 0 \label{csc_uymx}.
\end{align}
Likewise, we can use \cref{csc_vx_ux} and \cref{csc_vy} to provide
\begin{align*}
\beta > {v}^T\hat y = {v}^T\left(x^{\star} + \hat y - x^{\star}\right) = \beta + {v}^T\left(\hat y - x^{\star}\right).
\end{align*}
After subtracting $\beta$ from both sides, we see that
\begin{align}
{v}^T\left(\hat y - x^{\star}\right) < 0 \Longrightarrow -{v}^T\left(\hat y - x^{\star}\right) > 0. \label{csc_vymx}
\end{align}
% which means $\hat y \ne x^{\star}$.
Lastly, we will need
\begin{align}
\gamma > \beta 
\Longrightarrow 1 - \beta^2 > 1 - \gamma^2
\Longrightarrow \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} > 1
\Longrightarrow \gamma \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} - \beta > 0. \label{csc_gamma_beta_positive}
\end{align}
Now, we can compute
\begin{align*}
{\left(\hat y - x^{\star}\right)}^Tx^{\star} = 
\beta {\left(\hat y - x^{\star}\right)}^Tv
+ \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} 
\left(u^T\left(\hat y - x^{\star}\right) - \gamma v^T \left(\hat y - x^{\star}\right) \right)\\ 
= \left[\gamma \sqrt{\frac{1 - \beta^2}{1 - \gamma^2}} - \beta\right] \left[-v^T\left(\hat y - x^{\star}\right)\right]
+ \left[\sqrt{\frac{1 - \beta^2}{1 - \gamma^2}}\right] \left[u^T\left(\hat y - x^{\star}\right) \right] > 0.
\end{align*}
because \cref{csc_uymx}, \cref{csc_vymx}, and \cref{csc_gamma_beta_positive} show that the first product is positive and the second product is non-negative.
However, this is a contradiction as
\begin{align*}
1 = \|\hat y\|^2 = \|x^{\star} + \hat y - x^{\star}\|^2 = \|x^{\star}\|^2 + 2{\left(\hat y - x^{\star}\right)}^Tx^{\star} + \|\hat y - x^{\star}\|^2 > \|x^{\star}\|^2 = 1
\end{align*}
and there is no such $y$.
Thus, any $y \in\Rn$ with $y \in S$ must also have $y \in B$.
\end{proof}


\begin{lemma}
\label[lemma]{large_zik_means_means_no_intersection}
Let
$\zik$ and $\fik$
be defined by
\cref{define_z} and \cref{define_fik} respectively.
For any $R > 0$ and $K > R\sqrt{n}$, there exists $\deltalargzik > 0$ such that if $\dk \le \deltalargzik$ and 
\begin{align*}
\zik \not \in B_{\infty}\left(\xk, (1+K) \dk\right),
\end{align*}
then $B_{\infty}\left(\xk, R \dk\right) \subseteq \fik$.
\end{lemma}
\begin{proof}
Let $\alpha$, $\beta$, $p_{\alpha}$, and $p_{\beta}$ be defined by \cref{define_abpab}.
We can define
\begin{align}
\deltalargzik = \min\left\{
\left(\frac 1 {\beta }\frac {K - R\sqrt{n}}{K + R\sqrt{n}}\right)^{\frac 1 {p_{\beta }}},
\left(\frac 1 {(1+K)\alpha}\right)^{\frac 1 {p_{\alpha}}}
\right\} \label{define_deltalargzik}
\end{align}
and suppose that $\|\xk - \zik\| \ge (1+K) \dk$.

First, note that by \cref{define_w}
\begin{align*}
\|\xk - \wik\| = (1 - \alpha\dk^{p_{\alpha}}) \|\xk - \zik\| \ge \frac {K} {(1+K)} (1+K)\dk = K\dk.
\end{align*}

Let $y \in B_{\infty}\left(\xk, R \dk\right)$, so that $\|y - \xk\| \le \sqrt{n}R\dk$.
Also, define
\begin{align*}
t  = \frac{(y - \wik)^T(\xk - \wik)}{\left(\xk - \wik\right)^T(\xk - \wik)} \\
p = \wik + t\left(\xk - \wik\right)
\end{align*}
so that
\begin{align*}
\|y - \wik\| \le \|y - \xk\| + \|\xk - \wik\| \\
\Longrightarrow \frac{\|y - \wik\|}{\|\xk - \wik\|} \le 1 +  \frac{\|y - \xk\|}{\|\xk - \wik\|} 
\le 1 + \frac{\sqrt{n}R\dk}{K \dk} = \frac {K+R\sqrt{n}}{K} \\
\Longrightarrow \frac{\|\xk - \wik\|}{\|y - \wik\|} \ge \frac {K}{K+R\sqrt{n}}
\end{align*}
and
\begin{align*}
\left(y - p\right)^T\left(\xk - \wik\right) = 
\left(y - \wik - t\left(\xk - \wik\right)\right)^T\left(\xk - \wik\right) \\
= \left(y - \wik\right)^T\left(\xk - \wik\right) - t\left(\xk - \wik\right)^T\left(\xk - \wik\right) = 0.
\end{align*}
Because
\begin{align*}
\|\xk - p\| = \|\xk - \wik + \wik - p\| = \left\|\xk - \wik - t\left(\xk - \wik\right)\right\| \\
= (1-t)\|\xk - \wik\| \ge (1-t)K\dk
\end{align*}
we have
\begin{align*}
nR^2\dk^2 \ge \|\xk - y\|^2 = \|\xk - p\|^2 + \|y - p\|^2 \ge K^2(1-t)^2\dk^2  \\
\Longrightarrow 1-t \le \frac {R\sqrt{n}} {K} 
\Longrightarrow t \ge \frac {K - R\sqrt{n}}{K}
\end{align*}

Putting this together, we see
\begin{align*}
\frac{\xk - \wik}{\left\|\xk - \wik\right\|}^T\frac{y - \wik}{\left\|y - \wik\right\|} 
= \frac{\left(\xk - \wik\right) ^T\left(y - p\right) + \left(\xk - \wik\right)^T\left(p - \wik\right)}{\left\|\xk - \wik\right\|\left\|y - \wik\right\|} \\
= t\frac{\left(\xk - \wik\right)^T\left(\xk - \wik\right)}{\left\|\xk - \wik\right\|\left\|y - \wik\right\|}
= t \frac{\left\|\xk - \wik\right\|}{\left\|y - \wik\right\|}
= \frac {K - R\sqrt{n}}{K + R\sqrt{n}} \ge \beta \dk^{p_{\beta}}.
\end{align*}
% \ge \frac {K - \sqrt{n}}{K} \frac {K}{K+\sqrt{n}}
\end{proof}

\begin{lemma}
\label[lemma]{inner_cone_inside_each_cone}
Let $\fcki$ and $\capcones$ be defined by \cref{define_inner_cone} and \cref{define_capcones} respectively.
If 
% $\dk \le 1$ and 
$\xkpo \in \capcones$, then $\fcki \subseteq \capcones$.
\end{lemma}
% \left(\frac 1 {\beta} \sqrt{\frac 3 4}\right)^{\frac 1 {p_{\beta}}}

\begin{proof}
Let 
$\huk$, $\bsk$, $\thetamink$, $\activeconstraintsk$, and $\fik$
be defined by
\cref{define_u}, \cref{define_bsk}, \cref{define_thetamink}, \cref{define_activeconstraints}, and \cref{define_fik} 
respectively and $\alpha$, $\beta$, $p_{\alpha}$, $p_{\beta}$ be defined by \cref{define_abpab}.
Fix some $i \in \activeconstraintsk$, and define $\gamma_i = -\left(\huk\right)^T\hgik$ as well as
\begin{align*}
S_1 = \left\{s\in\Rn | \quad s^T\huk\ge\bsk\|s\| \right\} \\
S_2 = \left\{s\in\Rn | \quad s^T\huk\ge\left[\beta\dk^{p_{\beta}}\gamma_i + \sqrt{(1 - \left(\dk^{p_{\beta}}\beta\right)^2)\left(1 - \gamma_i^2\right)}\right]\|s\| \right\} \\
S_3 = \left\{s\in\Rn | \quad s^T\left(-\hgik\right)\ge\beta\dk^{p_{\beta}}\|s\| \right\}
\end{align*}
The set $S_1$ is that set of all feasible directions from $\xk$ for $\fcki$, and $S_3$ is that set of all feasible direction from $\wik$ for $\fik$.
Because $\xkpo \in \fik$, $\fcki \subseteq \fik$ follows from $S_1 \subseteq S_3$.
We show this next.


% = -\frac{m_{c_i}(\xk)}{\|\gmcik\|}\frac{\|\gmcik\|}{m_{c_i}(\xk)}
% Now, we show that $S_1 \subseteq S_3$.
By letting $u \gets \huk$, $v \gets -\hgik$, $\beta \gets \beta \dk^{p_{\beta}}$, \cref{cone_subset_cone} tells us that
$S_2 \subseteq S_3$.
But $\gamma_i \ge \thetamink$, so that
\begin{align*}
\beta\dk^{p_{\beta}}\gamma_i + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \gamma_i^2\right)}
\le \max_{i\in\activeconstraintsk} \left(\beta\dk^{p_{\beta}}\gamma_i + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \gamma_i^2\right)}\right) \\
\le \beta\dk^{p_{\beta}} \max_{i\in\activeconstraintsk}\left\{\gamma_i\right\} + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \left(\min_{i\in\activeconstraintsk}\left\{\gamma_i\right\}\right)^2\right)} \\
\le \beta\dk^{p_{\beta}} + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \left(\thetamink\right)^2\right)} = \bsk
\end{align*}

and for any $s\in S_1$,
\begin{align*}
\left(\frac{s}{\|s\|}\right)^T\huk \ge \bs 
\ge \beta\dk^{p_{\beta}}\gamma_i + \sqrt{(1 - \dk^{2p_{\beta}}\beta^2)\left(1 - \gamma_i^2\right)}
\Longrightarrow s \in S_2 \subseteq S_3.
\end{align*}

Thus, $\fcki \subseteq \fik$.
\end{proof}


% \color{red}
% The red is no longer needed.
% First, we show that $\xk \in \fik$.
% We see that:
% \begin{align*}
% \xk = \xk + \left(1 - \alpha\dk^{p_{\alpha} }\right)(\zik - \xk) - \left(1 - \alpha\dk^{p_{\alpha} }\right)(\zik - \xk)
% \end{align*}
% where
% \begin{align*}
% \frac{-\left(1 - \alpha\dk^{p_{\alpha} }\right)(\zik - \xk)}{\left\|-\left(1 - \alpha\dk^{p_{\alpha} }\right)(\zik - \xk)\right\|}^T\hgik = 1 \ge \beta \dk^{p_{\beta}}
% \end{align*}
% because $\dk \le 1$.
% \color{black}


\begin{lemma}
\label[lemma]{ellsoid_is_suitable_theorem_p1}
Let $\activeconstraintsk$ and $\omegainc$ be defined by \cref{define_activeconstraints} and \cref{define_the_omegas} respectively.
If $\activeconstraintsk = \emptyset$ and $\dkpo \le \omegainc\dk$, 
then there exists $\deltalargzik>0$ such that if $\dk \le \deltalargzik$ the
ellipsoids defined by \cref{define_trivial_ellipsek} 
are trusted, and adjacent according to \cref{ellipsoids_notation_definitions}.

% satisfy \cref{ellipsoids_notation_definitions}.
\end{lemma}
\begin{proof}
Also, \cref{define_suitable_close_to_iterate} is satisfied because $(\xkpo - \xkpo)^T\qkpo(\xkpo - \xkpo) = 0 \le \sdkpo^2$.
Let $\omegainc$, $\capcones$, $\fik$, and $\zikthresh$ be defined by \cref{define_the_omegas}, \cref{define_capcones}, \cref{define_fik}, and \cref{define_zikthresh} respectively.
Because $\activeconstraintsk = \emptyset$, we can use 
\cref{large_zik_means_means_no_intersection} with $R = (2 + \omegainc)\sqrt{n}$, $K = \zikthresh$ to conclude the existence of $\deltalargzik > 0$ such that 
if $\dk \le \deltalargzik$, then for each 
$i \in [m]$, $\trkpo \subseteq \fik$.
Thus, \cref{define_suitable_in_tr} is satisfied because 
\begin{align*}
\unshiftedellipsoidkpo = \left\{x \in \Rn \bigg| \|x - \xkpo\|^2 \le \dkpo^2 \right\} \subseteq \trkpo \subseteq \capcones.
\end{align*}
\end{proof}


\begin{lemma}
\label[lemma]{ellsoid_is_suitable_theorem_p2}
Let $\activeconstraintsk$ 
% and $\omegainc$ 
be defined by 
\cref{define_activeconstraints}
% and \cref{define_the_omegas} respectively
.
If $\activeconstraintsk \ne \emptyset$ and $\dk \le 1$, 
then the ellipsoids defined by \cref{define_ellipsek} are trusted and adjacent according to \cref{ellipsoids_notation_definitions}.

% There exsists $\dacc > 0$ such that if $\dk \le \dacc$
% and $\activeconstraintsk \ne \emptyset$
% \begin{align}
% \bs = \max\left\{\frac 1 2, \bsk\right\} \label{define_bs} \\
% \gamma = 1 + \frac 1 {\sqrt 2}
% \end{align}
% where $\dacc$ is defined in \cref{define_delta_accuracy}, and $\rotk$ be defined as in \cref{define_rotation}.
% Then, \cref{ellipsoids_notation_definitions} is satisfied by letting
% as in \cref{define_ellipsek}.
\end{lemma}
\begin{proof}
Let $\rotk$, $\gamma$, $f_e$, and $\bs$ be defined as in \cref{define_rotation}, \cref{define_the_constant_gamma}, \cref{define_ellipse_function}, and \cref{define_bs} respectively.
% Definition \cref{define_ellipsek} states
% \begin{align*}
% \qk = \rotk^T \begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\bs^2}{1 - \bs^2} \boldsymbol I \\
% \end{pmatrix} \rotk, \quad
% \ck = \xk  + \frac 1 {2\gamma} \dk\huk, \quad
% \sdk = \frac 1 {2\gamma} \dk.
% \end{align*}
Because $\rotk\huk = e_1$, we have for any $\delta > 0$:
\begin{align*}
\left(x - \ckpo\right)^T\qkpo\left(x - \ckpo\right) - \frac 1 2 \delta^2 
= \left(\rotk\left(x-\xkpo\right) - \frac 1 {2\gamma} \dkpo e_1\right)^T \\ \begin{pmatrix}
1 & \boldsymbol0^T \\
\boldsymbol 0 & \frac{\left(\bs\right)^2}{1 - \left(\bs\right)^2} \boldsymbol I \\
\end{pmatrix} \left(\rotk\left(x-\xkpo\right) - \frac 1 {2\gamma} \dkpo e_1\right) - \frac 1 2 \delta^2 \\
= f_e\left(\frac 1 {2\gamma} \dkpo, \delta, \bs; \rotk\left(x - \xkpo\right)\right).
\end{align*}

% This used to be a line, but it was too long...
% = \left(x - \left(\xkpo + \frac 1 2 \dkpo \huk\right)\right)^T\rotk^T\begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\left(\bs\right)^2}{1 - \left(\bs\right)^2} \boldsymbol I \\
% \end{pmatrix} \rotk\left(x - \left(\xkpo + \frac 1 2 \dkpo \huk\right)\right) - \frac 1 2 \delta^2 \\


In particular, with definitions \cref{define_unshifed_ellipsoid} and \cref{define_scaledunshiftedellipsoid},
we can let $\delta = \sdkpo$ and $\delta = \sqrt{2}\sdkpo$ to see that with $s = x - \xkpo$
\begin{align*}
\unshiftedellipsoidpo = \left\{\xkpo + s \in \Rn | f_e\left(\frac 1 {2\gamma} \dkpo, \frac 1 {2\gamma} \dkpo, \bs; \rotk s \right) \le 0 \right\} \\
\scaledunshiftedellipsoidpo = \left\{\xkpo + s \in \Rn | f_e\left(\frac 1 {2\gamma} \dkpo, \sqrt{2} \frac {1}{2\gamma}\dkpo, \bs; \rotk s\right) \le 0 \right\}.
\end{align*}
With $y = \rotk s$,  we can use \cref{ellipse_in_cone} to conclude
\begin{align*}
\unshiftedellipsoidpo
\subseteq \xkpo + \rotk\left\{y \in \Rn | f_e\left(\frac 1 {2\gamma} \dkpo, \frac 1 {2\gamma} \dkpo, \bs; y \right) \le 0 \right\} \\
\subseteq  \xkpo + \rotk \left\{t y \in \Rn \bigg| e_1^Ty \ge \bs, \left\|y\right\| = 1, t > 0 \right\} \\
= \xkpo + \left\{t s \in \Rn \bigg| e_1^T\rotk s \ge \bs, \left\|\rotk s\right\| = 1, t > 0 \right\} \\
= \left\{x  \in \Rn \bigg| x = \xkpo + ts, s^T\huk \ge \bs, \|s\| = 1, t > 0 \right\}
\end{align*}
% .
% \end{align*}
% % because $\bs \ge \bsk$ and $\frac 1 {2\gamma} \dkpo \le \frac 1 {2\gamma} \dkpo$.
% % With a change of variables $s \gets x - \xkpo$, we see that
% \begin{align*}

as $\rotk = \rotk^T$.
Because $\bs \ge \bsk$ and $\bs \ge \frac 1 2$, we know two things:
\begin{align*}
\unshiftedellipsoidpo \subseteq \left\{x \in \Rn | x = \xkpo + ts, s^T\huk \ge \frac 1 2, \|s\|= 1, t > 0 \right\} \\
\textrm{and} \quad \unshiftedellipsoidpo \subseteq \left\{x \in \Rn | x = \xkpo + ts, s^T\huk \ge \bsk, \|s\|= 1, t > 0 \right\} = \fcki
\end{align*}
where $\fcki$ is defined by \cref{define_inner_cone}.
Thus, for any $x \in \unshiftedellipsoidpo$, we can let $x = \xkpo + ts$, $\|s\| = 1$, $s^T\huk \ge \frac 1 2$.
Because $\frac 1 2 \dkpo \le \frac 1 2$, we can apply \cref{ellipse_fits_part_2} to conclude $\|x - \xkpo\| = t \le \left(2 + \sqrt{2}\right)\frac 1 {2\gamma} \dkpo = \dkpo$.
Thus, $x \in \trkpo$, and $\unshiftedellipsoidpo \subseteq \fcki \cap \trkpo$.
By \cref{inner_cone_inside_each_cone}, $\fcki \subseteq \capcones$ and \cref{define_suitable_in_tr} is satisfied.
For \cref{define_suitable_close_to_iterate}, \cref{ellipse_fits} tells us that
$f_e\left(\frac 1 {2\gamma} \dkpo, \frac 1 {\sqrt{2}\gamma}\dkpo, \bs; \rotk\left(\xkpo - \xkpo\right)\right) = 0$, so that $\xkpo \in \scaledunshiftedellipsoidpo$.
\end{proof}



\section{Infeasible Trial Points}
\label{convex_model_reduction}
\sbnote{Rework this introduction to reflect the new organization}
The second instance that the algorithm may attempt to evaluate an infeasible point is after solving the trust region subproblem.
We have two methods for dealing with this case.

\subsection{Linear cuts}
\label{linear_cuts_section}

\sbnote{Consider removing this section, or moving it to a ``other ideas'' or ``future works'' section.}
One strategy is to solve the trust region subproblem again, after adding a linear constraint that removes this infeasible point.
Each new linear constraint not only cuts off the last trial point, but also stays at least a fraction of the trust region radius away.

\begin{figure}[ht]
    \centering
    \includegraphics[width=300px]{images/pyomo_cut_solution.png}
    \caption
    		[Infeasible points can be removed from the search with linear cuts.]
    	{
			Adding linear cuts to remove infeasible points.
			Sample points that have already been evaluated and found to be feasible are shown as green circles.
			Attempted evaluations that resulted in infeasible evaluations are shown as red x's.
			The linear cuts (blue lines) are chosen to minimize the value of the objective (in black) while ensuring that all failed evaluations are infeasible.
	}
    \label{pvip}
\end{figure}


Suppose that the algorithm has evaluated several infeasible points after solving the trust region subproblem.
These are stored in a set $\trsinfset = \{n_1, n_2, \ldots, n_{|\trsinfset|}\}$.
We then choose one hyperplane $\{x \in \Rn | d_k^Tx = b_k\}$ for each infeasible point to remove that feasible point from our next attempt to solve the trust region subproblem.
Notice that these hyperplanes are \emph{decision variables} during the next attempt, so as to give as much freedom for our next trial solution.

If we let $\trstol \in (0, 1)$ be the percentage of the trust region radius with which we wish buffer our next solution, 
we arrive at the following optimization problem with $\searchtrk = \capcones \cap \tr $:
\begin{align}
\label{buffered_trust_region_subproblem}
\begin{array}{ccc}
\sk = \argmin_{s, d^{(k)} \in \Rn, b_i \in \reals}	& \mfk(x) & 	\\
 \mbox{subject to}  & n_k^Td^{(k)} \ge b_k + \trstol \dk& \forall k \in \left[ |\trsinfset |\right] \\
 & s^T d^{(k)} \le b_k &   \forall k \in \left[|\trsinfset |\right]  \\
 & \|d^{(k)}\| = 1 & \forall k \in \left[|\trsinfset |	\right]\\
 & \nabla \mcik(\xk) ^T s \le \mcik(\xk) & \forall i \in [m] \\
 & \|s - \xk \|_{\infty} \le \dk & \\
\end{array}
\end{align}

We use this optimization problem as a subroutine of the trust region subproblem algorithm:

\begin{algorithm}[H]
    \caption{Solve Trust Region Subproblem}
    \label{linear_cut_trust_region_subproblem}
    \begin{itemize}
        \item[\textbf{Step 0}] \textbf{(Initialization)} \\
	    Initialize the set of infeasible points $\trsinfset = \emptyset$.
        
        \item[\textbf{Step 1}] \textbf{Solve Trust Region Problem} \\
	    Solve \cref{buffered_trust_region_subproblem} to find trial point $s$.
	    If the feasible set is empty, \textbf{Fail}
        
        \item[\textbf{Step 2}] \textbf{(Check feasibility)} \\
            Evaluate the objective and constraints $s$. \\
            If $s\in\feasible$, \textbf{return} $s$.
            Otherwise, if $s\in\feasible$ and $s \in \sampletrk$, \textbf{Fail}
	    Otherwise, if $s\in\feasible$ and $s \not \in \sampletrk$ \begin{itemize}
	    	\item[] $\trsinfset \gets \trsinfset \cup \{s\}$
	    	\item[] Go to Step 1
	    \end{itemize}
            
        \item[\textbf{Step 3}] \textbf{(Update maximum volume ellipsoid)} \\
	    Update $E_{max}$
	    decrease random search variance
            
        $k \gets k+1$ and go to Step 1.
    \end{itemize}
\end{algorithm}

% The properties of the trial point found by this algorithm,
% as well as its convergence are detailed in \cref{efficiency_condition_analysis}.



\subsection{Decrease the Trust Region Radius}
\label{decreasing_the_trust_region_for_infeasible_trial}

Another option is to simply decrease the trust region radius.
However, in order to do this, we must be sure that we use a search region contained within $\feasible$ for small enough $\dk$.
In \cref{ellipsoid_is_feasible}, we show that the set $\capcones$ defined in \cref{define_capcones} is just this set.
Thus, we set $ \searchtrk = \capcones \cap \tr$ by defining the trust region subproblem as
\begin{align}
\label{capcones_tr_subproblem}
\begin{array}{ccc}
\min_{s^{(i)},x,t_i} & m_f(x) & \\
 & x = \wik + t _i s^{(i)} & \quad \forall i \in \activeconstraintsk \\
 & \|s^{(i)}\| = 1 & \quad \forall i \in \activeconstraintsk \\
 & -\left(s^{(i)}\right)^T\hgik \ge \beta \dk^{p_{\beta}} & \quad \forall i \in \activeconstraintsk \\
 & s^{(i)} \in \Rn  & \quad \forall i \in \activeconstraintsk \\
 & t_i > 0          & \quad \forall i \in \activeconstraintsk \\
 & x \in \tr		& \\
\end{array}
\end{align}
In \cref{sufficient_reduction_theorem} we show the existence a point within $ \searchtrk $ that satisfies the efficiency condition \cref{efficiency}.


% \begin{boxedcomment}
% Double check that this is the same as in the masters2.tex
% \end{boxedcomment}

% The optimization program for finding this constraint is given by:
% 
% A set of $u^i, 1 \le i \le n_{I}$ infeasible points.
% A set of $v^i, 1 \le i \le n_{F}$ feasible points.
% 
% The current Lagrange polynomial $\frac 1 2 x^T Q x + b^Tx$.
% Require all infeasible point to be a distance at least $d$ from the feasible region.
% 
% 
% Find a set of planes $(n^i, b^i), 1 \le i \le n_{P}$.
% 
% Require $n_P \ge n_I$.
% 
% Let $n_I$ be the number of infeasible points
% Tolerance $\delta$
% \begin{align}
% \begin{array}{ccc}
% \min_{s, d_i \in \Rn, b_i \in \reals}	& \mfk(x) & 	\\
%  \mbox{subject to}  & d_k^T n_k \ge b_i + \trstol & \forall 1 \le k \le |\trsinfset | \\
%  & d_k^T s \le b_i &   \forall 1 \le k \le |\trsinfset |  \\
%  & \|d_k\| = 1 & \forall 1 \le k \le |\trsinfset |	\\
%  & \nabla \mcik(\xk) ^T s \le \mcik(\xk) & \forall i \in [m]\\
%  & \|s - \xk \|_{\infty} \le \dk & \\
% \end{array}
% \end{align}



\section{Recover Feasible Ellipsoid}
\label{recovering_feasiblity_section}

Although $\sampletrk$ will be feasible for small enough $\dk$, there may be some iterations in which it contains infeasible points.
When this happens, and a point we attempt to use as a sample point is infeasible, we call a \emph{RestoreFeasibility} subroutine that may decrease the trust region radius.
However, this means the previous sample points are no longer poised for the next iteration.

For convex constraints, when a sample point is found to be infeasible, it is possible to recover a feasible ellipsoid by choosing points within the convex hull of previously evaluated points.
This is discussed in \cref{convex_restoration}.
For general constraints, this is still straight forward when all the constraint values are strictly negative at the current iterate.
This means that the current iterate lies within the interior of the feasible region,
and there is gauranteed to be a feasible ellipsoid centered at the current iterate with a small enough trust region.
However, if the current iterate lies on the boundary of the feasible region, namely some constraint value is $0$ at $\xk$, finding the sample region can be much more difficult.
According to \cref{nonconvex_ellipsoid_existence}, \cref{minangleassumption_alt} ensures a feasible ellipsoid near the current iterate,
but we do not know which direction $\minangledirk$ to use.

Of course, we could approximate this with linear models of the constraints, by constructing a direction similar to $\huk$ defined in \cref{define_u}.
However, it may be necessary to shrink the trust region, and as the trust region shrinks all evaluated points become relatively further from the current iterate.
This means that we lose model information.
It is possible that as the trust region becomes small, we only have a single feasible evaluation.
The lack of information precludes several intelligent algorithms,
but it is still possible to exhaustively search over possible sample region attempting to construct a sample set.

\begin{itemize}
\item Set $R =\dk$, $M = 2*n$
\item For each of $M$ relatively equally spaced directions from $\xk$ \\
For each of \\
\item Consider ellipsoids of radius $R$
\item Decrease trust region radius
\end{itemize}


\begin{algorithm}[H]
    \caption{Recover Feasible Ellipsoid for Non convex constraints}	
    \label{general_recover}
    \begin{itemize}
        \item[\textbf{Step -}] \textbf{(Initialization)} \\
        		Set $i\gets0$, $R^{(0)} \gets \dk$, $M^{(0)} \gets 2n$.
        		
        \item[\textbf{Step 1}] \textbf{(Search for sample points)} \\
        	For each of $M^{(i)}$ relatively equally spaced directions,
        	construct an ellipsoid at distance $R^{(i)}$ from $\xk$, that satisfies \cref{ellipsoid_requirements}
        	
        \item[\textbf{Step 2}] \textbf{(Construct sample points)} \\
        	Attempt to evaluate points within the ellipsoid.
        	If all points required to construct a $\Lambda$-poised sample set are feasible, return this ellipsoid.
        \item[\textbf{Step 3}] \textbf{(Repeat)} \\
        	Set, $R^{(i+1)} = \frac 1 2 R^{(i)}$, $M^{(i+1)} \gets 2 M^{(i)}$, and $i \gets i + 1$.
    \end{itemize}
\end{algorithm}

With the suitable choices of directions, this algorithm would eventually find a feasible ellipsoid.
However, rather than discussing this further, we believe it is cleaner to assume the existence of such an algorithm.
Thus, we make \cref{restorability_assumption}.
In many situations, there may be more practical means of finding a feasible sample set.





\subsection{Goes somewhere}




To show that a feasible ellipsoid exists, we use a modification of \cref{minangleassumption_alt}.
We would have preferred to use this version, as discussed in \cref{alternative_assumptions_section},
but instead used an assumption about the models of the constraints.
However, for showing a feasible ellipsoid without reference to the current iterate, it is more appropriate to use the following version:

\begin{assumption}
\label{true_constraints_minangle_assumption}
There exists $\minangledelta > 0$ and $\minanglealpha > 0$ such that for any $x_0 \in \Omega$,
there is a unit vector $\minangledirx \in \Rn$ such that 
for any $i \in [m]$ with
\begin{align*}
\left|c_i(x_0)\right| \le \minangledelta \left\|\nabla c_i(x_0)\right\|,
\end{align*}
we also have
\begin{align*}
-\nabla c_i(x_0)^T \minangledirx \ge \minanglealpha \left\| \nabla c_i(x_0) \right\|.
\end{align*}
Without loss of generality, $\minanglealpha \le \frac 1 2$.
\end{assumption}

Note the similarity to \cref{minangleassumption_alt}:
$\zik \in B_{\infty}(\xk, \minangledelta)$ is loosely
\begin{align*}
\left\|\xk - \frac{c_i\left(\xk\right)}{\left\|\nabla c_i\left(x\right)\right\|^2} \nabla c_i\left(x\right)  - \xk\right\|_{\infty} \le \minangledelta
\end{align*}
which is
$\left|c_i(x)\right| \le \minangledelta \left\|\nabla c_i(x)\right\|_{\infty}$.

With this assumption, we are able to show a family of feasible ellipsoids about any feasible point.

\begin{theorem}
\label{nonconvex_ellipsoid_existence}
Let $\feasible$ and $f_e$ be defined by \cref{define_feasible} and \cref{define_ellipse_function} respectively.

Suppose that 
\cref{true_constraints_minangle_assumption},
\cref{bounded_gradients_lemma},
\cref{bounded_hessians_assumption},
and \cref{mingradassumption} hold.

For any $x_0 \in \feasible$,
there exists $\epsilon_{\textrm{max}} > 0$, $\beta^{\star}>0$, and rotation matrix $R$ such that for each $\epsilon < \epsilon_{\textrm{max}}$, the ellipsoid
\begin{align*}
E(\epsilon) = \left\{v \in \Rn \bigg| f_e\left(\epsilon, \epsilon, \beta^{\star}; R(v - x_0)\right) \le 0\right\}
\end{align*}
is adjacent, feasible, and non-empty according to \cref{ellipsoids_notation_definitions}
\end{theorem}

\begin{proof}

Let $\mingradepsilon$ and $\mingrad$ be defined by \cref{mingradassumption};
$\maxgrad$ by \cref{bounded_gradients_lemma};
$\minangledelta$, $\minanglealpha$ and $\minangledirx$ by \cref{true_constraints_minangle_assumption};
and $\maxhessian$ by \cref{bounded_hessians_assumption}.

Let $x_0 \in \feasible$ be arbitrary, and define
\begin{align*}
\beta^{\star} = \left(1 + \frac {\minanglealpha^2} 4\right)^{-\frac 1 2} \\
C = \left\{x_0 + v \in \Rn \bigg | v^T\minangledirx \ge \beta^{\star} \|v\| \right\} \\
R = 2\frac{(e_1 + \minangledirx)(e_1 + \minangledirx)^T}{(e_1 + \minangledirx)^T(e_1 + \minangledirx)} - \boldsymbol I \\
B = B_2\left(x_0, \min\left\{
\frac {\mingradepsilon} {\maxgrad}, \frac{\minangledelta \mingrad}{\maxgrad},
 \frac {2\minanglealpha  \min\{\mingrad, \frac{\mingradepsilon}{\minangledelta}\}}{\maxhessian\left(4 + \minanglealpha\right)}
\right\}\right)
\end{align*}
% \epsilon_{\textrm{max}} =  \\

By \cref{ellipse_in_cone}, for any $\epsilon > 0$, we have
\begin{align*}
\left\{v \in \Rn \bigg| f_e\left(\epsilon, \epsilon, \beta^{\star}; v\right) \le 0\right\}
\subseteq \left\{tv\in\Rn\bigg| e_1^T v \ge \beta^{\star},\|v\|=1, t>0\right\} \\
\
=
\left\{v\in\Rn\bigg| v^Te_1 \ge \beta^{\star}\|v\|\right\} \\
\
\Longrightarrow \left\{v \in \Rn \bigg| f_e\left(\epsilon, \epsilon, \beta^{\star}; Rv\right) \le 0\right\}
\subseteq \left\{v\in\Rn\bigg| v^T\minangledirx \ge \beta^{\star}\|v\|\right\} \\
\
\Longrightarrow \left\{v \in \Rn \bigg| f_e\left(\epsilon, \epsilon, \beta^{\star}; R(v - x_0)\right) \le 0\right\}
\subseteq \left\{x_0+ v\in\Rn\bigg| v^T\minangledirx \ge \beta^{\star}\|v\|\right\} \\
\Longrightarrow E(\epsilon) \subseteq C.
\end{align*}

Thus, if we can show that $C \cap B \subseteq \feasible$, we will have shown feasibility for sufficiently small $\epsilon_{\textrm{max}}$.
Note that \cref{ellipse_fits} also ensures adjacency, and $\beta^{\star} > 0$ ensures it is non-empty.

Let $i \in [m]$ be arbitrary.
We wish to show $c_i\left(x\right) \le 0 \; \forall x \in C \cap B$.

\paragraph*{Case 1}
Suppose that $i$ is such that $\left|c_i(x_0)\right| > \minangledelta \left\|\nabla c_i(x_0)\right\|$, and let $y \in \Rn$ with $c_i(y) = 0$ be arbitrary.
By \cref{bounded_gradients_lemma}, there exists $\maxgrad > 0$ such that
\begin{align}
\label{fufeeq1}
0 = c_i(y) \le c_i(x_0) + \|y - x\| \maxgrad \Longrightarrow \|y - x_0\| \ge \frac{-c_i(x_0)}{\maxgrad}.
\end{align}
By \cref{mingradassumption}, there are constants $\mingradepsilon$ and $\mingrad$ such that if $-c_i(x_0) = \left|c_i(x_0)\right| \le \mingradepsilon$, then 
$\| \nabla c_i(x_0) \| \ge \mingrad$.
If $-c_i(x_0) > \mingradepsilon$, \cref{fufeeq1} becomes $\|y - x_0\| > \frac{\mingradepsilon}{\maxgrad}$, otherwise it is
\begin{align*}
\|y - x_0\| \ge \frac{\minangledelta \left\|\nabla c_i(x_0)\right\|}{\maxgrad} \ge \frac{\minangledelta \mingrad}{\maxgrad}.
\end{align*}
Because one of these cases must be true, we see
\begin{align*}
\|y - x_0\| \ge \frac 1 {\maxgrad}\min\left\{\mingradepsilon, \minangledelta \mingrad\right\} \Longrightarrow y \not \in B.
\end{align*}

\paragraph*{Case 2}
Now, suppose that $i$ is such that 
$\left|c_i(x_0)\right| \le \minangledelta \left\|\nabla c_i(x_0)\right\|$.
Then by \cref{true_constraints_minangle_assumption}, there exists $\minangledirx$ with
$-\nabla c_i(x_0)^T\minangledirx \ge \minanglealpha \left\| \nabla c_i(x_0) \right\|.$

Now, if $\mingradepsilon < -c_i(x_0)$, then
$\mingradepsilon < -c_i(x_0) \le \minangledelta \left\|\nabla c_i(x_0)\right\|$
so that 
$\left\|\nabla c_i(x_0)\right\| \ge \frac{\mingradepsilon}{\minangledelta}$.
Otherwise, if $-c_i(x_0) \le \mingradepsilon$, \cref{mingradassumption} implies $\left\|\nabla c_i(x_0)\right\| \ge \mingrad$.
In either case, \begin{align}
\label{fufeeq2}
\left\|\nabla c_i(x_0)\right\| \ge \min\left\{\mingrad, \frac{\mingradepsilon}{\minangledelta}\right\}.
\end{align}

By \cref{bounded_hessians_assumption}, there exists $\maxhessian > 0$ such that for any $y \in C \cap B$, we have
\begin{align*}
c(y) \le c(x_0) + \nabla c(x_0)^T (y - x_0) + \maxhessian \|y - x_0\|^2.
\end{align*}

Notice with a change of variables $v \gets y-x_0$ and $t\minangledirx + s \gets v$, and we have
\begin{align*}
C = \left\{x_0 + v \in \Rn \bigg | v^T\minangledirx \ge \beta^{\star} \|v\| \right\}. \\
= \left\{x_0 + v \in \Rn \bigg | v^T\minangledirx \ge \left(1 + \frac {\minanglealpha^2} 4\right)^{-\frac 1 2} \|v\| \right\} \\
= \left\{x_0 + v \in \Rn \bigg | \|v\|^2 \le \left[1 + \frac {\minanglealpha^2} 4\right]\left(v^T\minangledirx\right)^2 \right\} \\
= \left\{x_0 + v \in \Rn \bigg | \|v\|^2 - 2 \left(v^T\minangledirx\right)^2 + \left(v^T\minangledirx\right)^2 \le \frac 1 4\left(v^T\minangledirx\right)^2 \minanglealpha^2 \right\} \\
= \left\{x_0 + v \in \Rn \bigg |  t =  v^T\minangledirx, \|v\|^2 - 2 t v^T\minangledirx + t^2\left\|\minangledirx\right\|^2 \le \frac 1 4t^2 \minanglealpha^2 \right\} \\
= \left\{x_0 + v \in \Rn \bigg | \left(v - t\minangledirx\right)^T \minangledirx = 0, \left\|v - t\minangledirx\right\| \le \frac 1 2 \minanglealpha t \right\} \\
= \left\{x_0 + t \minangledirx + s \in \Rn \bigg | s^T \minangledirx = 0, \|s\| \le \frac 1 2 \minanglealpha t \right\}.
\end{align*}
Now, $s^T \minangledirk = 0$ and $x_0 + v \in B$ imply that
\begin{align*}
t = \sqrt{\|v\|^2  - \|s\|^2} \le \|v\| \le  \frac {2\minanglealpha  \min\{\mingrad, \frac{\mingradepsilon}{\minangledelta}\}}{\maxhessian\left(4 + \minanglealpha\right)}.
\end{align*}
% We can then let $0 < t \le \frac {2\minanglealpha  \min\{\mingrad, \frac{\mingradepsilon}{\minangledelta}\}}{\maxhessian\left(4 + \minanglealpha\right)}$ be arbitrary,
% and suppose that $s \in \Rn$ satisfies $s^T\minangledirx = 0$, $\|s\|\le \frac 1 2 \minanglealpha t$.
We can multiply
\begin{align*}
t \le \frac {2\minanglealpha  \min\{\mingrad, \frac{\mingradepsilon}{\minangledelta}\}}{\maxhessian\left(4 + \minanglealpha\right)}
\Longrightarrow 0 \ge -2\minanglealpha \left\|\nabla c_i(x)\right\| +  \maxhessian\left(4 + \minanglealpha\right) t
\end{align*}
by $\frac t 4$ to find
\begin{align*}
0 \ge \left(-1 + \frac 1 2\right) \minanglealpha \left\|\nabla c_i(x)\right\| t +  \maxhessian\left(1 + \frac 1 4 \minanglealpha\right) t^2 \\
\ge 0 -  t\minanglealpha \left\| \nabla c_i(x) \right\| + \left\| \nabla c(x)\right\| \left\|s\right\| + \maxhessian\left(t^2 + \|s\|^2\right) \\
\ge c(x) + \nabla c(x)^T (t \minangledirx + s) + \maxhessian \|t \minangledirx + s\|^2 \\
\ge c(x + t \minangledirx + s) = c(y).
\end{align*}


\end{proof}

\section{Convergent Algorithm}
\label{the_algroithm_section}

We can now state the algorithm.
The algorithm not only assumes the customary initial iterate $ \xinit $ with 
\begin{align}
\label{initial_point_is_feasible}
c\left(\xinit\right) < 0 \Longrightarrow \xinit \in \feasible.
\end{align}
but also an initial feasible ellipsoid
\begin{align}
\left\{x \in\Rn | \left(x - \xinit\right)^T Q^{(0)} \left(x - \xinit\right) \le \delta_0 \right\} \subseteq B_{\infty}\left(\xinit, \Delta_{0}\right) \cap \feasible \label{initial_ellipsoid}
\end{align}
to construct the first model.


% In addition to decreasing the trust region to innacurate trial steps, 
% it must also be decreased when the criticallity measure is small relative to the trust region.
% To accomplish this, we introduce constants
We introduce constants
\begin{align}
0 < \kappa_{\chi} \label{define_kappa_chi} \\
0 < p_{\Delta} < \min\{p_{\alpha}, p_{\beta}\} \le 1 \label{define_p_delta} 
\end{align}
and check
\begin{align}
\kappa_{\chi} \dk^{p_{\Delta}} \le \chik \label{criticallity_check}
\end{align}
to ensure the trust region radius does not remain large while approaching criticallity.
Here,
$p_{\alpha}$ and $p_{\beta}$ is defined by \cref{define_abpab},
while $\chik$ is defined by \cref{define_criticality_measure}.



The algorithm is as follows:

\begin{algorithm}[H]
    \caption{Always-feasible Constrained Derivative Free Algorithm}	
    \label{constrained_dfo}
    \begin{itemize}
        \item[\textbf{Step 0}] \textbf{(Initialization)} \\
        	Initialize user supplied constants $\tolcrit, \tolrad$ by \cref{define_algorithm_tolerances};
        	$\gammasm$, $\gammabi$ by \cref{define_the_gammas};
        	$\omegadec$, $\omegainc$ by \cref{define_the_omegas};
        	$\alpha$, $\beta$, $p_{\alpha}$, and $p_{\beta}$ by \cref{define_abpab};
			$\kappa_{\chi}$ by \cref{define_kappa_chi};
			$\ximin$ by \cref{define_ximin};
			and $p_{\Delta}$ by \cref{define_p_delta}. \\
        	Initialize iteration counter: $k=0$, initial iterate $\xinit \in \feasible$, trust region radius $\Delta_0 > 0$, and feasible ellipsoid $T^{(0)}_{\textrm{interp}}$ as in \cref{initial_ellipsoid}.
            
        \item[\textbf{Step 1}] \textbf{(Construct the models)} \\
%         by \cref{define_ellipsek} if $\activeconstraintsk \ne \emptyset$ and \cref{define_trivial_ellipsek} if $\activeconstraintsk =\emptyset$.
        Call \cref{model_improving_algorithm} with respect to $\sampletrk$ to ensure the current sample set $Y^{(k)}$ satisfies \cref{accuracy}.
        Construct $\mfk$ and $\mcik$ as described in \cref{reg}. \\
        While constructing the sample set, if some point within $\sampletrk$ is not feasible, or $\sampletrk = \emptyset$ 
        run the \emph{RestoreFeasibility} subroutine to construct a new $\sampletrkpo$ and $\dkpo<\omegadec \dk$, $k \gets k + 1$ and go to Step 1.
        
        \item[\textbf{Step 2}] \textbf{(Check stopping criteria)} \\
        	Compute $\activeconstraintsk$ by \cref{define_activeconstraints} and $\zik$, $\wik$ by \cref{define_z} and \cref{define_w} for each $i \in \activeconstraintsk$.
            Compute $\chi_k$ as in \cref{define_criticality_measure}. \begin{itemize}
                \item[] If $ \chik < \tau_{\xi} $ and $\dk <\tau_{\Delta}$ then return $\xk$ as the solution.
                \item[] Otherwise, if \cref{criticallity_check} is not satisfied, then \\
                $\Delta_{k+1} \gets \omegadec\dk$, 
                $x^{(k+1)} \gets \xk$,
                $k \gets k+1$ and go to Step 1.
            \end{itemize}
		
		\end{itemize}
		\end{algorithm}
		
		\vspace{10cm}
		\newpage
		
		\begin{algorithm}[H]
		\begin{itemize}
            
        \item[\textbf{Step 3}] \textbf{(Solve the trust region subproblem)} \\
        	Construct $\huk$, $\thetamink$, $\bsk$, $\fik$, and $\capcones$ according to
        	\cref{define_u}, \cref{define_thetamink}, \cref{define_bsk}, \cref{define_fik}, \cref{define_capcones}. \\
        	Use \cref{capcones_tr_subproblem} to compute the trial point $\sk$.
        	Evaluate the objective and constraints at $\sk$.
        	If $\sk \not \in \feasible$, reduce the trust region $\Delta_{k+1} = \omegadec\dk$, $k \gets k+1$ and go to Step 1.
            
        \item[\textbf{Step 4}] \textbf{(Test for improvement)} \\
            Evaluate $f\left(\xk + \sk\right)$ and evaluate $\rk$ as in \cref{define_rhok} \begin{itemize}
                \item[] If $\rk < \gammasm$ then $\xkpo=\xk$ (reject) and $\Delta_{k+1} = \omegadec\dk$
                \item[] If $\rk \ge \gammasm$ and $\rk < \gammabi$ then $\xkpo=\xk+\sk$ (accept), $\Delta_{k+1} = \omegadec\dk$
                \item[] If $\rk > \gammabi$ then $\xkpo=\xk+\sk$ (accept), $\Delta_{k+1} = \omegainc\dk$
                % and either increase the radius or decrease if $\nabla \mfk(\xk)$ is small
            \end{itemize}
            
        \item[\textbf{Step 5}] \textbf{(Construct next sample region)} \\
        	Compute $\rotk$, according to \cref{define_rotation}.
%         	Approximate $\activeconstraintskpo$ with $\approxactiveconstraintskpo$ defined by \cref{define_active_approximation} by checking if any $\zik \in B_{\infty}\left(\xkpo, \dkpo\right)$.
        	If $\activeconstraintskpo \ne \emptyset$, define $\sampletrkpo$ according to \cref{define_ellipsek}.
        	Otherwise, use \cref{define_trivial_ellipsek}. \\
            $k \gets k+1$ and go to Step 1.
    \end{itemize}
\end{algorithm}







\begin{boxedcomment}
Don't always increase...
\end{boxedcomment}


\color{red}

We take a moment to make some observations about the algorithm.


Somewhere should I say that there is only a finite number of times that the restore subroutine is called?


Because $\dk \le \dacc$, by \cref{sampletrk_is_nonempty} we have $\sampletrk \ne \emptyset$.
Because $\dk \le \dfeas$, by \cref{ellipsoid_is_feasible} we have $\sampletrk \subseteq \feasible$ and $\searchtrk \subseteq \feasible$.
These along with \cref{sr_chi_big_enough} ensure that the algorithm reaches Step 3.
This is because the only way to quit iteration $k$ without reaching Step 3 is
\begin{itemize}
\item have $\sampletrk = \emptyset$
\item have $\sampletrk \not \subseteq \feasible$
\item have $\searchtrk \not \subseteq \feasible$
\item have \cref{criticallity_check} not satisfied.
\end{itemize}
\color{black}

\section{Convergence Analysis}
\label{convex_convergence_analysis}

Let $\left\{\xk\right\}$ be the sequence of iterates generated by \cref{constrained_dfo}.
For convex constraints, we can let $\chi\left(x\right) = \left\|x - \textrm{Proj}_{\feasible}\left(x- \nabla f\left(x\right)\right)\right\|$ be the criticallity measure for \cref{the_dfo_problem}, 
For general constraints, this projection is not well defined.
However, to show convergence to a first order critical point, we can project onto the linearization of the constraints.
Namely, for any $x \in \feasible$, if we define $F_c(x) = \left\{y \in \Rn \big | c_i(x) + \nabla c_i(x)^T(y - x) \le 0 \; \forall i \in [m] \right\}$, 
then we can let the criticallity measure be
$\chi\left(x\right) = \left\|x - \textrm{Proj}_{F_c(x)}\left(x- \nabla f\left(x\right)\right)\right\|$.
In this section we prove that under appropriate conditions, the criticality measure $\chi\left(\xk\right)$ converges to $0$ as $k\to\infty$.

For much of our analysis, we work with the model criticality measure
\begin{align*}
\chi_m\left(x\right) = \left\|x - \textrm{Proj}_{\feasiblek}\left(x- \nabla \mfk\left(x\right)\right)\right\|.
\end{align*}
in accordance with \cref{define_criticality_measure}: $\chik = \chi_m\left(\xk\right)$.
$\chik$ depends on the current approximation of the constraints, which change each iteration.
Thus, we show that the model criticallity measure converges to the true criticallity measure
\begin{align}
\label{verbiage_hoffman_use_one}
\chik \approx \chi (\xk).
\end{align}
For some results, we also show that the criticallity \cref{define_criticality_measure} measure converges across iterations:
\begin{align}
\label{verbiage_hoffman_use_two}
\chik \approx \chil.
\end{align}


Our convergence analysis follows closely that of \cite{Conejo:2013:GCT:2620806.2621814}.
% To use their analysis, we show that the accuracy and efficiency assumptions required for their proof hold, 
% this is done within \cref{ellipsoidal_lambda} and \cref{sufficient_reduction_section} respectively.
We summarize the steps of the proof here:
\begin{itemize}
\item Show that the accuracy condition \cref{accuracy} is satisfied. \\
This is done within \cref{accuracy_section}. 
To do this, we must show our construction of $\sampletrk$ satisfies the conditioned requirement defined by \cref{ellipsoids_notation_definitions}.
This is done within \cref{bounded_condition_numbers_section}.
\item Show that the efficiency condition \cref{efficiency} is satisfied. \\
This is the topic of \cref{sufficient_reduction_section}.
We use a slightly weaker efficiency condition, as stated in \cref{sufficient_reduction_theorem}.
\item Show $\liminf_{k\to\infty} \chik \to 0$. \\
This is done within \cref{initial_convergence_results}, and is a fairly straight forward use of the analysis within \cite{Conejo:2013:GCT:2620806.2621814}.
\item Show $\lim_{k\to\infty} \chik \to 0$ whenever $\gammabi > 0$. \\
This is done within \cref{limit_of_criticallity_to_zero}.
This result relies on \cref{verbiage_hoffman_use_one}, which in turn, depends on results within \cref{bounding_the_projection_section}.
% the Hoffman constant of the constraint's model's Jacobians.
% This is done within \cref{bounding_the_projection_section}.
This is complicated by the fact that we do not assume bounded level sets, so we do not have a gaurantee that the sequence $\left\{\xk\right\}$ is Cauchy.
% Thus, we introduce \cref{criteria_from_contradiction}, which
% \begin{itemize}
% \item is sufficient to prove \cref{verbiage_hoffman_use_two},
% \item follows from a false assumption within a proof of contradiction \cref{the_contradiction},
% \item would be true under the stronger assumption of bounded level sets.
% \end{itemize}
% (For a discussion on simplifying the analysis with bounded level sets see \cref{bounded_level_sets_section}).
% \begin{itemize}
% \item This is one uses of the hoffman bound
% \item motivate criteria mention contradiction
% \end{itemize}
\item Show the limit of true criticallity measure is zero \\
This is done within \cref{limit_of_true_criticallity}.
This relies on \cref{verbiage_hoffman_use_two}.
% , which in turn relies on bounding the Hoffman constant of the Jacobians of the constraints.
Because of the similarity to \cref{verbiage_hoffman_use_one}, this is done in parallel within \cref{bounding_the_projection_section}.
% bounding the constraint's model's Jacobian's Hoffman's constant in \cref{bounding_the_projection_section}.
\end{itemize}

Throughout this section, we assume that 
$\left\{\xk\right\}$,
$\left\{\dk\right\}$,
$\left\{m_{c_i}\right\}$,
and
$\left\{m_{f}\right\}$
are generated by 
\cref{constrained_dfo}.
Other quantities such as $\activeconstraintsk$, $\huk$, $\thetamink$, $\qk$, $\ck$, and $\sdk$ 
that are generated by the conservative ellipsoid method described in
\cref{ellipsoid_requirements} and \cref{constructing_and_analysizing_conservative_ellipsoid}
include references to their definitions when used.


% \subsection{Restoration Assumption}


\subsection{Assumptions}
Let $\domain$ be some open region containing $\feasible$ defined in \cref{define_feasible}.
We make the following assumptions for the results in this section.


% \paragraph{Bounded functions}
% The following assumptions let us know that the functions $f$ and $c_i$ for each $i \in [m]$ are bounded functions.

\begin{assumption}
\label{bounded_below_assumption}
The function $f$ is bounded below in $ \domain $. That is, there exists $\fmin \in \reals$ such that
\begin{align*}
f(x) \ge \fmin \quad  \forall x \in \domain.
\end{align*}
\end{assumption}

% \color{red}
% \begin{assumption}
% \label{maximum_constraint_value_lemma}
% % Suppose that \cref{constraints_are_convex}, \cref{bounded_hessians_assumption}, \cref{max_norm_assumption}, and \cref{lipschitz_gradients_assumption} hold.
% The functions $c_i$ for $i \in [m]$ are bounded below in $ \domain $.
% That is, there exists $M_c>0$ such that
% \begin{align*}
% c_i(x) \ge -M_c \quad \forall i \in [m], x \in \domain.
% \end{align*}
% \end{assumption}
% \color{black}

\begin{assumption}
\label{bounded_gradients_lemma}
% Suppose \cref{max_norm_assumption} and \cref{lipschitz_gradients_assumption} hold.
The functions $f$ and $c_i$ for all $ i \in [m]$ have bounded gradient in $ \domain $.
That is, there exists $\maxgrad > 0$ such that
\begin{align*}
\|\gradf(x)\| \le \maxgrad \quad  \forall x \in \domain \\
\|\nabla c_i(x)\| \le \maxgrad \quad  \forall x \in \domain, \quad \forall i \in [m].
\end{align*}
\end{assumption}

\begin{assumption}
\label{bounded_hessians_assumption}
Each function $c_i$ for $i \in [m]$ and $f$ has bounded Hessian. That is, there exists $ \maxhessian > 0$ such that:
\begin{align*}
\|\nabla^2 f(x) \| \le \maxhessian \quad \forall x \in \domain \\
\|\nabla^2 c_i(x) \| \le \maxhessian \quad \forall x \in \domain, \forall i \in [m]
\end{align*}
\end{assumption}

% \begin{assumption}
% \label{bounded_objective_hessians_assumption}
% The matrices $\hk$ are uniformly bounded, that is, there exists a constant $ \maxhessian \ge 1 $ such that 
% \begin{align}
% \|\hk\| \le \maxhessian \quad \forall k \ge 0.
% \end{align}
% \end{assumption}

% \color{red}
% We believe that the following assumption is stronger than required.
% This assumption is used while bounding differences in criticality measure across iterations, 
% so that it we would likely only need the intersection of $\feasiblek$ and the half space of vectors whose dot product with the negative gradient of the objective is positive to be bounded.
% \begin{assumption}
% \label{max_norm_assumption}
% The set $\feasible$ is bounded. That is, there exists $\maxnorm > 0$ such that
% \begin{align*}
% \|x\| \le \maxnorm \quad \forall x \in \feasible.
% \end{align*}
% \end{assumption}
% \color{black}


% \paragraph{All functions are smooth}
% The following assumptions let us know that the functions $f$ and $c_i$ for each $i \in [m]$ are smooth functions.

\begin{assumption}
\label{lipschitz_gradients_assumption}
The functions $c_i$ for all $i \in [m]$ and $f$ are differentiable and their gradients are Lipschitz continuous with constant $\lipgrad > 0$ in $ \domain $:
\begin{align*}
\|\gradf(x) - \gradf(y)\| \le \lipgrad \|x - y\| \quad \forall x,y \in \domain, \\
\|\nabla c_i(x) - \nabla c_i(y)\| \le \lipgrad \|x - y\| \quad \forall x,y \in \domain, \forall i \in [m].
\end{align*}
\end{assumption}


\begin{assumption}
\label{lipschitz_hessians_assumption}
The functions $c_i$ for all $i \in [m]$ and $f$ are twice differentiable and their Hessians are Lipschitz continuous with constant $\liphess > 0$ in $ \domain $:
\begin{align*}
\|\nabla^2 f(x) - \nabla^2 f(y)\| \le \liphess \|x - y\| \quad \forall x,y \in \domain, \\
\|\nabla^2 c_i(x) - \nabla^2 c_i(y)\| \le \liphess \|x - y\| \quad \forall x,y \in \domain, \forall i \in [m] .
\end{align*}
\end{assumption}


\paragraph*{Regularity Assumptions}

% We use the following assumptions as our regularity assumptions.

The following was inspired by the Mangasarian Fromovitz constraint qualifications with only inequality constraints.
It provides a uniform bound across all points, rather than at just a critical point.

% For an alternative to this assumption, please see \cref{alternative_assumptions_section}.
% 
% \color{red}
% \begin{assumption}
% \label{minangleassumption}
% Define $\activeconstraintsk$ by \cref{define_activeconstraints}.
% There exists $\minangledelta > 0$ and $\minanglealpha > 0$ such that for each $k \in \naturals$ with $\dk \le \minangledelta$ there exists
% a unit vector $\minangledirk \in \Rn$
% such that
% \begin{align*}
% -\frac {\gmcik}{\left\|\gmcik\right\|} ^T\minangledirk \ge \minanglealpha \quad \forall i \in \activeconstraintsk.
% \end{align*}
% Without loss of generality, $\minanglealpha \le \frac 1 2$.
% \end{assumption}
% \color{black}

Remember that $\zik$ is defined by \cref{define_z}.
\begin{assumption}
\label{minangleassumption_alt}
There exists $\minangledelta > 0$ and $\minanglealpha > 0$ such that for each $k \in \naturals$ there exists
a unit vector $\minangledirk \in \Rn$
such that if $\dk \le \minangledelta$ and
$i$ is such that $\zik \in B_{\infty}(\xk, \minangledelta)$
then
\begin{align*}
-\frac {\gmcik}{\left\|\gmcik\right\|} ^T\minangledirk \ge \minanglealpha.
\end{align*}
Without loss of generality, $\minanglealpha \le \frac 1 2$.
\end{assumption}



\begin{boxedcomment}
Use continuity to make $c_i(x) = 0$, then....
\end{boxedcomment}

\begin{assumption}
\label{mingradassumption}
There exist $\mingradepsilon > 0$ and $\mingrad > 0$ such that for each $x \in \domain$, if $\left|c_i(x)\right| \le \mingradepsilon$, then
\begin{align*}
\| \nabla c_i(x) \| \ge \mingrad.
\end{align*}
% \| \nabla c_i(x) \| \ge \mingrad \quad \forall i \in \epsactive(x; \mingradepsilon).
\end{assumption}


\paragraph*{Restoration}


The following assumption is motivated in \cref{recovering_feasiblity_section}.
In \cref{convex_restoration}, we show how to satisfy the assumption for convex constraints.

\begin{assumption}
\label{restorability_assumption}
There exists a subroutine \emph{RestoreFeasibility} that accepts a feasible point $x \in \feasible$ and a maximum radius $\dmax$ to produce
a new trust region radius $\Delta < \dmax$ and ellipsoid $T_{\textrm{interp}}$ that is trusted for 
$x, \Delta$, feasible, adjacent to $x$, and non-empty according to \cref{ellipsoids_notation_definitions}.
\end{assumption}




% \paragraph{}
% 
% 
% \begin{lemma}
% Suppose that \cref{minangleassumption_alt} holds.
% If $\dk \le \Delta_{\textrm{a}}$, then
% \end{lemma}
% \begin{proof}
% 
% By \cref{minangleassumption_alt}, there exists an $\epsilon > 0$ and a $\minanglealpha$ such that for any $i \in \epsactive(x; \epsilon)$, we have
% \begin{align*}
% \nabla c_i(x)^T\minanglediralt(x) > \minanglealpha.
% \end{align*}
% Suppose that 
% 
% \begin{align*}
% \kappa_g \dk^2 \le \minanglealpha
% \end{align*}
% 
% \begin{align*}
% \minanglealpha < \nabla c_i(x)^T\minanglediralt(x) =
% \left(\nabla \mcik(x) + \kappa_g \dk^2 \sqrt{\sigma_{\textrm{max}}}\nu\right)^T\minanglediralt(x) \\
% \minanglealpha - \kappa_g \dk^2 \sqrt{\sigma_{\textrm{max}}}\nu^T\minanglediralt(x) < \nabla \mcik(x)^T\minanglediralt(x) \\ 
% \minanglealpha\left(1 - \sqrt{\sigma_{\textrm{max}}}\right) < \nabla \mcik(x)^T\minanglediralt(x) \\ 
% \end{align*}
% 
% \begin{align*}
% \nabla c_i(x) =  \\
% \dk \le \Delta_{\textrm{a}}
% \end{align*}
% 
% Suppose that $i \in \epsactive(x; \epsilon)$.
% Then, 
% Then there e
% 
% There exists a unit vector $\nu \in \Rn$, with $0 \le \|\nu\| \le 1$ such that 
% 
% 
% 
% \begin{align*}
% \nabla c_i(x)^T\minanglediralt(x) > \minanglealpha\quad \forall i \in  \\
% \nabla c_i(x) = \nabla \mcik(x) + \kappa_g \dk^2 \sqrt{\sigma_{\textrm{max}}}\nu \\
% \dk \le \Delta_{\textrm{a}}
% \end{align*}
% 
% \end{proof}




% \paragraph{Simple Theorems}

% \begin{proof}
% \begin{boxedcomment}
% The result follows from these being continuous functions over compact sets.
% \end{boxedcomment}
% 
% We are given a $\xinit \in \feasible$. Let $\maxgrad = 2\lipgrad \maxnorm + \max\left\{|\gradf\left(\xinit\right)|, \max_{i \in [m]}\left\{\nabla c_i\left(\xinit\right)|\right\} \right\}$.
% Thus, for any $x \in \domain$,
% \begin{align*}
% \gradf(x) \le \left|\gradf\left(\xinit\right)\right| + \lipgrad \left\|x - 0 - \left(\xinit - 0\right)\right\| \le \left|\gradf\left(\xinit\right)\right| + 2\lipgrad \maxnorm \le \maxgrad.
% \end{align*}
% The same applies to each $c_i$.
% \end{proof}


% \begin{proof}
% \begin{boxedcomment}
% The result follows from these being continuous functions over compact sets.
% \end{boxedcomment}
% 
% Let $M_c = -c_i\left(\xinit\right) + \maxgrad\maxnorm + \maxhessian \maxnorm^2$.
% Because $\xinit \in\feasible$, $M_c > 0$.
% But, for all $i \in [m]$ and all $x \in \feasible$ we have by 
% \cref{constraints_are_convex} and \cref{bounded_hessians_assumption}
% \begin{align*}
% c_i(x) \ge c_i\left(\xinit\right) + \left(\nabla c_i\left(\xinit\right)\right)^T\left(x - \xinit\right) - \maxhessian \left\|x - \xinit\right\|^2
% \end{align*}
% and by \cref{bounded_gradients_lemma}, and \cref{max_norm_assumption}
% \begin{align*}
% c_i(x) \ge c_i\left(\xinit\right) - \maxgrad\maxnorm - \maxhessian \maxnorm^2 = -M_c.
% \end{align*}
% \end{proof}



\subsection{Bounded Condition Numbers}
\label{bounded_condition_numbers_section}
In this section, we show that the condition numbers $\condition\left(\qk\right)$ are bounded.
This is required to show the accuracy condition \cref{accuracy}, because this condition number determines the sample sets poisedness, as discussed in \cref{geometry}.
% The accuracy condition dependThis condition number influences model's accuracy. 

\begin{lemma}
\label[lemma]{theta_min_is_bounded}
Let $\minangledelta$ and $\minanglealpha$ be defined by \cref{minangleassumption_alt}, and
define $\thetamink$ by \cref{define_thetamink}.

\input{assumptions.d/theta_min_is_bounded}

If $\dk \le \minangledelta$, then $\thetamink \ge \minanglealpha$.
\end{lemma}

\begin{proof}
Let $\activeconstraintsk$ be defined by \cref{define_activeconstraints}.
If $\activeconstraintsk = \emptyset$, then $\thetamink = 1$.
Otherwise, by \cref{minangleassumption_alt}, there is a $\minangledirk$ such that 
$-\minangledirk^T\frac{\gmcik}{\|\gmcik\|} \ge \minanglealpha$ for any $i \in \activeconstraintsk$.
But, by definition of $\huk$ in \cref{define_u}:
$\huk = -\argmin_{\|u\| = 1} \max_{i \in \activeconstraintsk} u^T\frac{\gmcik}{\left\|\gmcik\right\|}$
so that for all $i \in \activeconstraintsk$,
\begin{align*}
-\left(\huk\right)^T\frac{\gmcik}{\|\gmcik\|}  \ge -\left(\minangledirk\right)^T\frac{\gmcik}{\|\gmcik\|} \ge \minanglealpha.
\end{align*}
Therefore, $\thetamink \ge \minanglealpha$.
\end{proof}

\begin{lemma}
\label[lemma]{bounded_condition_numbers}
Let $\activeconstraintsk$ be defined by \cref{define_activeconstraints};
$\qkpo$ be defined by \cref{define_trivial_ellipsek} when $\activeconstraintsk = \emptyset$ and \cref{conservative_ellipsoid_details} otherwise;
and $\minanglealpha$ be defined by \cref{minangleassumption_alt}.

\input{assumptions.d/bounded_condition_numbers}

% Then the sequence $\left(\qk, \ck, \sk\right)$ is conditioned as defined by \cref{ellipsoids_notation_definitions}.
There exists a $\dacc > 0$, such that $\condition\left(\qkpo\right) \le {12}\left(\minanglealpha\right)^{-2}$ whenever $\dk \le \dacc$.
\end{lemma}
\begin{proof}
If $\activeconstraintsk = \emptyset$, then 
% \cref{define_suitable_condition_numbers} is satisfied because 
$\condition \left(\qkpo\right) = 1$.
If $\activeconstraintsk \ne \emptyset$, then result follows directly from \cref{theta_min_is_bounded} and \cref{boundbeta} by letting
\begin{align}
\dacc = \min\left\{\minangledelta, \frac 1 {4 \beta} \left(\minanglealpha \right)^2 \right\} \label{define_delta_accuracy}
\end{align}
where $\minangledelta$ is defined by \cref{minangleassumption_alt}.
\end{proof}


\begin{lemma}
\label[lemma]{sampletrk_is_nonempty}
Let $\dacc$ and $\fcki$ be defined by \cref{define_delta_accuracy} and \cref{define_inner_cone}.

\input{assumptions.d/sampletrk_is_nonempty}

If $\dk \le \dacc$, then the tuple $\left(\qk, \ck, \sdk\right)$ defined in \cref{define_ellipsek}
is non-empty according to \cref{ellipsoids_notation_definitions}, and $\fcki \ne \emptyset$.
\end{lemma}
\begin{proof}
Let $\minanglealpha$ be defined by \cref{minangleassumption_alt}.
\cref{theta_min_is_bounded} and \cref{boundbsk} tell us that 
if $\dk \le \dacc$, then $\thetamink \ge \minanglealpha$, 
so that the definitions \cref{define_bsk} and \cref{define_bs} imply
$\bsk < 1 \Longrightarrow \frac{\bsk}{1 - \bsk} > 0$.
Thus, the set of directions in \cref{define_inner_cone} is non empty, and
$\fcki \ne \emptyset$.
\end{proof}


\subsection{Accuracy}
\label{accuracy_section}


\begin{lemma}
\label[lemma]{accuracy_is_satisfied_lemma}
Suppose that $\qkmo$, $\ckmo$, and $\sdkmo$ are non-empty according to \cref{ellipsoids_notation_definitions}.

\input{assumptions.d/accuracy_is_satisfied_lemma}


Fix an iterate $k$, and suppose that the sample points chosen for iteration $k$ are feasible.
There exist $\kappa_g>0$ and $\kappa_h>0$ such that
\begin{align*}
\|\gradf(\xk) - \nabla \mfk(\xk) \| \le \kappa_g \sqrt{\condition \left(\qkmo\right)} \dk^2, \\
\|\nabla^2 f(\xk) - \hk \| \le \kappa_h \condition \left(\qkmo\right) \dk,  \\
\|\nabla c_i(\xk) - \gmcik \| \le \kappa_g \sqrt{\condition \left(\qkmo\right)} \dk^2 \quad \forall i \in [m],\\
\textrm{and} \quad \|\nabla^2 c_i(\xk) - \nabla^2 m_{c_i}^{(k)}(\xk) \| \le \kappa_h \condition \left(\qkmo\right) \dk \quad \forall i \in [m]. \\
\end{align*}
\end{lemma}

\begin{proof}
Give $\qkmo = LD^2L^T$ its eigen-decomposition, and define $\delta = \max_{x \in \unshiftedellipsoid} \|x - \ckmo\|$, as in \cref{shifted_ellipsoid}.
Then the transformation $T(x) = \delta D L^T(x - \ckmo)$ maps $\unshiftedellipsoid$ to the $\delta$ ball.
As also described in \cref{shifted_ellipsoid}, we create transformed functions
\begin{align*}
\begin{array}{ccc}
\hat {m}_f(u) = m_f(T^{-1}(u)),&  \hat f (u) = f(T^{-1}(u)), &\\
\hat {m}_{c_i}(u) = m_{c_i}(T^{-1}(u)), &  \quad \textrm{and} \quad \hat c_i (u) = c(T^{-1}(u))& \forall i \in [m]
\end{array}
\end{align*}
After using \cref{model_improving_algorithm} to choose sample points, we know by \cref{quadratic_errors} that
there exist constants $\kappa_f, \kappa_g, \kappa_h$ such that for all $u \in B_2(0, \delta)$ and all $i \in [m]$:
\begin{align*}
\begin{array}{cc}
\left| \hat {f}\left(u\right) -  \hat{m}_f\left(u\right) \right|\le \kappa_f \delta^3, &
\left| \hat {{c_i}}\left(u\right) -  \hat{m}_{c_i}\left(u\right) \right|\le \kappa_f \delta^3, \\
\left\|\nabla \hat {f}\left(u\right) - \nabla \hat{m}_f\left(u\right) \right\|\le \kappa_g \delta^2, &
\left\|\nabla \hat {{c_i}}\left(u\right) - \nabla \hat{m}_{c_i}\left(u\right) \right\|\le \kappa_g \delta^2, \\
\left\|\nabla^2 \hat {f}\left(u\right) - \nabla^2 \hat{m}_f\left(u\right) \right\|\le \kappa_h \delta, &
\quad \textrm{and} \quad \left\|\nabla^2 \hat {{c_i}}\left(u\right) - \nabla^2 \hat{m}_{c_i}\left(u\right) \right\|\le \kappa_h \delta.
\end{array}
\end{align*}
By \cref{bounded_hessians_assumption} and \cref{lipschitz_hessians_assumption}, we know that the assumptions for \cref{change_radius} hold.
Thus, there are constants $\Lambda_2, \Lambda_3$ such that for all $u \in B_2(0, 2\delta)$ and $i \in [m]$:
\begin{align*}
\begin{array}{cc}
\left\|\nabla \hat {f}\left(u\right) - \nabla \hat{m}_f\left(u\right) \right\|\le 2\Lambda_2 \delta^2 = {\kappa'}_g\delta^2, &
\left\|\nabla \hat {c_i}\left(u\right) - \nabla \hat{m}_{c_i}\left(u\right) \right\|\le {\kappa'}_g\delta^2 \\
\left\|\nabla^2 \hat {f}\left(u\right) - \nabla^2 \hat{m}_f\left(u\right) \right\|\le 2\Lambda_3 \delta = {\kappa'}_h\delta^2, &
\quad \textrm{and} \quad \left\|\nabla^2 \hat {c_i}\left(u\right) - \nabla^2 \hat{m}_{c_i}\left(u\right) \right\|\le {\kappa'}_h\delta^2 \\
\end{array}
\end{align*}
where $\kappa_{g}' = 2 \Lambda_2$ and $\kappa_{h}' = 2\Lambda_3$.
% Define $\kappa''_{g} =  \sqrt{\sigmamax}\kappa'_g$ and $\kappa''_h = \sigmamax\kappa'_h$.
Notice that because $\sigma_{\textrm{min}} = 1$, we have that $\sigmamax = \condition \left(\qk\right)$.
\cref{shifted_ellipsoid} tells us for all $x_0 \in \scaledunshiftedellipsoid$:
\begin{align*}
\left\|\gradf\left(x_0 \right) - \nabla m^{(k)}_f\left(x_0\right)\right\| \le 
\kappa'_g  \dk^2 \sqrt{\condition \left(\frac 2 {\sdk} \qkmo\right)} \le \kappa'_g \sqrt{\condition \left(\qkmo\right)}\dk^2 \\
\left\|\nabla {c_i}\left(x_0 \right) - \nabla m^{(k)}_{c_i}\left(x_0\right)\right\| \le \kappa'_g\sqrt{\condition \left(\qkmo\right)} \dk^2 \quad \forall i \in [m] \\
\left\|\nabla^2\left(x_0 \right) - \nabla m^{(k)}_f\left(x_0\right)\right\| \le \kappa'_h \condition \left(\qkmo\right)\dk, \\
\left\|\nabla^2 {c_i}\left(x_0 \right) - \nabla^2 m^{(k)}_{c_i}\left(x_0\right)\right\| \le \kappa'_h \condition \left(\qkmo\right)\dk \quad \forall i \in [m]
\end{align*}
In particular, $\xk \in \scaledunshiftedellipsoid$.
\end{proof}


\begin{theorem}
\label{accuracy_is_satisfied}
Let $\activeconstraintsk$ and $\dacc$ be defined by \cref{define_activeconstraints}, and \cref{define_delta_accuracy}.
Also, let $\qk$, $\ck$, and $\sdk$ 
be as generated by \cref{constrained_dfo}.

\input{assumptions.d/accuracy_is_satisfied}

There exist $\kappa_g>0$ and $\kappa_h>0$ such that
\begin{align*}
\begin{array}{ccc}
\|\gradf(\xk) - \nabla \mfk(\xk) \| \le \kappa_g \dk^2, & \|\nabla^2 f(\xk) - \hk \| \le \kappa_h \dk, & \\
\|\nabla c_i(\xk) - \gmcik \| \le \kappa_g \dk^2, & \|\nabla^2 c_i(\xk) - \nabla^2 m_{c_i}^{(k)}(\xk) \| \le \kappa_h \dk & \forall i \in [m]. \\
\end{array}
\end{align*}
whenever $\dk \le \dacc$.
\end{theorem}
\begin{proof}
This is a direct consequence of \cref{accuracy_is_satisfied_lemma}, \cref{ellsoid_is_suitable_theorem_p1}, and \cref{bounded_condition_numbers}.
\end{proof}

\begin{lemma}
\label[lemma]{bounded_model_hessian_lemma}
Let $\dacc$ be defined by \cref{accuracy_is_satisfied}.

\input{assumptions.d/bounded_model_hessian_lemma}

Then there exists a $\maxmodelhessian > 0$ such that 
\begin{align*}
\| \hk \| \le \maxmodelhessian \quad \forall x \in \feasible \\
\|\nabla^2 m_{c_i}^{(k)}(\xk) \| \le \maxmodelhessian \quad \forall x \in \feasible, \forall i \in [m]
\end{align*}
whenever $\dk \le \dacc$.
\end{lemma}

\begin{proof}
Let $\kappa_h$ and $\maxhessian$ be defined by \cref{accuracy_is_satisfied} and \cref{bounded_hessians_assumption} respectively,
and let $\maxmodelhessian = \maxhessian + \kappa_h \dacc$.
Then \cref{bounded_hessians_assumption} tells us that
$\|\nabla^2 f(\xk)\| \le \maxhessian$, so that by \cref{accuracy_is_satisfied},
$\|\hk\| \le \|\nabla^2 f(\xk)\| + \kappa_h\dacc = \maxmodelhessian$.
The same applies to each $\nabla^2 m_{c_i}^{(k)}(\xk)$.
\end{proof}

\begin{lemma}
\label[lemma]{i_thought_i_proved_this_already}
Let $\dacc$ be defined by \cref{accuracy_is_satisfied}.

\input{assumptions.d/i_thought_i_proved_this_already}


There exists $\maxmodelgrad > 0$ such that for each $k \in \naturals$, 
\begin{align*}
\left\| \gk \right\| \le \maxmodelgrad \quad
\textrm{and} \quad \left\|\gmcik\right\| \le \maxmodelgrad \quad \forall i \in [m]
\end{align*}
whenever $\dk \le \dacc$.
Without loss of generality, we may assume that $\maxmodelgrad \ge 1$.
\end{lemma}
\begin{proof}
Let $\kappa_g$ and $\maxgrad$ be defined by \cref{accuracy_is_satisfied} and \cref{bounded_gradients_lemma} respectively,
and let $\maxmodelhessian = \maxgrad + \kappa_g \dacc^2$.
Using \cref{accuracy_is_satisfied} to define $\kappa_g$, we can let $\maxmodelgrad = \maxgrad + \kappa_g \dacc^2$.
Then \cref{bounded_gradients_lemma} tells us that
$\|\nabla f(\xk)\| \le \maxgrad$, so that by \cref{accuracy_is_satisfied},
$\|\hk\| \le \|\nabla f(\xk)\| + \kappa_g\dacc^2 = \maxmodelgrad$.
The same applies to each $\left\|\gmcik\right\|$.
\end{proof}

\subsection{Sample Region Feasibility}
\label{ellipsoid_is_feasible_section}

Within this section, we will show that the conservative ellipsoids defined by \cref{define_ellipsek} are feasible as defined by \cref{ellipsoids_notation_definitions}:
that is, $\sampletrk \subseteq \feasible$.
% In order to do this, we would first show 
% \begin{align*}
% c_i(y) \le 0 \quad \forall y \in \fik \cap \tr.
% \end{align*}
% During iteration $k$, we construct an ellipsoid that is meant to be feasible for 

\begin{lemma}
\label[lemma]{only_small_z_matters}
Let $\zik$ and $\mingrad$ be defined by \cref{define_z} and \cref{mingradassumption} respectively.

\input{assumptions.d/only_small_z_matters}

There exists $\mingraddelta > 0$ such that if $\dk \le \mingraddelta$ then for all $i \in [m]$ either
\begin{itemize}
\item $c_i(y) \le 0 \quad \forall y \in \left[\tr \cup \trkpo\right]$ or
\item both $\zik \in \tr$ and $\left\|\gmcik\right\| > \frac 1 2 \mingrad$.
\end{itemize}
\end{lemma}
\begin{proof}
Fix an $i \in [m]$.
Let
$\dacc$,
$\maxgrad$,
$\omegainc$, 
and $\maxmodelhessian$
be defined by
\cref{define_delta_accuracy},
\cref{bounded_gradients_lemma},
\cref{define_the_omegas},
and \cref{bounded_model_hessian_lemma}
respectively.
By \cref{accuracy_is_satisfied}, there exists a $\kappa_g$ such that
\begin{align}
\label{oszm_accuracy}
\left\|\nabla c_i(x) - \gmcik\right\| \le \kappa_g \dk^2.
\end{align}
Define
\begin{align}
\dk \le \mingraddelta \nonumber \\= \min\left\{
\dacc,
\sqrt{\frac 1 {2\kappa_g} \mingrad},
\sqrt{\frac 1 {2\kappa_g} \maxgrad},
\frac{\mingradepsilon}{\frac 3 2 \maxgrad\sqrt{n}\left(1 + \omegainc\right) + n \maxmodelhessian \left(1 + \omegainc\right)^2},
\sqrt{\frac{2\mingradepsilon}{\mingrad}}
\right\} \label{define_mingraddelta}
\end{align}
and let $y\in \left[\tr \cup \trkpo\right]$.
Notice that $\dkpo \le \omegainc \dk$ and $\left\|\xk - \xkpo\right\| \le \sqrt{n}\dk$ imply
\begin{align*}
\left\|y - \xk\right\| \le \left\|y - \xkpo\right\| + \left\|\xkpo - \xk\right\| \le \sqrt{n}\left(1 + \omegainc\right)\dk.
\end{align*}

By \cref{bounded_hessians_assumption}, we have
\begin{align}
c_i(y) \le c_i(\xk) + \left(\gmcik\right)^T(y - \xk) + \frac 1 2 \left(y - \xk\right)^T\nabla^2{c_i}(\xk)\left(y - \xk\right) \nonumber \\
\le c_i(\xk) + \left\|\gmcik\right\|\left(1 + \omegainc\right) \sqrt{n}\dk +n \left(1 + \omegainc\right)^2\maxmodelhessian\dk^2.
\label{oszm_first_bound}
\end{align}

\emph{Case 1}
Suppose that $-m_{c_i}(\xk) \ge \mingradepsilon \Longrightarrow -c_i(\xk) \ge \mingradepsilon$.
Using \cref{oszm_accuracy}, \cref{bounded_gradients_lemma} and 
$\dk \le \sqrt{\frac {\maxgrad} {2\kappa_g}} \Longrightarrow \kappa_g \dk^2 \le \frac 1 2 \maxgrad$ from \cref{define_mingraddelta} we have
\begin{align}
\label{oszm_second_bound}
\left\| \gmcik \right\| \le \left\|\nabla c_i(\xk)\right\| + \kappa_g \dk^2 \le \frac 3 2 \maxgrad.
\end{align}
Also from \cref{define_mingraddelta} we have
\begin{align*}
\left(\frac 3 2 \maxgrad\sqrt{n} \left(1 + \omegainc\right) + n \maxmodelhessian \left(1 + \omegainc\right)^2 \right)\dk \le \mingradepsilon
\end{align*}
and using $\dk \le 1$, we can multiply the second term in paranthesis by $\dk$:
\begin{align*}
\frac 3 2 \maxgrad\sqrt{n} \left(1 + \omegainc\right) \dk + n \maxmodelhessian \left(1 + \omegainc\right)^2\dk^2 \le \mingradepsilon.
\end{align*}
Substituting \cref{oszm_second_bound} into the first term and using our assumption $-c_i(\xk) \ge \mingradepsilon$, we see
\begin{align*}
\left\|\gmcik\right\|\sqrt{n}\left(1 + \omegainc\right) \dk + n \maxmodelhessian \left(1 + \omegainc\right)^2\dk^2  \le \mingradepsilon \le -c_i(\xk).
\end{align*}
Adding $c_i(\xk)$ to both sides yields
\begin{align*}
c_i(\xk) + \left\|\gmcik\right\|\sqrt{n}\left(1 + \omegainc\right) \dk + n \left(1 + \omegainc\right)^2\maxmodelhessian\dk^2 \le 0.
\end{align*}
Combine this with \cref{oszm_first_bound} to see that $c(y) \le 0$.


\emph{Case 2}
Suppose that $-m_{c_i}(\xk) \le \mingradepsilon \Longrightarrow -c_i(\xk) \le \mingradepsilon$.
Then, by \cref{mingradassumption}, 
$\left\|\nabla c(\xk) \right\| \ge \mingrad$.
Using \cref{oszm_accuracy} and \cref{define_mingraddelta}, this implies
$\|\gmcik\| \ge \mingrad - \kappa_g \dk^2 \ge \frac 1 2 \mingrad > 0$.
Continuing with \cref{define_z}, \cref{define_mingraddelta}, and $\dk \le 1$ we find
\begin{align*}
\|\zik-\xk\| = \frac{-c\left(\xk\right)}{\left\|\gmcik\right\|} \le \frac{2\mingradepsilon}{\mingrad}\le \dk^2 \le \dk \Longrightarrow \zik \in \tr.
\end{align*}
\end{proof}

\begin{lemma}
\label[lemma]{each_constraints_cone_is_feasible}
Let $\fik$ be defined by \cref{define_fik}.
% Suppose that \cref{bounded_hessians_assumption}, \cref{lipschitz_gradients_assumption}, \cref{minangleassumption_alt}, and \cref{mingradassumption} hold.

\input{assumptions.d/each_constraints_cone_is_feasible}

There exists $\dfeas > 0$ such that
if $\dk \le \dfeas$, then
\begin{align*}
c_i(y) \le 0 \quad \forall y \in \fik \cap \left[\tr \cup \trkpo\right].
\end{align*}
\end{lemma}

\begin{proof}
Let
$\alpha$, $\beta$, $p_{\alpha}$, and $p_{\beta}$
be defined by
\cref{define_abpab},
and let
$\omegainc$,
$\mingraddelta$,
$\maxhessian$
$\lipgrad$
and $\mingrad$
be defined as in
\cref{define_the_omegas},
\cref{define_mingraddelta},
\cref{bounded_hessians_assumption},
\cref{lipschitz_gradients_assumption},
and \cref{mingradassumption}
respectively.

By \cref{accuracy_is_satisfied}, there exist $\kappa_g > 0$ and $u\in\Rn$ with $\|u\|\le 1$ such that:
\begin{align}
\nabla c_i(\xk) = \nabla m_{c_i}(\xk) + \kappa_g\dk^2 u. \label{model_error_for_gradient}
\end{align}

We define
\begin{align}
\dfeas = \min\bigg\{
1,
\mingraddelta,
\left(\frac{\alpha \mingrad}{2 \maxhessian \sqrt{n} + \kappa_g}\right)^{\frac 1 {1-p_{\alpha}}}, \nonumber \\
\left(\frac{\beta}{4\kappa_g}\mingrad\right)^{\frac 1 {2 - p_{\beta}}}, 
\left[\frac {\mingrad  \beta} {4\maxhessian\sqrt{n}\left(2 + \omegainc + \frac {\lipgrad} \maxhessian \right)}\right]^{\frac1 {1 - p_{\beta}} }
\bigg\}. \label{define_delta_feasible}
\end{align}



Because $\dk \le \mingraddelta$, we satisfy the assumptions for \cref{only_small_z_matters}.
If $c(y) \le \forall y \in \left[\tr \cup \trkpo\right] \subseteq \fik \cap \left[\tr \cup \trkpo\right]$ we are done.
From here on, we can assume both
\begin{align}
\zik \in \tr, \quad \textrm{and} \quad \left\|\gmcik\right\| \ge \frac 1 2 \mingrad. \label{z_is_active}
\end{align}
Let
\begin{align}
y = \wik + ts \in \fik \cap \left[\tr \cup \trkpo\right] \label{t_is_bounded}
\end{align}
with $t > 0, \|s\| = 1, -s^T\hgik \ge \beta \dk^{p_{\beta}}$.
Note that $y \in \tr \cup \trkpo$, $\dkpo \le \omegainc\dk$, and $\xkpo \in \tr$ imply
\begin{align}
\label{eccif_bound_t}
t = \frac{\left\|y - \wik\right\|}{\left\|s\right\|} 
\le \left\|y - \xkpo\right\| + \left\|\xkpo - \xk\right\| + \left\|\xk - \wik\right\| \\
\le \sqrt{n}\left(2 + \omegainc \right)\dk.
\end{align}
We know from \cref{bounded_hessians_assumption} that for all $x \in \Rn$,
\begin{align}
c_i(x) \le c_i(\xk) + \nabla c_i(\xk)^T(x - \xk) + \maxhessian \left \|x - \xk \right\|^2 \label{constraint_lower_bound}
\end{align}

First, we will show that $c(\wik) \le 0$.
For simplicity, we use \cref{define_z}, and \cref{define_w} to compute
% \begin{align}
% \wik - \xk = \xk + \left(1 - \alpha \dk^{\frac 1 2 }\right)\left(\zik - \xk\right) - \xk 
% = \left(1 - \alpha \dk^{\frac 1 2 }\right)\left(\zik - \xk\right) \\
% =  \left(1 - \alpha \dk^{\frac 1 2 }\right)\left(\xk - \frac{c_i(\xk)}{\|\gmcik\|^2}\gmcik - \xk\right) 
% = \left(1 - \alpha \dk^{\frac 1 2 }\right)\frac{-c_i(\xk)}{\|\gmcik\|^2}\gmcik. \label{simple_computation}
% \end{align}


\begin{align}
\wik - \xk = \left(1 - \alpha \dk^{p_{\alpha} }\right)\frac{-c_i(\xk)}{\|\gmcik\|^2}\gmcik. \label{simple_computation}
\end{align}

Also, note by $\dk \le \dfeas$, \cref{define_abpab}, and $\dk^{1 - p_{\alpha}} \ge \dk^{2 - p_{\alpha}}$ that both
\begin{align*}
\dk^{1-p_{\alpha}} \le \frac{\alpha \|\gmcik\|}{\maxhessian \sqrt{n} + \kappa_g} 
\Longrightarrow \maxhessian \sqrt{n}\dk^{1- p_{\alpha}} + \kappa_g \dk^{2 - p_{\alpha}} \le \alpha \|\gmcik\|
\end{align*}
and $0 < 1 - \alpha \dk^{p_{\alpha}} < 1$.
Combine these along with \cref{z_is_active} to find 
\begin{align*}
\maxhessian \sqrt{n}\dk^{1-p_{\alpha}}\left(1 - \alpha \dk^{p_{\alpha}}\right)^2  + \kappa_g \dk^{2 - p_{\alpha}} \left(1 - \alpha \dk^{p_{\alpha}}\right) \\
\le \maxhessian \sqrt{n}\dk^{1 - p_{\alpha}} + \kappa_g \dk^{2 - p_{\alpha}} \le \alpha \|\gmcik\|
\end{align*}
Multiplying by $\dk^{p_{\alpha}}$ we see,
\begin{align*}
-\alpha \dk^{p_{\alpha}}\|\gmcik\| + \maxhessian \left(1 - \alpha \dk^{p_{\alpha}}\right)^2 \sqrt{n}\dk+ \kappa_g \dk^2 \left(1 - \alpha \dk^{p_{\alpha}}\right) \le  0.
\end{align*}
Multiplying by $-\frac{c_i(\xk)}{\|\gmcik\|} > 0$ and using $\zik \in \tr \Longrightarrow \frac{-c_i(\xk)}{\|\gmcik\|} \le \sqrt{n}\dk$, we see
\begin{align*}
-\alpha \dk^{p_{\alpha}}\left\|\gmcik\right\|\left(-\frac{c_i(\xk)}{\|\gmcik\|}\right)  \\
+ \maxhessian \left(1 - \alpha \dk^{p_{\alpha}}\right)^2 \left(-\frac{c_i(\xk)}{\|\gmcik\|}\right)^2 \\
+\kappa_g \dk^2 \left(1 - \alpha \dk^{p_{\alpha}}\right)\left(-\frac{c_i(\xk)}{\|\gmcik\|}\right) \le 0.
\end{align*}
Cancelling terms, we see
\begin{align*}
0 
\ge \alpha \dk^{p_{\alpha}} c_i(\xk) 
+ \maxhessian \frac {c_i(\xk)^2}{\|\gmcik\|^2}\left(1 - \alpha \dk^{p_{\alpha}}\right)^2 \\
+ \kappa_g \dk^2 \left(1 - \alpha \dk^{p_{\alpha}}\right)\frac{-c_i(\xk)}{\|\gmcik\|}.
\end{align*}
Because the last term is positive, we can only decrease the expression by multiplying by $\nu^T \frac{\gmcik}{\|\gmcik\|} \le 1$:
\begin{align*}
0\ge \alpha \dk^{p_{\alpha}} c_i(\xk) + \maxhessian \frac {c_i(\xk)^2}{\|\gmcik\|^2}\left(1 - \alpha \dk^{p_{\alpha}}\right)^2  \\
+ \kappa_g \dk^2 \left(1 - \alpha \dk^{p_{\alpha}}\right)\frac{-c_i(\xk)}{\|\gmcik\|^2}\nu^T\gmcik
\end{align*}
Using \cref{simple_computation}, this is
\begin{align*}
0 \ge c_i(\xk)\left[1 - \left(1 - \alpha \dk^{p_{\alpha}}\right)\right] 
+ \maxhessian \frac {c_i(\xk)^2}{\|\gmcik\|^2}\left(1 - \alpha \dk^{p_{\alpha}}\right)^2 \\
+ \kappa_g \dk^2\nu^T \left(\wik - \xk\right)
\end{align*}
Multiplying $c_i(\xk)$ by $1 = \frac{\|\gmcik\|^2}{\|\gmcik\|^2}$ and distributing, we find
\begin{align*}
0 \ge c_i(\xk) + \left(\gmcik\right)^T\left(1 - \alpha \dk^{p_{\alpha}}\right)\frac{-c_i(\xk)}{\|\gmcik\|^2}\gmcik  \\
+ \maxhessian \left\|\left(1 - \alpha \dk^{p_{\alpha}}\right)\frac{-c_i(\xk)}{\|\gmcik\|^2}\gmcik\right\|^2
+ \kappa_g \dk^2\nu^T \left(\wik - \xk\right) \\
\Longrightarrow 0 \ge c_i(\xk) + \left(\gmcik\right)^T\left(\wik - \xk\right) \\
+ \maxhessian \left\|\wik - \xk\right\|^2  + \kappa_g \dk^2\nu^T \left(\wik - \xk\right).
\end{align*}
by \cref{simple_computation}.
Using \cref{model_error_for_gradient} and \cref{constraint_lower_bound} we see
\begin{align}
0 \ge c_i(\xk) + \nabla c_i(\xk)^T\left(\wik - \xk \right) + \maxhessian \left\|\wik - \xk\right\|^2 \ge c_i(\wik). \label{c_is_negative}
\end{align}

Also, by \cref{define_fik} we have $-s^T \hgik \ge \beta \dk^{p_{\beta}}$ where $\|s\| = 1$.
By our assumption $\dk \le \dfeas$, \cref{define_abpab}, \cref{z_is_active} we know
\begin{align*}
\dk \le \left[\frac{\beta}{4\kappa_g}\mingrad \right]^{\frac 1 {2 - p_{\beta}}}
\Longrightarrow \dk^{2 - p_{\beta}} \le \frac{\beta}{\kappa_g}\left(\frac 1 2\mingrad  - \frac 1 4 \mingrad \right) \\
\Longrightarrow \frac{\kappa_g}{\beta} \dk^{2 - p_{\beta}} \le \|\gmcik\| - \frac 1 4 \mingrad.
\end{align*}
Multiplying by $\dk^2$, and using $-s^T \hgik \ge \beta \dk^{p_{\beta}}$, we see
\begin{align*}
-s^T\gmcik =  -\|\gmcik\|s^T\hgik \\
\ge \|\gmcik\|\beta\dk^{p_{\beta}} 
\ge \frac 1 4 \mingrad  \beta \dk^{p_{\beta}} + \kappa_g\dk^2.
\end{align*}
Because $\|\nu\| = \|s\| = 1$,
\begin{align*}
-s^T\gmcik \ge \frac 1 4 \mingrad  \beta \dk^{p_{\beta}} + \kappa_g\dk^2 \ge \frac 1 4 \mingrad  \beta \dk^{p_{\beta}} + \kappa_g\dk^2|\nu^T s|
\end{align*}
Subtracting the last term and using \cref{model_error_for_gradient}
\begin{align}
-s^T\left(\gmcik + \kappa_g\dk^2\nu\right) \ge \frac 1 4 \mingrad  \beta \dk ^{p_{\beta}}
\Longrightarrow -s^T\nabla c_i(\xk) \ge \frac 1 4 \mingrad  \beta \dk^{p_{\beta}}. \label{nsc_pos}
\end{align}
We also know by $\dk \le \dfeas$ and \cref{define_abpab} that
\begin{align*}
\dk \le \left[\frac {\mingrad  \beta} {4\maxhessian\sqrt{n}\left(2 + \omegainc + \frac {\lipgrad} \maxhessian \right)}\right]^{\frac1 {1 - p_{\beta}} } \\
\Longrightarrow \sqrt{n}\left(2 + \omegainc + \frac {\lipgrad} \maxhessian \right) \dk^{1-p_{\beta}}\le \frac 1 {4\maxhessian} \mingrad  \beta
\end{align*}
Multiplying by $\dk^{p_{\beta}}$,
\begin{align*}
\sqrt{n}\left(2 + \omegainc + \frac {\lipgrad} \maxhessian \right) \dk \le \frac 1 {4\maxhessian} \mingrad  \beta \dk^{p_{\beta}} \\
\Longrightarrow \sqrt{n}\left(2 + \omegainc \right) \dk \le -\frac 1 \maxhessian \sqrt{n}\dk \lipgrad + \frac 1 {4\maxhessian} \mingrad  \beta \dk^{p_{\beta}}
\end{align*}
which implies by \cref{z_is_active}, \cref{t_is_bounded}, and \cref{nsc_pos} that
\begin{align*}
t 
\le \sqrt{n} \left(2 + \omegainc \right) \dk 
\le -\frac 1 \maxhessian \sqrt{n}\dk \lipgrad + \frac 1 {4\maxhessian} \mingrad \beta \dk^{p_{\beta}} \\
\le -\frac 1 \maxhessian \sqrt{n}\dk \lipgrad -\frac 1 \maxhessian \nabla c_i(\xk)^Ts.
\end{align*}
Multiplying by $\maxhessian$ and adding the right hand side, we see
\begin{align*}
\Longrightarrow \sqrt{n}\dk \lipgrad + \nabla c_i(\xk)^Ts + \maxhessian t \le 0.
\end{align*}
Multiplying by $t$ and using \cref{z_is_active},
\begin{align*}
 t \left(\lipgrad\|\wik - \xk\| + \nabla c_i(\xk)^Ts + \maxhessian t\right) \\
 \le t \left(\sqrt{n}\dk \lipgrad + \nabla c_i(\xk)^Ts + \maxhessian t\right) \le 0.
\end{align*}
Using \cref{lipschitz_gradients_assumption},
\begin{align*}
t \left(\nabla c_i(\wik)^Ts - \nabla c_i(\xk)^Ts + \nabla c_i(\xk)^Ts + \maxhessian t\right) \le 0.
\end{align*}
Using \cref{constraint_lower_bound} and \cref{c_is_negative} we can then conclude
\begin{align*}
c_i(y) = c_i(\wik + ts) \le c_i(\wik) + t\nabla c_i(\wik)^Ts + \maxhessian t^2 \le 0.
\end{align*}

% model_error_for_gradient
\end{proof}




\begin{lemma}
\label[lemma]{cone_and_tr_are_feasible}
Let $\dfeas$, $\activeconstraintsk$, $\fik$, $\feasible$
be defined by
\cref{each_constraints_cone_is_feasible},
\cref{define_activeconstraints},
\cref{define_fik},
\cref{define_feasible}.

\input{assumptions.d/cone_and_tr_are_feasible}

If $\dk \le \dfeas$, then $\cap_{i \in \activeconstraintsk} \fik \cap \left[\tr \cup \trkpo\right] \subseteq \feasible$.
\end{lemma}


\begin{boxedcomment}
Add corollary to be referenced when defining the inner cone.
\end{boxedcomment}

\begin{proof}
This follows directly from the definitions and \cref{each_constraints_cone_is_feasible}.
\end{proof}


\begin{lemma}
\label[lemma]{ellipsoid_is_feasible}
Let $\dfeas$ and $\feasible$
be defined by
\cref{each_constraints_cone_is_feasible} and
\cref{define_feasible}.
Let $\unshiftedellipsoid$ be defined by \cref{define_trivial_ellipsek} and \cref{conservative_ellipsoid_details}

\input{assumptions.d/ellipsoid_is_feasible}

If $\dk \le \dfeas$, then $\unshiftedellipsoid$ is feasible according to \cref{ellipsoids_notation_definitions}:
\begin{align*}
\unshiftedellipsoid \subseteq \feasible.
\end{align*}
\end{lemma}

\begin{proof}
This is an immediate consequence of \cref{ellsoid_is_suitable_theorem_p1} and \cref{ellsoid_is_suitable_theorem_p2} and \cref{each_constraints_cone_is_feasible}:
$\unshiftedellipsoid \subseteq \capcones \cap \trkpo \subseteq \feasible$.
\end{proof}



\begin{lemma}
\label[lemma]{searchtrk_is_feasible}
Let $\dfeas$, $\capcones$, and $\feasible$
be defined by
\cref{each_constraints_cone_is_feasible},
\cref{define_capcones},
and
\cref{define_feasible}.

\input{assumptions.d/searchtrk_is_feasible}

If $\dk \le \dfeas$, then $\searchtrk = \capcones$ is feasible:
$\capcones \cap \tr \subseteq \feasible$.
\end{lemma}

\begin{proof}
This is an immediate consequence of \cref{each_constraints_cone_is_feasible}:
$\capcones \cap \tr \subseteq \feasible$.
\end{proof}


% Notice that Lemma 3.1 and Lemma 3.2 within \cite{doi:10.1080/10556788.2015.1026968} show that $\lim_{k\to\infty}\dk = 0$ without requiring the accuracy condition.
% This is because Lemma 3.1 assumes that $\dk \le 1$ explicitly, while Lemma 3.2 uses the accuracy of the model's hessians.
% This justifies the use of a $\dmax > 0$, such that $\dk \le \dmax\;\forall k \in \naturals$.


\subsection{Sufficient Reduction}
\label{sufficient_reduction_section}

To ensure sufficient reduction of the objective's model function during each iteration, the authors of \cite{Conejo:2013:GCT:2620806.2621814} impose the following efficiency condition:
\begin{align}
\label{efficiency}
\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right) \ge \kappa_f \chi_k \min\left\{ \frac{\chi_k}{1+\|\nabla^2 \mfk(\xk)\|}, \dk, 1 \right\}
\end{align}
where $\kappa_f$ is a constant independent of $k$.
This is widely used within trust region frameworks such as \cite{Conejo:2013:GCT:2620806.2621814} and \cite{Conn:2000:TM:357813}.

In most applications, an algorithm computes an approximate solution to
\begin{align}
\label{whole_trust_region_subproblem}
\begin{array}{ccc}
\min_{s \in \Rn} & m_{f}(\xk + s) & \\
\textrm{s.t.} & m_{c_i}(\xk + s) \le 0 & i \in [m]\\
& s \in \tr & \\
\end{array}.
\end{align}
There are methods, such as the projected gradient descent method described in \cite{Conn:2000:TM:357813},
that can efficiently compute an approximate solution to \cref{whole_trust_region_subproblem} satisfying \cref{efficiency}.
However, to maintain feasibility, we have restricted our trial point to lie within a smaller domain:
\begin{align}
\label{capcones_trust_region_subproblem}
\begin{array}{ccc}
\min_{s \in \Rn} & m_{f}(\xk + s) & \\
\textrm{s.t.} & m_{c_i}(\xk + s) \le 0 & i \in [m]\\
& s \in \tr \cap \capcones &
\end{array}.
\end{align}
In general, there may be no solution to \cref{capcones_trust_region_subproblem} that satisfies \cref{efficiency}.
However, we show there is for those iterates with $\chik \ge \kappa_{\Delta}\dk^{p_{\Delta}}$ and sufficiently small $\dk$.
This statement is formalized in \cref{sufficient_reduction_theorem}.

% We construct a solution by

% It can be shown that the \emph{generalized Cauchy point} satisfies this condition \cite{Conn:2000:TM:357813}.
% A efficiency condition \cref{efficiency} is satisfied:
% \begin{align}
% \mfk(\xk) - \mfk(\xk + \sk) \ge \kappa_f \chi_k \min\{ \frac{\chi_k}{1+\|\nabla^2 \mfk(\xk)\|}, \Delta_k, 1 \}
% \end{align}

% With explicit constraints, we know that this is satisfied by the Generalized Cauchy Point.
% For convex constraints, we use the minimization within the buffered feasible region.
% However, we show that this is not required during every iteration, but those iterations in which $\chik \ge \kappa_{\Delta}\dk^{p_{\Delta}}$.


% We wish to show that the projection onto the linearization of the constraints provides a fraction of the reduction of the linearized Cauchy Point.

% Unfortunately, this generalized Cauchy point is not guaranteed to lie within the buffered region $\capcones$, although our algorithm requires the trial point to lie in $\capcones$ to ensure it is feasible.
% Although it is possible to construct a generalized Cauchy point for using the buffered region as the convex constraints, the criticality measure would then be interpreted as the projection onto the buffered region, rather than \cref{define_criticality_measure}.
% Thus, we show a method using the generalized Cauchy point to construct a point within the feasible region that also satisfies the efficiency condition.
% 
% With explicit linear constraints, we could simply use $u_{GC}$ to satiwe know that this is satisfied by the Generalized Cauchy Point.
% For convex constraints, we use the minimization within the buffered feasible region.
% The point given by \cref{sufficient_reduction_theorem} does not satisfy the efficiency condition during every iteration.
% Instead, we show convergence when the efficiency condition is satisfied for at least those iterates with $\chik \ge \kappa_{\Delta}\dk^{p_{\Delta}}$.
% We wish to show that the projection onto the linearization of the constraints provides a fraction of the reduction of the linearized Cauchy Point.
% 
% 
% 
% \begin{lemma}
% Suppose that we are give some points $u, p, \xk \in \Rn$ and $\dk, \delta > 0$ such that
% $\|p - \xk\|_{\infty} \le \dk$ and
% $\|p - u \| \le \delta$.
% Define $v = (1 - t^{\star})\xk + t^{\star}u$ where
% \begin{align*}
% \begin{array}{ccc}
% t^{\star} =& \max_{t\in \reals} &t \\
% & \textrm{s.t. }& \| (1-t) \xk + tu \|_{\infty} \le \dk \\
% & & t \le 1
% \end{array}
% \end{align*}
% Then $\|v - p\| \le 2\delta + (\sqrt{n} - 1)\dk$.
% \end{lemma}
% 
% \begin{proof}
% If $\| u - x \|_{\infty} \le r$, then $t^{\star} = 1$ so that $|(1 - t^{\star})\xk + t^{\star}u - p\| = \| u - p\| \le \delta \le 2\delta + (\sqrt{n} - 1)\dk$.
% Otherwise, by choosing $s \in \reals$ to be $s = \frac{\|u - \xk\|}{\|u - x^{(k)}\|_{\infty}}$, we see that
% \begin{align}
% \label{dumb_lemma_eqn1}
% 1 \le s \le \sqrt{n}, \quad \textrm{and} \quad -\dk \le \dk s \frac {u_i - x^{(k)}_i}{\|u - \xk\|} \le \dk \quad \forall 1 \le i \le n.
% \end{align}
% so that $\|\xk + \dk\frac {u - \xk}{\|u - \xk\|}s - \xk\|_{\infty} \le \dk$.
% If $s$ were to increase, then there would be an $i$ for which \cref{dumb_lemma_eqn1} would not be satisfied, implying
% \begin{align*}
% v = \xk + \dk \frac {u - \xk}{\|u - \xk\|}s.
% \end{align*}
% We can combine this with $\|u - \xk\| \le \|u - v\| + \|v - \xk\| \le \delta + \sqrt{n}\dk $
% to see that
% \begin{align*}
% \|u - v\| = \|u - \xk\| - \|v - \xk\| \le \delta + \sqrt{n}\dk - \dk s \le \delta + (\sqrt{n} - 1)\dk
% \end{align*}
% which then implies
% \begin{align*}
% \|v - p\| \le \|v - u\| + \|u - p\| \le 2\delta + (\sqrt{n} - 1)\dk
% \end{align*}
% 
% 
% 
% 
% 
% % 
% % Otherwise, notice that the optimization program defining $t^{\star}$ can be written as
% % \begin{align*}
% % \begin{array}{cccc}
% % t^{\star} =& \max_{t\in \reals} &t & \\
% % & \textrm{s.t. }& -e_i^T\left((1-t) c + tu\right) + e_i^T\left(c - \dk\right) \le 0 & \forall 1 \le i \le n \\
% % &               & e_i^T\left((1-t) c + tu\right) -  e_i^T\left(c + \dk\right) \le 0 & \forall 1 \le i \le n \\
% % & & t \le 1 & 
% % \end{array}.
% % \end{align*}
% % Because $t^{\star}$ must satisfy the first order critical conditions, we know that there are some subset $P, N \subseteq {i \in \naturals | 1 \le i \le n }$
% % and some $\mu \in \mathbb R^{|P|}_+$, and $\pi \in \mathbb R^{|N|}_+$
% % such that
% % \begin{align*}
% % 1 = \sum_{i \in |P|} \mu_i \left[u_i - c_i\right] + \sum_{i \in |N|} \mu_i \left[c_i - u_i\right]
% % \end{align*}
% % 
% % 
% % Otherwise, there is some set of active constraints
% % \begin{align*}
% % x + e_i
% % \end{align*}
% 
% \end{proof}


% \color{red}
% 
% \begin{lemma}
% \label{sufficient_reduction_theorem}
% Let
% $p_{\alpha}$, $p_{\beta}$, $p_{\Delta}$
% be defined as in \cref{define_p_alpha}, \cref{define_p_beta}, and \cref{define_p_delta} respectively.
% 
% 
% 
% Suppose that
% \cref{max_norm_assumption},
% \cref{minangleassumption},
% \cref{bounded_hessians_assumption},
% \cref{lipschitz_gradients_assumption}
% and the assumptions for
% \cref{bounded_model_hessian_lemma}
% hold.
% 
% We will also use
% $\kappa_{\chi}$,
% $\maxhessian$, and
% $\maxgrad$ as defined in
% \cref{define_kappa_chi},
% \cref{bounded_hessians_assumption},
% and \cref{bounded_gradients_lemma}.
% Let $\alpha$ and $\beta$ be defined as in \cref{define_alpha_beta}.
% Let $\minanglealpha$ and $\minangledelta$ be defined as in \cref{minangleassumption}.
% We will also use $\dacc$, $\deltalargzik$ and defined in \cref{define_delta_accuracy} and \cref{define_deltalargzik}.
% Also, define $\feasiblek$ as in \cref{define_feasiblek}.
% 
% Define
% \begin{align}
% u_l = P_{\feasiblek}(\xk - \gk) \\
% u_{GC} = P_{\feasiblek\cap B_{\infty}\left(\frac 1 2 \dk\right)}(\xk-\gk) \\
% p = 1 + \frac 1 2 p_{\Delta} + \frac 1 2\min\{p_{\alpha}, p_{\beta}\} \label{sr_def_p}\\
% \dsr = \min\left\{
% \dacc,
% \deltalargzik,
% \minangledelta,
% \left(\frac{\delta \minanglealpha}{\left(\delta + 2\sqrt{n}\right)\left(\beta +\alpha\right)}\right)^{\frac 1 {\epsilon_{\Delta}}}
% \right\} \label{define_delta_sufficient_reduction} \\
% \epsilon_{\Delta} = 1-p+\min\{p_{\alpha}, p_{\beta}\}. \label{sr_def_epsilon_delta} \\
% \delta_2 = \kappa_f \kappa_{\chi} \min\left\{ \frac{\kappa_{\chi}}{1 + \maxhessian}, \frac 1 2 \right\} \label{define_delta2} \\
% \delta = \min\left\{1, \frac{\delta_2}{2\left(\maxgrad + 2\maxhessian\right)}\right\} \label{sr_define_delta}
% \end{align}
% If, during iteration $k$, we have
% \begin{align}
% \chik \ge \kappa_{\chi} \dk^{p_{\Delta}} \label{sr_chi_big_enough}
% \end{align}
% and
% \begin{align}
% \dk \le \dsr \label{sr_delta_small_enough}
% \end{align}
% then there exists a $v \in \capcones \cap \tr$ such that
% \begin{align*}
% m_f^{(k)}(\xk) - m_f^{(k)}(v) \ge \frac 1 2 \left(m_f^{(k)}(\xk) - m_f^{(k)}(u_{GC})\right) \ge \left(\frac{\kappa_f}{2} \right)\chi_k \min\left\{ \frac{\chi_k}{1+\|\nabla^2 \mfk(\xk)\|}, \dk, 1 \right\}.
% \end{align*}
% \end{lemma}
% 
% \color{black}


% \begin{definition}
% Let $\feasiblek$, be defined by \cref{define_feasiblek}.
% The projected gradient descent is defined by:
% \begin{align}
% u_{GC} = P_{\feasiblek\cap B_{\infty}\left(\xk, \frac 1 2 \dk\right)}\left(\xk-\gk\right) \label{define_projected_gradient}.
% \end{align}
% \end{definition}
% 
% \begin{lemma}
% \label{sufficient_reduction_of_projected_gradient}
% Let 
% $u_{GC}$ and $\chi_k$
% be defined by
% \cref{define_projected_gradient} and \cref{define_criticality_measure}
% respetively.
% There exists $\kappa_{f}$ such that
% \begin{align*}
% m_f^{(k)}\left(\xk\right) - m_f^{(k)}\left(u_{GC}\right) \ge \kappa_f\chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \frac 1 2 \dk, 1 \right\}.
% \end{align*}
% \end{lemma}

\begin{theorem}
\label{sufficient_reduction_theorem}
Let 
$\chik$,
$\kappa_{\chi}$,
$p_{\Delta}$,
and $\capcones$
be defined by
\cref{define_criticality_measure}
\cref{define_kappa_chi}
\cref{define_p_delta}
and \cref{define_capcones}
respectively.

\input{assumptions.d/sufficient_reduction_theorem}

There exist $\dsr>0$ and $\kappa_f > 0$, such that if
\begin{align}
\chik \ge \kappa_{\chi} \dk^{p_{\Delta}} \label{sr_chi_big_enough}
\end{align}
and
\begin{align}
\dk \le \dsr \label{sr_delta_small_enough}
\end{align}
then there exists $v \in \capcones \cap \tr$ such that
\begin{align*}
m_f^{(k)}(\xk) - m_f^{(k)}(v)
\ge \kappa_f \chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \dk, 1 \right\}.
\end{align*}
% \ge \frac 1 2 \left(m_f^{(k)}(\xk) - m_f^{(k)}(u_{GC})\right)

\end{theorem}

\begin{proof}

Let $\kappa_f' > 0$ be a fixed constant, and $u_{\textrm{GC}}$ be any approximate solution to 
\begin{align*}
\begin{array}{cccc}
u_{\textrm{GC}} \approx & \argmin_{s \in \Rn} & m_{f}(\xk + s) & \\
& \textrm{s.t.} & m_{c_i}(\xk + s) \le 0 & i \in [m]\\
& & s \in B_2\left(\xk, \frac 1 2 \dk\right) & \\
\end{array}.
\end{align*}
satisfying
\begin{align}
\label{what_ugc_satisfies}
m_f^{(k)}(\xk) - m_f^{(k)}(u_{\textrm{GC}}) \ge \kappa_f' \chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \frac 1 2 \dk, 1 \right\}.
\end{align}
This can be done by using the Generalized Cauchy Point as described in section 12.2.1 of \cite{Conn:2000:TM:357813}.
We wish to find a point within the buffered region that provides a percentage of the decrease $u_{\textrm{GC}}$ provides.
To accomplish this, we move from $u_{\textrm{GC}}$ a distance $t$ along the direction $\minangledirk$ defined in \cref{minangleassumption_alt} by letting 
\begin{align*}
v(t) = u_{\textrm{GC}} + t \minangledirk.
\end{align*}
For small enough values of $t$, $v(t)$ will still provide sufficient reduction,
while large values of $t$ ensure $v(t)$ lies within $\capcones$.

\paragraph*{Sufficient Reduction}
First, we show that for small enough values of $t$, $v(t)$ provides sufficient reduction.
% To do this, we bound the affect of moving along $\huk$.
By \cref{i_thought_i_proved_this_already} and \cref{bounded_model_hessian_lemma}, we see that there exists some $\dacc$ such that if
$\dk \le \dacc$, then
\begin{align*}
\left|m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}\left(v(t)\right)\right| \\
\le \left|\gk^T(u_{\textrm{GC}} - v) 
+ 2 \left(u_{\textrm{GC}} - v(t)\right) ^T \nabla^2m_f^{(k)}\left(\xk\right)\left(u_{\textrm{GC}} - v(t)\right) \right| \\
\le \left\|\gk\right \|  \left\|u_{\textrm{GC}}- v(t)\right\| + 2  \|u_{\textrm{GC}} - v(t)\|^2 \left\|\nabla^2m_f^{(k)}\left(\xk\right)\right\| \\
\le t \maxmodelgrad + 2t^2\maxmodelhessian. 
\end{align*}
On the other hand, by defining
\begin{align}
\delta_2 = \kappa_f' \kappa_{\chi} \min\left\{ \frac{\kappa_{\chi}}{1 + \maxmodelhessian}, \frac 1 2 \right\}, \label{define_delta2}
\end{align}
we can use \cref{sr_chi_big_enough}, \cref{bounded_model_hessian_lemma}, and $\dk \le 1$, to simplify \cref{what_ugc_satisfies}:
\begin{align*}
\mfk\left(\xk\right) - \mfk\left(u_{\textrm{GC}}\right)
\ge \kappa_f' \kappa_{\chi} \dk^{p_{\Delta}} \min\left\{ \frac{\kappa_{\chi} \dk^{p_{\Delta}}}{1+\|\nabla^2 \mfk(\xk)\|}, \frac 1 2 \dk \right\} \\
\ge \kappa_f' \kappa_{\chi} \dk^{1 + p_{\Delta}} \min\left\{ \frac{\kappa_{\chi}}{1 + \maxmodelhessian}, \frac 1 2  \right\}
\ge \delta_2 \dk^{1 + p_{\Delta}}.
\end{align*}
Thus, if we can ensure that
\begin{align}
t \maxmodelgrad + 2t^2\maxmodelhessian \le \frac 1 2 \delta_2 \dk^{1 + p_{\Delta}}, \label{ensure_this}
\end{align}
we will have
\begin{align*}
m_f^{(k)}\left(\xk\right) - m_f^{(k)}\left(v(t)\right) = \left[\mfk\left(\xk\right) - \mfk(u_{\textrm{GC}})\right] - \left[m_f^{(k)}\left(v(t)\right) - m_f^{(k)}\left(u_{\textrm{GC}}\right)\right] \nonumber \\
 \ge \left[\mfk(\xk) - \mfk(u_{\textrm{GC}})\right] - \left|m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}(v(t)) \right| \nonumber \\
\ge \frac 1 2 \left(m_f^{(k)}(\xk) - m_f^{(k)}(u_{\textrm{GC}})\right)
\end{align*}
implying $v(t)$ provides $\frac 1 2$ the reduction of $u_{\textrm{GC}}$.
We can then define $\kappa_f = \frac 1 4 \kappa_f'$, to see
\begin{align*}
m_f^{(k)}\left(\xk\right) - m_f^{(k)}\left(v(t)\right)
\ge \frac 1 2 \left[m_f^{(k)}(\xk) - m_f^{(k)}(u_{\textrm{GC}})\right] \\
\ge \frac 1 2 \kappa_f' \chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \frac 1 2 \dk, 1 \right\} \\
\ge \kappa_f \chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \dk, 1 \right\}.
\end{align*}
If we assume $t \le 1$, then \cref{ensure_this} will be satisfied whenever
\begin{align*}
t \le \frac {\delta_2}{2\left(\maxmodelgrad + 2\maxmodelhessian\right)} \dk^{1 + p_{\Delta}}.
\end{align*}



% t \maxmodelgrad + 2t^2\maxmodelhessian \le \frac 1 2 \delta_2 \dk^{1 + p_{\Delta}} \\
% \Longleftrightarrow t \maxmodelgrad + 2t\maxmodelhessian \le \frac 1 2 \delta_2 \dk^{1 + p_{\Delta}} \\

\paragraph*{Buffered Feasibility}
Next, we will show that for large enough values of $t$, $v(t)$ will lie within $\capcones \cap \tr$.
Notice that because $u_{\textrm{GC}} \in B_2\left(\xk, \dk\right)$, as long as $t \le \frac 1 2 \dk$, we will have $v(t) \in \tr$.
By \cref{large_zik_means_means_no_intersection} and \cref{sr_delta_small_enough}, if $i \not \in \activeconstraintsk$, then $v \in \tr \Longrightarrow v \in \fik$.
Therefore, we only consider the case that $i \in \activeconstraintsk$.
Thus, as long as $\dk \le \minangledelta$, \cref{minangleassumption_alt} implies that
\begin{align}
-\frac {\gmcik}{\|\gmcik\|} ^T\huk \ge \minanglealpha \Longrightarrow -\left(\zik - \xk\right)^T\huk \ge \minanglealpha \left\|\zik - \xk\right\| \label{u_is_feasible}
\end{align}
where $\zik$ is defined by \cref{define_z}.
Similarily, because $u_{\textrm{GC}}$ is feasible with respect to the linearization of the constraints, we have
\begin{align}
\left(\zik - \xk\right)^T(u_{\textrm{GC}} - \xk) \le \left\|\zik - \xk\right\|^2. \label{gc_is_feasible}
\end{align}

% for any $i$ with $\zik \in B_{\infty}\left(\xk, \minangledelta\right)$
With $\wik$ defined by \cref{define_w}, we can combine \cref{u_is_feasible} and \cref{gc_is_feasible} to find
\begin{align}
\left(v(t) - \wik \right)^T\left(\xk - \zik \right) \nonumber \\
=\left[ u_{\textrm{GC}} + t\huk - \xk - \left(1 - \alpha\dk^{p_{\alpha}}\right)\left(\zik - \xk\right) \right]^T\left(\xk - \zik \right) \nonumber \\
=- \left[
\left( u_{\textrm{GC}} - \xk\right)^T\left(\zik - \xk \right)
\right]
+\left(1 - \alpha\dk^{p_{\alpha}}\right)\left\|\zik - \xk\right\|^2 \nonumber \\
+ t\left[ -\left(\zik - \xk\right)^T\huk\right] \nonumber \\
\ge 
-\left\|\zik - \xk\right\|^2
+\left(1 - \alpha\dk^{p_{\alpha}}\right)\left\|\zik - \xk\right\|^2
+ t\minanglealpha \left\|\zik - \xk\right\| \nonumber \\
= \left\|\zik - \xk\right\|
\left[
t\minanglealpha
- \alpha\dk^{p_{\alpha}} \left\|\zik - \xk\right\|
\right]. \label{lowerbounddotproduct}
\end{align}
Notice that if
\begin{align}
t\minanglealpha - \alpha\dk^{p_{\alpha}} \left\|\zik - \xk\right\| \ge \beta \dk^{p_{\beta}} \left\|v(t) - \wik\right\| \label{backwards_proof_still_require}
\end{align}
then \cref{lowerbounddotproduct} implies
\begin{align*}
\left(v(t) - \wik \right)^T\left(\xk - \zik \right) \ge \beta \dk^{p_{\beta}}\left\|v(t) - \wik\right\| \left\|\xk - \zik\right\| \\
\Longrightarrow -\frac {\left(v(t) - \wik \right)}{\left\|v(t) - \wik \right\|}^T\frac{\gmcik}{\left\|\gmcik\right\|}\ge\beta \dk^{p_{\beta}} 
\Longrightarrow v(t) \in \fik.
\end{align*}

To satisfy \cref{backwards_proof_still_require}, we assume that 
$\frac{6 \sqrt{n}}{\minanglealpha} \dk^{1 + \min\left\{p_{\alpha}, p_{\beta}\right\}} \le t \le \sqrt{n}\dk$.
Because $t \le \sqrt{n} \dk$, the triangle inequality, \cref{define_z}, and \cref{define_w}, we see
\begin{align*}
\left\|v(t) - \wik\right\| \le \left\|v(t) - u_{\textrm{GC}}\right\| + \left\|u_{\textrm{GC}} - \xk \right\| + \left\|\xk - \wik\right\| \\
\le t + \frac 1 2 \dk + \sqrt{n}\dk 
\le 3\sqrt{n}\dk  \\
\textrm{and} \quad \|\zik - \xk \| \le \sqrt{n}\dk \le 3\sqrt{n}\dk.
\end{align*}
% \label{sr_v_minus_w_small}
% \label{sr_z_minus_x_small}
These, along with $\dk \le 1$, and $\alpha, \beta, p_{\alpha}, p_{\beta} \le (0, 1)$ from \cref{define_abpab} imply
\begin{align*}
\alpha\dk^{p_{\alpha}} \left\|\zik - \xk\right\| + \beta \dk^{p_{\beta}} \left\|v(t) - \wik\right\| 
\le 6 \sqrt{n} \dk^{1 + \min\left\{p_{\alpha}, p_{\beta}\right\}} \le t \minanglealpha.
\end{align*}

\paragraph*{Choosing t}

We have shown that $v(t) = u_{\textrm{GC}} + t \minangledirk \in \capcones \cap \tr$ satisfies sufficient reduction 
if $\dk \le \min\left\{1, \dacc\right\}$, $t \le \min\left\{1, \frac 1 2 \dk\right\} \le \sqrt{n} \dk$, and
\begin{align*}
\frac{6 \sqrt{n}}{\minanglealpha} \dk^{1 + \min\left\{p_{\alpha}, p_{\beta}\right\}} \le t 
\le \frac {\delta_2}{2\left(\maxmodelgrad + 2\maxmodelhessian\right)} \dk^{1 + p_{\Delta}}
\end{align*}
where $\delta_2$ is defined by \cref{define_delta2}.
Notice from \cref{define_p_delta}, there exists a $p$ with $0 < p_{\Delta} < p < \min\left\{p_{\alpha}, p_{\beta}\right\}$.
Thus, we can let $t = \dk^{1+p}$ and choose $\dk$ sufficiently small.
Namely,
\begin{itemize}
\item by letting 
$\dk \le n^{\frac 1 {2p} }$, we have
$t = \dk^{1+p} \le \sqrt{n} \dk$
\item by letting 
$\dk \le \left(\frac{\minanglealpha}{6 \sqrt{n}}\right)^{\left(\min\left\{p_{\alpha}, p_{\beta}\right\} - p\right)^{-1}}$, we have
$\frac{6 \sqrt{n}}{\minanglealpha} \dk^{1 + \min\left\{p_{\alpha}, p_{\beta}\right\}} \le \dk^{1+p} = t,$
\item by letting 
$\dk \le \left[ \frac {\delta_2}{2\left(\maxmodelgrad + 2\maxmodelhessian\right)} \right]^{\left(p - p_{\Delta}\right)^{-1}}$, we have
$t = \dk^{1+p} \le  \frac {\delta_2}{2\left(\maxmodelgrad + 2\maxmodelhessian\right)} \dk^{1 + p_{\Delta}}$.
\end{itemize}




\color{red}
% so that  will be satisfied if
% We can use \cref{define_delta2} to simplify this:
% \begin{align*}
% \mfk(\xk) - \mfk(u_{\textrm{GC}}) \ge 
% \end{align*}
% Then \cref{sr_define_delta} gives
% \begin{align*}
% \delta_2 \dk^{1 + p_{\Delta}} \ge \left(2\maxmodelgrad + 4\maxmodelhessian\right)\delta\dk^{1 + p_{\Delta}},
% \end{align*}
% and \cref{sr_p_big} implies $\dk^{1+p_{\Delta}} \ge \dk^p$ so that
% \begin{align*}
% 2\left(\maxmodelgrad + 2\maxmodelhessian\right)\delta\dk^{1 + p_{\Delta}}
% \ge \left(2\maxmodelgrad + 4\maxmodelhessian\right)\delta\dk^{p}
% \end{align*}
% 
% Because $\dk \le 1$, $\delta \le 1$, and $p \ge 1$, we see that
% \begin{align*}
% 2\left(\maxmodelgrad + 2\maxmodelhessian\right)\delta\dk^{p}
% = 2\maxmodelgrad \delta \dk^p + 4\maxmodelhessian\delta\dk^{p}
% \ge 2\maxmodelgrad \delta \dk^p + 4\maxmodelhessian\delta^2 \dk^{2p}
% \end{align*}
% 
% 
% Putting these inequalities together yields
% \begin{align*}
% \mfk(\xk) - \mfk(u_{\textrm{GC}}) \ge 2\left|m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}(v)\right| \\
% \Longrightarrow -\left|m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}(v)\right| \ge -\frac 1 2 \left(\mfk(\xk) - \mfk(u_{\textrm{GC}})\right)
% \end{align*}
% so that
% \begin{align*}
% m_f^{(k)}(\xk) - m_f^{(k)}(v) = \left[\mfk(\xk) - \mfk(u_{\textrm{GC}})\right] - \left[m_f^{(k)}(v) - m_f^{(k)}(u_{\textrm{GC}})\right] \nonumber \\
%  \ge \left[\mfk(\xk) - \mfk(u_{\textrm{GC}})\right] - \left|m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}(v) \right| \nonumber \\
% \ge \frac 1 2 \left(m_f^{(k)}(\xk) - m_f^{(k)}(u_{\textrm{GC}})\right)
% \end{align*}
% and $v$ provides $\frac 1 2$ the reduction of $u_{\textrm{GC}}$.
% 
% 
% 
% \begin{boxedcomment}
% Add more explanation here...
% \end{boxedcomment}
% 
% Let
% $\kappa_{\chi}$,
% $\maxmodelhessian$,
% $\maxmodelgrad$,
% $\dacc$,
% $\dfeas$,
% $\deltalargzik$,
% $\activeconstraintsk$,
% and $\feasiblek$
% be defined by
% \cref{define_kappa_chi},
% \cref{bounded_model_hessian_lemma},
% \cref{i_thought_i_proved_this_already},
% \cref{define_delta_accuracy},
% \cref{define_delta_feasible},
% \cref{define_deltalargzik},
% \cref{define_activeconstraints},
% and \cref{define_feasiblek}
% respectively.
% 
% Let $\alpha$, $\beta$ $p_{\alpha}$, and $p_{\beta}$
% be defined as in \cref{define_abpab}, as well as $\minanglealpha$ and $\minangledelta$ be defined by \cref{minangleassumption_alt}.
% Define
% \begin{align}
%  \\
% \delta =& \min\left\{1, \frac{\delta_2}{2\left(\maxmodelgrad + 2 \maxmodelhessian \right)}\right\} \label{sr_define_delta} \\
% p =& 1 + \frac 1 2 p_{\Delta} + \frac 1 2\min\{p_{\alpha}, p_{\beta}\} \label{sr_def_p}\\
% v =& u_{\textrm{GC}} - \delta \dk^{p} \huk \label{define_v} \\
% \epsilon_{\Delta} =& 1-p+\min\{p_{\alpha}, p_{\beta}\}. \label{sr_def_epsilon_delta} \\
% \dsr =& \min\left\{
% \dfeas,
% \dacc,
% \deltalargzik,
% \frac{\minangledelta}{3\sqrt{n}},
% \left(\frac{\delta \minanglealpha}{\left(\delta + 2\sqrt{n}\right)\left(\beta +\alpha\right)}\right)^{\frac 1 {\epsilon_{\Delta}}}
% \right\} \label{define_delta_sufficient_reduction}.
% \end{align}
% 
% By \cref{sr_delta_small_enough}, $\dk \le \dsr \le \minangledelta$ so 
% Let $\huk$ be defined as in \cref{define_u},
% Then $-\frac {\gmcik}{\|\gmcik\|} ^T\huk \ge \minanglealpha$ for any $i$ with $\zik \in B_{\infty}\left(\xk, \minangledelta\right)$.
% 
% Note that by \cref{sr_def_p}, we have
% \begin{align}
% p = 1 + \frac 1 2 p_{\Delta} + \frac 1 2 \min\{p_{\alpha}, p_{\beta} \} \Longrightarrow 1 + p_{\Delta} < p. \label{sr_p_big}
% \end{align}
% From \cref{sr_def_p}, it also follows that
% \begin{align}
% p < 1 + p_{\alpha} \Longrightarrow 1 - p + p_{\alpha} > 0   \quad \textrm{and} \quad
% p < 1 + p_{\beta}\Longrightarrow 1 - p + p_{\beta} > 0. \label{sr_p_small_alpha_beta}
% \end{align}
% Combine these with \cref{sr_def_epsilon_delta} to find
% \begin{align}
% \epsilon_{\Delta} = \min\{1 - p + p_{\alpha}, 1 - p + p_{\beta} \} > 0 \label{sr_epsilon_delta_positive}.
% \end{align}
% From \cref{define_v}, we see
% \begin{align}
% \|u_{\textrm{GC}} - v\| = \delta \dk^{p} \label{sr_v_close_u}
% \end{align}
% % Observe that $\zeta \ge 0$ because $\xk \in \tr$.
% % We know that when $\zeta' = 0,  \zeta' \left(u_{\textrm{GC}} - \delta \dk^{p} u\right) + (1-\zeta')\xk = \xk \in \tr$, so that $\zeta \ge 0$.
% % Also, if $u_{\textrm{GC}} - \delta \dk^{p} \huk\in\tr$, then $\zeta' = 1 \Longrightarrow v \in \tr$.
% % When $u_{\textrm{GC}} - \delta \dk^{p} \huk \not\in\tr$, then because $\tr$ is a convex set, there is a unique, well defined value of $\zeta \in (0, 1)$.
% and because $u_{\textrm{GC}} \in B_{\infty}(\xk, \frac 1 2 \dk)$, $\delta \le \frac 1 2 $, $p > 1$, and $\dk \le 1$,
% 
% 
% % so that $v \in \tr$.
% % This means that $v \in \tr$ is well defined.
% % then $\zeta' = 1$ is not feasible, but $\zeta' = 0$ still is.
% % Thus, because $\tr$ is a convex set, there is a well defined value $\zeta \in (0, 1]$ to define $v$.
% 
% % Note that because $u_{\textrm{GC}} \in \tr$, 
% % \begin{align}
% % \|u_{\textrm{GC}} - v\| \le \left\|u_{\textrm{GC}} - \left(u_{\textrm{GC}} - \delta \dk^{p} \huk\right)\right\| + \left\|\left(u_{\textrm{GC}} - \delta \dk^{p} \huk\right) - v\right\| \nonumber \\
% % \le \delta 2\dk^{p} + \left\|v - \xk \right\| + \left\|\xk - u_{\textrm{GC}}\right\|
% % \le 2 \delta\left(1 + \sqrt{n}\right) \dk^{p} \label{sr_v_close_u}
% % \end{align}
% % 
% % By \cref{sufficient_reduction_of_projected_gradient}, $u_{\textrm{GC}}$ satisfies
% % % Because $u_{\textrm{GC}}$ is the Cauchy point for the trust region subproblem with $\frac 1 2$ the current radius, it satisfies
% % \begin{align*}
% % \mfk(\xk) - \mfk(u_{\textrm{GC}}) \ge \kappa_f \chi_k \min\left\{ \frac{\chi_k}{1+\|\nabla^2 \mfk(\xk)\|}, \frac 1 2 \dk, 1 \right\}.
% % \end{align*}
% 
% 
% % \begin{boxedcomment}
% % Make 2 different $\kappa_f$
% % \end{boxedcomment}
% 
% 
% 
% 
% \begin{boxedcomment}
% $ = \ldots$ where $\delta_2 = ?$.
% \end{boxedcomment}
% 
% % \kappa_f \kappa_{\chi} \dk^{1 + p_{\Delta}} \min\left\{ \frac{\kappa_{\chi}}{1 + \maxhessian}, 1 \right\} 
% % \ge 4\delta\maxgrad \left(1 + \sqrt{n}\right) \dk^{p} + 16\delta^2 \maxhessian\left(1 + \sqrt{n}\right)^2\dk^{2p}.
% 
% % \begin{boxedcomment}
% % If we could have:
% % % 4\delta\maxgrad \left(1 + \sqrt{n}\right) \dk^{p} + 16\delta^2 \maxhessian\left(1 + \sqrt{n}\right)^2\dk^{2p} \\
% % \begin{align}
% % \frac {\delta_2} 6 \dk^{1 + p_{\Delta}} \ge 4\maxgrad \delta 
% % \end{align}
% % \begin{align}
% % \frac {\delta_2} 6 \dk^{1 + p_{\Delta}} \ge 2\maxgrad\left(\sqrt{n} - 1\right)\dk
% % \end{align}
% % \begin{align}
% % \frac {\delta_2} 6 \dk^{1 + p_{\Delta}} \ge16\maxhessian\delta^2
% % \end{align}
% % \begin{align}
% % \frac {\delta_2} 6 \dk^{1 + p_{\Delta}} \ge16\maxhessian\delta \left(\sqrt{n} - 1\right)\dk
% % \end{align}
% % \begin{align}
% % \frac {\delta_2} 6 \dk^{1 + p_{\Delta}} \ge4\maxhessian \left(\sqrt{n} - 1\right)^2\dk^2
% % \end{align}
% % \end{boxedcomment}
% % 4\maxgrad \delta + 2\maxgrad\left(\sqrt{n} - 1\right)\dk + 
% % 16\maxhessian\delta^2 +
% % 16\maxhessian\delta \left(\sqrt{n} - 1\right)\dk + 
% % 4\maxhessian \left(\sqrt{n} - 1\right)^2\dk^2 \\
% % = 2\maxgrad \left[2\delta + \left(\sqrt{n} - 1\right)\dk\right] + 4\maxhessian\left[2\delta + \left(\sqrt{n} - 1\right)\dk\right]^2 \\
% % 
% % 
% % % \begin{align*}
% % % \dk^{2 - 1 - p_{\Delta}} \le \frac{\delta_2}{5 \cdot 4\maxhessian \left(\sqrt{n} - 1\right)^2} \\
% % % \frac {\delta_2} 5 \dk^{} \ge 16\maxhessian\delta \left(\sqrt{n} - 1\right)\dk^{1 + p_{\Delta}}
% % % \end{align*}
% 
% 
% 
% % \cref{bounded_gradients_lemma}, and \cref{bounded_hessians_assumption}
% % Using this, \cref{sr_define_delta}, \cref{sr_p_big}, and \cref{sr_v_close_u} we know that
% % \begin{align*}
% % m_f^{(k)}(u_{\textrm{GC}}) - m_f^{(k)}(v) = 
% % \left(\gk\right)^T(u_{\textrm{GC}} - v) + \left(u_{\textrm{GC}} - v\right) ^T\left(\nabla^2m_f^{(k)}\left(\xk\right)\right)\left(u_{\textrm{GC}} - v\right) \\
% % \le \left\|\gk\right \|  \left\|u_{\textrm{GC}}- v\right\|  + \|u_{\textrm{GC}} - v\|^2 \left\|\nabla^2m_f^{(k)}\left(\xk\right)\right\|\\
% % \le \delta\maxgrad \left(1 + \sqrt{n}\right) \dk^{p}  + \delta^2 \left(\maxhessian - 1\right)\left(1 + \sqrt{n}\right)^2\dk^{2p} \\
% % \le \frac 1 2 \kappa_f \kappa_{\chi} \dk^{1 + p_{\Delta}}\min\left\{ \frac{\kappa_{\chi}}{\maxhessian}, 1 \right\}
% % \le \frac 1 2 m_f^{(k)}(\xk) - m_f^{(k)}(u_{\textrm{GC}}) \\
% % \end{align*}
% 
% % \ge \frac 1 4 \kappa_f' \chi_k \min\left\{ \frac{\chi_k}{1+\left\|\nabla^2 \mfk(\xk)\right\|}, \dk, 1 \right\} \\
% 
% Next, we will show that $v \in \capcones$.
% First, we need some identities for constraint $c_i$.
% 
% Because $i \in \activeconstraintsk$, and $\dk\le\minangledelta$ by \cref{sr_delta_small_enough} we can use \cref{minangleassumption_alt} to conclude
% 
% 
% 
% % If $u_{\textrm{GC}} \in \tr$, then by \cref{sr_chi_big_enough} and \cref{define_p_delta}: $\left\|u_{\textrm{GC}} - \xk\right\| = \chik \ge \kappa_{\chi} \dk^{p_{\Delta}} \ge  \kappa_{\chi} \dk$.
% % If $u_{\textrm{GC}} \not \in \tr$, then a trust region constraint is active and $\left\|u_{\textrm{GC}} - \xk\right\| \ge \dk$.
% % In either case, we find that 
% % \begin{align}
% % \left\|u_{\textrm{GC}} - \xk\right\| \ge \dk \min\{\kappa_{\chi}, 1 \} \label{gc_big_enough}.
% % \end{align}
% 
% 
% 
% Using \cref{gc_is_feasible} and \cref{u_is_feasible} we see this is
% \begin{align}
% \left(v - \wik \right)^T\left(\xk - \zik \right)  \ge - \alpha \dk ^{p_{\alpha}} \left\|\zik - \xk\right\|^2
% + \delta\dk^{p} \minanglealpha \|\zik - \xk\| \nonumber \\
% = \left(
% \delta\dk^{p} \minanglealpha
% - \alpha \dk ^{p_{\alpha}} \left\|\zik - \xk\right\|
% \right)\|\zik - \xk\|. \label{sr_what_to_bound}
% \end{align}
% 
% On the other hand, using \cref{define_delta_sufficient_reduction}, \cref{sr_delta_small_enough}, \cref{sr_p_small_alpha_beta}, \cref{sr_epsilon_delta_positive} we see
% \begin{align*}
% \dk^{\epsilon_{\Delta}} \le \frac{\delta \minanglealpha}{\left(\delta + 2\sqrt{n}\right) \left(\beta +\alpha\right)} 
% \Longrightarrow\frac{ \delta \minanglealpha }{\left(\delta + 2\sqrt{n}\right)} \ge \beta\dk^{\epsilon_{\Delta}} + \alpha\dk^{\epsilon_{\Delta}} 
% \ge \beta \dk^{1 + p_{\beta} - p} + \alpha \dk ^{1 + p_{\alpha} - p}.
% \end{align*}
% Using \cref{sr_v_minus_w_small} and \cref{sr_z_minus_x_small}
% \begin{align*}
% \Longrightarrow \delta\dk^{p} \minanglealpha  \ge \beta \left(\delta + 2\sqrt{n}\right) \dk^{1 + p_{\beta}} + \alpha\left(\delta + 2\sqrt{n}\right)  \dk ^{1 + p_{\alpha}}
% \ge \beta \dk^{p_{\beta}}\left\|v - \wik\right\| + \alpha \dk ^{p_{\alpha}} \left\|\zik - \xk\right\|\\
% \Longrightarrow \delta\dk^{p} \minanglealpha - \alpha \dk ^{p_{\alpha}} \left\|\zik - \xk\right\|  \ge \beta \dk^{p_{\beta}}\left\|v - \wik\right\|.
% \end{align*}
% \color{black}


\end{proof}



\subsection{Initial Convergence Results}
\label{initial_convergence_results}
\begin{boxedcomment}
$\eta$ corresponds to $\gammasm$, $\eta_1$ corresponds to $\gammabi$.
$\bar S$ corresponds to $\suffiterates$.
$S$ corresponds to $\miniterates$.
\end{boxedcomment}


With 
$\gammasm$ and $\gammabi$
defined by \cref{define_the_gammas},
let
\begin{align}
\suffiterates &= \left\{k \in \naturals | \rk > \gammabi \right\} \label{define_suffiterates} \\
\textrm{and} \quad \miniterates &= \left\{k \in \naturals | \rk \ge \gammasm \right\} \label{define_miniterates}.
\end{align}

With
$\kappa_g$, $\kappa_f$, $\lipgrad$, $\maxmodelhessian$
defined by
\cref{sufficient_reduction_theorem}, \cref{sufficient_reduction_theorem}, \cref{lipschitz_gradients_assumption}, and \cref{bounded_model_hessian_lemma}
respectively, let
\begin{align}
\theircnot &= \lipgrad + \kappa_{g} + \frac 1 2 \maxmodelhessian \label{define_theircnot} \\
\textrm{and} \quad \theirc &= \frac{\theircnot}{\kappa_f} .  \label{define_theirc}
\end{align}

With
$\chik$, $p_{\Delta}$, $\dsr$, $\kappa_{\chi}$, $\maxmodelhessian$, $\gammasm$
defined by 
\cref{define_criticality_measure}, \cref{define_p_delta}, \cref{sufficient_reduction_theorem}, \cref{define_kappa_chi}, \cref{bounded_hessians_assumption}, and \cref{define_the_gammas}
let
\begin{align}
\label{define_reduceiterates}
\reduceiterates = \left \{ k \in \naturals \bigg| \dk \le \min \left\{ 
\dsr, 
\frac {\chik}{\maxmodelhessian}, 
\frac{1-\gammasm}{\theirc}\chik,
\left(\frac 1 {\kappa_{\chi}}  \chik\right)^{\frac 1 {p_{\Delta}}}, 
1
\right\} \right \}.
\end{align}

\subsubsection{Trust Region Radius}

% Assume that 
% \cref{lipschitz_gradients_assumption}
% and the assumptions for
% \cref{accuracy_is_satisfied},
% \cref{bounded_model_hessian_lemma},
% and \cref{sufficient_reduction_theorem}
% hold.

\begin{lemma}
\label[lemma]{mathcal_k_subset_bar_s}

Let $\miniterates$ and $\reduceiterates$ be defined by \cref{define_miniterates} and \cref{define_reduceiterates} respectively.

\input{assumptions.d/mathcal_k_subset_bar_s}

We have $\reduceiterates \subset \miniterates$.
\end{lemma}
 

\begin{proof}
By the Mean Value Theorem, there exists a $t_k \in (0, 1)$ such that
\begin{align*}
f\left(\xk + \sk\right) = f\left(\xk\right) + \gradf\left(\xk + t_k\sk\right)^T\sk
\end{align*}

By \cref{lipschitz_gradients_assumption}, \cref{bounded_model_hessian_lemma}, \cref{accuracy_is_satisfied}, if $\dk \le 1$, then
\begin{align*}
\left|f\left(\xk\right) - f\left(\xk + \sk\right) - \left[\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right)\right]\right| \\
= \left|-\left[\gradf\left(\xk + t_k\sk\right) - \gk\right]^T\sk + \frac 1 2 \left(\sk\right)^T \hk \sk\right| \\
\le \left(\left\| \gradf\left(\xk + t_k\sk\right) - \gradf\left(\xk\right) \right\| 
+ \left\| \gradf\left(\xk\right)-\gk \right\|\right) \left\|\sk\right\| \\
+ \frac 1 2 \left\|\sk\right\|^2\left\|\hk\right\| \\
\le \left(t_k \lipgrad \|\sk\| + \kappa_{g}\dk\right) \left\|\sk\right\| + \frac 1 2 \maxmodelhessian \left\|\sk\right\|^2
\end{align*}

Since $\left\| \sk \right\| \le \dk$ and $t_k \in (0, 1)$ we have
\begin{align*}
\left|f\left(\xk\right) - f\left(\xk + \sk\right) - \left[\mfk\left(\xk\right) + \mfk\left(\xk + \sk\right)\right]\right| \le \theircnot \dk^2
\end{align*}
where $\theircnot$ is defined by \cref{define_theircnot}.

By \cref{define_reduceiterates}, for every $k \in \reduceiterates$ we have that $\kappa_{\chi}\dk^{p_{\Delta}} \le \chik$ and consequently $\chik > 0$.
By \cref{sufficient_reduction_theorem}, this means that $\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right) \ne 0$.
Then,
\begin{align*}
\left|\rk - 1\right| 
= \left|\frac
{f\left(\xk\right) - f\left(\xk + \sk\right) - \left[\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right)\right]}
{\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right)} \right| \\
\le \frac {\theircnot \dk^2} {\kappa_f \chik \min\left\{\frac{\chik}{\maxmodelhessian}, \dk, 1\right\}} \\
= \frac {\theirc \dk^2} {\chik \min\left\{\frac{\chik}{\maxmodelhessian}, \dk, 1\right\}}
\end{align*}

We also know by \cref{define_reduceiterates}
\begin{align*}
\dk = \min\left\{\frac {\chik} {\maxmodelhessian}, \dk, 1 \right\}, \quad \textrm{and} \quad
\frac {\theirc \dk}{\chik} \le 1 - \gammasm
\end{align*}
so that
\begin{align*}
\left|\rk - 1\right| \le 1 - \gammasm
\Longrightarrow \rk \ge \gammasm
\end{align*}
so that $k \in \miniterates$.
\end{proof}


% Suppose that \cref{bounded_gradients_lemma}, \cref{bounded_hessians_assumption}, and \cref{minangleassumption_alt} hold.
% 
% Suppose that the assumptions for
% \cref{bounded_model_hessian_lemma} and
% \cref{sufficient_reduction_theorem}
% hold.

\begin{lemma}
\label[lemma]{delta_to_zero}

\input{assumptions.d/delta_to_zero}

The sequence $\left\{\dk\right\}$ converges to zero.
\end{lemma}

\begin{proof}
Let
$\suffiterates$, $\kappa_f$, $\chik$, $\maxmodelhessian$, $\kappa_{\chi}$, $p_{\Delta}$ $\gammabi$
be defined by
\cref{define_miniterates}, \cref{sufficient_reduction_theorem}, \cref{define_criticality_measure}, \cref{bounded_model_hessian_lemma}, \cref{define_kappa_chi}, \cref{define_p_delta}, \cref{define_the_gammas}
respectively,
and let $\omegadec$ and $\omegainc$ be defined by \cref{define_the_omegas}.
Suppose that $\suffiterates$ is finite.
Then there exists $k_0 \in \naturals$ such that for all $k \ge  k_0$, $\dkpo \le \omegadec \dk$.
Thus, $\left\{\dk\right\} \to 0$ because $\omegadec < 1$.
This is because the only places in the algorithm where we do not compute $\rk$ is if $\sampletrk \not \subseteq \feasible$ of the trial point $\sk \not \in \feasible$.
In both of these scenarios, the new trust region radius is decreased by $\omegadec$.
Firstly, note that the trust region always decreases when calling the \emph{RestoreFeasibility} subroutine.

From now on, we may assume that $\suffiterates$ is infinite.  
For any $k \in \suffiterates$, we have $\chik \ge \kappa_{\chi}\dk^{p_{\Delta}}$.
Using \cref{bounded_model_hessian_lemma} and \cref{sufficient_reduction_theorem} we have
\begin{align*}
f\left(\xk\right) - f\left(\xkpo\right) \ge \gammabi \left[\mfk\left(\xk\right) - \mfk\left(\xk + \sk\right)\right] \\
\ge \gammabi \kappa_f \chik \min\left\{\frac{\chik}{\maxmodelhessian}, \dk, 1\right\}
\ge \gammabi \kappa_f \kappa_{\chi}\dk^{p_{\Delta}}\min\left\{\frac{\dk}{\oalpha \maxmodelhessian}, \dk, 1\right\}.
\end{align*}
% 
Because $f\left(\xk\right)$ is nonincreasing, the left hand side goes to zero.
Thus,
\begin{align}
\lim_{k \in \suffiterates} \dk = 0.
\end{align}

Consider the set
$\mathcal U = \{ k \in \naturals | k \not \in \suffiterates \}$.
If $\mathcal U$ is finite, then $\lim_{k\to\infty}\dk = 0$.
Otherwise, consider $k \in \mathcal U$ and define $l_k$ to be the last index in $\suffiterates$ before $k$.
Then $l_k$ is well-defined for all large $k$  and $\dk \le \omegainc \Delta_{l_k}$ which implies that
\begin{align}
\lim_{k \in \mathcal U } \dk \le \omegainc \lim_{k \in \mathcal U} \Delta_{l_k} = \omegainc \lim_{l_k \in \miniterates} \Delta_{l_k}
\end{align}
so that $\lim_{k \in \mathcal U} \dk = 0$.
\end{proof}


\subsubsection{Criticallity Goes to Zero, Part 1}

% \begin{assumptions}
% Suppose the assumptions for \cref{mathcal_k_subset_bar_s} and \cref{delta_to_zero} hold.
% \end{assumptions}

\begin{lemma}
\label[lemma]{liminf_chi_to_zero}
Let $\chik$ be defined by \cref{define_criticality_measure}.

\input{assumptions.d/liminf_chi_to_zero}

We have $\liminf_{k\to\infty} \chik = 0$.
\end{lemma}
 

\begin{proof}
Suppose for a contradiction that there exists a constants $\epsilon > 0$ and an integer $k_1 > 0$ such that $\chik \ge \epsilon$ for each $k \ge k_1$.
Using the same constants as used in \cref{define_reduceiterates}, let
\begin{align*}
\tilde \Delta = \min \left\{
\dsr, 
\frac{\epsilon}{\maxhessian}, 
\frac{(1 - \gammasm)}{\theirc} \epsilon,
\left(\frac 1 {\kappa_{\chi}} \epsilon \right)^{\frac 1 {p_{\Delta}}},
1\right\}
\end{align*}
and consider a $k \ge k_1$.
If $\dk \le \tilde \Delta$, then $k \in \reduceiterates$.
Then, by \cref{mathcal_k_subset_bar_s},  $k \in \miniterates$ and thus $\dkpo \ge \dk$.
Therefore, the trust region radius can only decrease if $\dk > \tilde \Delta$, and in this case $\dkpo = \omegadec\dk > \omegadec \tilde \Delta$.
Therefore, one can see that for all $k \ge k_1$
\begin{align}
\dk \ge \min\left\{\omegadec \tilde \Delta, \dk \right\}
\end{align}
which contradicts \cref{delta_to_zero}.
\end{proof}

Our next goal is to show that if $\gammasm > 0$, then $\lim_{k\to\infty} \chik = 0$.
Observe that if this limit does hold, then there exists an $\epsilon_{\textrm{lb}} > 0$ such that the set
\begin{align}
\ulb = \left\{k \in \naturals | \chik \ge\elb \right\} \label{define_the_finite_set}
\end{align}
is infinite.
By \cref{liminf_chi_to_zero}, given some $\epsilon_{\textrm{lb}} > 0$, we are able to define the sequence of pairs
\begin{align}
\label{define_slb}
\slb = \left\{\left(k,
\min_{i \in \naturals, i > k, \chi_{i} \le \frac 1 2 \elb} i
\right)\right\}_{k=1}^{\infty}.
\end{align}
Using the same constants as in \cref{define_reduceiterates}, we define
\begin{align}
\dlb = \min\left\{
\dsr,
\frac{\elb}{\maxmodelhessian}, 
\frac{(1-\gammasm)}{\theirc}\elb, 
\left(\frac 1 {\kappa_{\chi}} \elb \right)^{\frac 1 {p_{\Delta}}}, 
1
\right\}. \label{define_delta_lb}
\end{align}


% Using the same , \cref{delta_to_zero} implies the existence of $k_0 \in \naturals$ such that for all $k \ge k_0$,


\begin{lemma}
\label[lemma]{contradiction_portion}
Let $\ulb$ and $\slb$, be defined by \cref{define_the_finite_set} and \cref{define_slb}.

\input{assumptions.d/contradiction_portion}

If there exists an $\elb > 0$ such that $\ulb$ is infinite, then
\begin{align*}
\lim_{(k, l) \in\slb, k \to \infty} \left\|\xk - \xl\right\| = 0.
\end{align*}


% If \cref{the_contradiction} were true, then the sequence $S_{lb}$ satisfies \cref{criteria_from_contradiction}.
\end{lemma}
\begin{proof}
% Notice that \cref{liminf_chi_to_zero} ensures that the sequence $S_{lb}$ is well defined.
By \cref{delta_to_zero}, there exists $k_1 \in \naturals$ such that if $k \ge k_1$, 
then $\dk \le\dlb$ as defined in \cref{define_delta_lb}.
Let $\miniterates$ be defined by \cref{define_miniterates}, and consider
$C_k = \left\{i \in \miniterates | k_1 \le k \le i < l \right\}$.
% for $(k, l) \in \slb$.
Note that $k \in \miniterates$, so $C_k \ne \emptyset $.
For each $i \in C_k$, we have by \cref{define_rhok}
\begin{align*}
f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right) 
= \rk \left( \mfk\left(x^{(i)}\right) - \mfk\left(x^{(i)} + s^{(i)}\right) \right).
\end{align*}
Because $i \in \miniterates$, this is
\begin{align*}
f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right) \ge \gammasm \left( \mfk\left(x^{(i)}\right) - \mfk\left(x^{(i)} + s^{(i)}\right) \right).
\end{align*}
Continuing with \cref{sufficient_reduction_theorem}, we see
\begin{align*}
f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right)\ge \gammasm \kappa_f \chi_i \min\left\{\frac{\chi_i}{1+\left\|\nabla^2 \mfk(x^{(i)})\right\|}, \Delta_i, 1\right\} 
\end{align*}
Lastly, \cref{bounded_model_hessian_lemma}, tells us
\begin{align*}
f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right)\ge \gammasm \kappa_f \chi_i \min\left\{\frac{\chi_{i}}{1 + \maxhessian}, \Delta_i, 1\right\}.
\end{align*}

Because $ k \le i < l$, $\chi_i > \frac 1 2 \elb$ for all $i \in C_k$.
As $i \ge k$, we have $\Delta_i \le \dlb \le \frac{\elb}{\maxhessian}$ and $\Delta_i \le 1$.
Therefore,
\begin{align*}
\frac{\Delta_i}{2} \le \frac{\elb}{2 \maxhessian} \le \frac{\chi_i}{\maxhessian}.
\end{align*}

It follows that
\begin{align*}
f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right) > \frac{\gammasm \kappa_f \elb \Delta_i}{4}
\end{align*}

and hence
\begin{align*}
\Delta_i < \frac{4}{\gammasm \kappa_f \elb} \left[ f\left(x^{(i)}\right) - f\left(x^{(i+1)}\right)\right].
\end{align*}

Meanwhile, because steps are taken whenever $\rk \ge \gammasm$, we have that
\begin{align*}
\left\|\xk - \xl\right\| \le \sum_{i = k}^{l-1}\left\|x^{(i)} - x^{(i+1)}\right\| = \sum_{i \in C_k}\left\|x^{(i)} - x^{(i+1)}\right\| \le \sum_{i \in C_k} \Delta_i
\end{align*}
so that

\begin{align}
\left\|\xk - \xl\right\| < \frac{4}{\gammasm \kappa_f \elb} \left[ f\left(\xk\right) - f\left(\xl\right)\right].
\end{align}


We also know that $\left\{f\left(\xk\right)\right\}$ is bounded below by \cref{bounded_below_assumption}, 
and since it is nonincreasing, $f\left(\xk\right)  - f\left(\xl\right) \to 0$.
Therefore $\left\{\left\|\xk - \xl\right\|\right\}_{(k,l) \in \slb}$ converges to zero.
\end{proof}


\subsection{Bounded Projection}
\label{bounding_the_projection_section}
As discussed at the beginning of \cref{convex_convergence_analysis}, this section is devoted to showing convergence of the difference
between a projection onto an iterate's constraint's linearization and
\begin{itemize}
\item a different iterate's constraint's linearization.
\item the true constraint's linearization.
\end{itemize}
This is required within \cref{convergence_section}.

% \color{red}
% We initially attempted to prove this result using the results found within \cite{pena2020new},
% which rely heavily on a quantity called the Hoffman constant.
% This Hoffman constant can be used to bound differences in projections of similar sets.
% However, to use the Hoffman constant across iterations, we needed to provide a uniform bound on the Hoffman constant.
% While were able to do this for a subset of the constraints, we found a simpler method that applied to all constraints.
% Thus, the strategy relying on the Hoffman constant is not required, but is still presented as it is an interesting result.
% The simplified proof lies within \cref{simplifed_bounded_projection}, 
% while the method we show a bound on the Hoffman constant in \cref{introducing_the_hoffman_constant}, 
% bound the difference in projections based on this bound in \cref{bounding_projection_onto_two_polyhedra},
% and finally show that the projection onto active constraints is bounded within \cref{bounding_the_difference_in_the_active_projection}.
% \color{black}

\subsubsection{Simple Properties}


\begin{figure}[ht]
	\centering
	\includegraphics[width=300px]{images/first_lemma.png}
	\caption[
    		A depiction of the quantities within \cref{properties_of_a_circle}.]{
			The distances from $u$, $v$ to $x$, $y$ are less than $\epsilon$ respectively, and $x$ and $y$ lie within a sphere of radius $\|a\|$.
			Also, $a^Tx \le a^Ta$ separates the sphere from $u$ and $v$.
	}
	\label{first_lemma}
\end{figure}
The following lemma is illustrated in \cref{first_lemma}.
\begin{lemma}
\label[lemma]{properties_of_a_circle}
Suppose that $x, y, u, v, a \in \Rn$ and $0 < \epsilon \le 1$ are given satisfying
\begin{align*}
\|x\| \le \|a\|, \quad
\|y \| \le \|a\|, \quad
\|x - u \| \le \epsilon, \quad
\|y - v \| \le \epsilon, \quad
a^T u \ge a^T a, \quad
a^T v \ge a^T a.
\end{align*}
Then
\begin{align*}
\|x - y\| \le 8(1 + \|a\|) \sqrt{\epsilon}.
\end{align*}
\end{lemma}
\begin{proof}
If $\|a\| = 0$, then $x = y = 0$ so that $\|x - y\| = 0$.
We may now assume that $a \ne 0$.
We can combine
$
a^Tx  \le \|a\| \|x\| \le \|a\|^2 = a^T a
% a^Ty  \le \|a\| \|y\| \le \|a\|^2 = a^T a
$
with
$
a^Tx = a^T(x - u) + a^T u \ge a^Ta - \|a\| \epsilon
% a^Ty = a^T(y - v) + a^T v \ge a^Ta - \|a\| \epsilon
$
to see
$
a^Ta - \|a\|\epsilon \le a^Tx \le a^Ta \Longrightarrow 0 \le a^Ta - a^Tx \le \|a\| \epsilon.
% a^Ta - \|a\|\epsilon \le a^Ty \le a^Ta \Longleftrightarrow 0 \le a^Ta - a^Ty \le \|a\| \epsilon 
$
By symmetry, $0 \le a^Ta - a^Ty \le \|a\| \epsilon$ so that
\begin{align*}
\left| a^Tx - a^Ty \right| \le \|a\|\epsilon.
\end{align*}


% 
% \begin{align*}
% \left(a^Ta - a^Tx\right)^2 \le \|a\|^2 \epsilon
% \end{align*}
% 
% \begin{align*}
% \end{align*}

Considering this identity further, we see
$
a^Ta - \|a\| \epsilon \le a^Tx \le \|a\|\|x\| 
\Longrightarrow \|a\| - \epsilon \le  \frac{a}{\|a\|}^T x \le \|x\| 
\Longrightarrow -\|a\| + \epsilon \ge  -\frac{a}{\|a\|}^T x \ge -\|x\| 
\Longrightarrow 0 \le \|x\| -\frac{a}{\|a\|}^T x \le \|x\|-\|a\| + \epsilon \le \epsilon
$
and
$
|\|x\| + \frac{a}{\|a\|}^T x| \le \|x - \frac{a}{\|a\|}^T x\| + 2|\frac{a}{\|a\|}^T x| \le \epsilon + 2\|x\| \le 2\|a\| + \epsilon
$.
We multiply these to find
\begin{align*}
\sqrt{\|x\|^2 - \frac{\left(a^Tx\right)^2}{a^Ta}}
= \sqrt{\left(\|x\| - \frac{a}{\|a\|}^Tx\right)\left(\|x\| + \frac{a}{\|a\|}^Tx\right)} \\
\le \sqrt{2\|a\|\epsilon + \epsilon^2} \le \epsilon + \sqrt{2\|a\|\epsilon}.
\end{align*}



% Also, we see
% $
% 0 \le \left\|a - x\right\|^2 
% = \|a\|^2 + \|x\|^2 - 2a^Tx 
% \le \|a\|^2 + \|x\|^2 - 2\left(a^Ta - \|a\|\epsilon\right)
% = \|x\|^2 - \|a\|^2 + 2\|a\|\epsilon
% $
% so that
% $
% \|a\|^2 - 2\|a\|\epsilon \le \|x\|^2 \le \|a\|^2
% \Longrightarrow 0 \le \|a\|^2 - \|x\|^2 \le 2\|a\|\epsilon.
% $
% By symmetry,
% $
% 0 \le \|a\|^2 - \|y\|^2 \le 2\|a\|\epsilon
% $
% and we see
% \begin{align*}
% \left|\|x\|^2 - \|y\|^2\right| \le 2 \|a\|\epsilon.
% \end{align*}

Because
$
\left(x - \frac{a^Tx}{a^Ta} a\right)^T \frac{a^Tx}{a^Ta} a = \frac{a^Tx}{a^Ta} a^Tx - \left(\frac{a^Tx}{a^Ta}\right)^2a^Ta = 0,
$
% \left(y - \frac{a^Ty}{a^Ta} a\right)^T \frac{a^Ty}{a^Ta} a = \frac{a^Ty}{a^Ta} a^Ty - \left(\frac{a^Ty}{a^Ta}\right)^2a^Ta = 0 \\
we can compute
$
\|x\|^2 = \left\|x - \frac{a^Tx}{a^Ta} a + \frac{a^Tx}{a^Ta} a\right\|^2 = \left\|x - \frac{a^Tx}{a^Ta} a \right\|^2 +\frac{\left|a^Tx\right|^2}{a^Ta}
$
% \|y\|^2 = \left\|y - \frac{a^Ty}{a^Ta} a + \frac{a^Ty}{a^Ta} a\right\|^2 = \left\|y - \frac{a^Ty}{a^Ta} a \right\|^2 +\left|a^Ty\right|^2
so that 
$
\left\|x - \frac{a^Tx}{a^Ta} a \right\| = \sqrt{\|x\|^2 - \frac{\left|a^Tx\right|^2}{a^Ta}}.
$
%  \left\|y - \frac{a^Ty}{a^Ta} a \right\| = \sqrt{\|y\|^2 - \left|a^Ty\right|^2}
By symmetry,
$
\left\|y - \frac{a^Ty}{a^Ta} a \right\| = \sqrt{\|y\|^2 - \frac{\left|a^Ty\right|^2}{a^Ta}},
$
so that

\begin{align*}
\left\|x - \frac{a^Tx}{a^Ta} a - y + \frac{a^Ty}{a^Ta} a \right\|
\le \left\|x - \frac{a^Tx}{a^Ta} a \right\| + \left\|y - \frac{a^Ty}{a^Ta} a \right\|
\le 2\epsilon + 2\sqrt{2\|a\|\epsilon}.
\end{align*}

% \begin{align*}
% \left\|x - \frac{a^Tx}{a^Ta} a \right\|^2 - \left\|y - \frac{a^Ty}{a^Ta} a \right\|^2
% = \|x\|^2 - \|y\|^2 + \left(-\left|a^Tx\right|^2 + \left|a^Ty\right|^2\right) \\
% \left|\left\|x - \frac{a^Tx}{a^Ta} a \right\|^2 - \left\|y - \frac{a^Ty}{a^Ta} a \right\|^2\right|
% \le \left|\|x\|^2 - \|y\|^2\right| + \left|\left|a^Tx\right|^2 - \left|a^Ty\right|^2\right| \\
% \le 2 \|a\|\epsilon + 2 |a^Tx| \|a\|\epsilon + \|a\|^2\epsilon^2 \\
% \Longrightarrow 
% \left|\left\|x - \frac{a^Tx}{a^Ta} a \right\| - \left\|y - \frac{a^Ty}{a^Ta} a \right\|\right|
% \le \sqrt{2 \|a\|\epsilon + 2 |a^Tx| \|a\|\epsilon + \|a\|^2\epsilon^2}
% \le \|a\|\epsilon + \sqrt{2 \|a\|\epsilon\left(1 + |a^Tx|\right)}
% \end{align*}


% \begin{align*}
% \left\|x - \frac{a^Tx}{a^Ta} a - \left(y - \frac{a^Ty}{a^Ta} a\right)\right\|^2
% = \left\|x - \frac{a^Tx}{a^Ta} a\right\|^2 + \left\|y - \frac{a^Ty}{a^Ta} a\right\|^2 - 2\left(x - \frac{a^Tx}{a^Ta} a\right)^T\left(y - \frac{a^Ty}{a^Ta} a\right) \\
% = \left\|x - \frac{a^Tx}{a^Ta} a\right\|^2 + \left\|y - \frac{a^Ty}{a^Ta} a\right\|^2 - 2\left(x^Ty - \frac{a^Tya^Tx}{a^Ta}\right) \\
% \end{align*}
% 
% \begin{align*}
% \left\|x - \frac{a^Tx}{a^Ta} a - \left(y - \frac{a^Ty}{a^Ta} a\right)\right\|^2
% = \left\|x - \frac{a^Tx}{a^Ta} a\right\|^2 + \left\|y - \frac{a^Ty}{a^Ta} a\right\|^2 - 2\left(x - \frac{a^Tx}{a^Ta} a\right)^T\left(y - \frac{a^Ty}{a^Ta} a\right) \\
% = \left\|x - \frac{a^Tx}{a^Ta} a\right\|^2 + \left\|y - \frac{a^Ty}{a^Ta} a\right\|^2 - 2\left(x^Ty - \frac{a^Tya^Tx}{a^Ta}\right) \\
% \end{align*}



Finally, we combine everything to see
\begin{align*}
\|x - y\| 
\le \left\|x - \frac{a^Tx}{a^Ta} a - \left(y - \frac{a^Ty}{a^Ta} a\right)\right\| + |a^Tx - a^Ty|
\le (2 + \|a\|)\epsilon + 2\sqrt{2\|a\|\epsilon} \\
\le \left(2 + 2\sqrt{2\|a\|} + \|a\|\right)\sqrt{\epsilon}
\le \left(\sqrt{2} + \sqrt{\|a\|}\right)^2 \sqrt{\epsilon}
\le \left(2\max\{\sqrt{2},\sqrt{\|a\|}\}\right)^2 \sqrt{\epsilon} \\
\le 4\max\{2,\|a\|\} \sqrt{\epsilon}
\le 8\max\{1,\|a\|\} \sqrt{\epsilon}
\le 8(1 + \|a\|) \sqrt{\epsilon}
\end{align*}
% is small.
% = \epsilon \|a\| \left(2 + 2|a^Tx| + \|a\|\epsilon\right) \\

% \begin{align*}
% \left\|x - \frac{a^Tx}{a^Ta} a \right\| - \left\|y - \frac{a^Ty}{a^Ta} a \right\|
% = \sqrt{\|x\|^2 - \|y\|^2} - \sqrt{\left|a^Tx\right|^2 + \left|a^Ty\right|^2}
% \end{align*}

% \begin{align*}
% |x - y| \le \epsilon \\
% \Longleftrightarrow x - \epsilon \le y \le x + \epsilon \\
% \Longleftrightarrow x^2 - 2x\epsilon - \epsilon^2 \le x^2 - 2x\epsilon + \epsilon^2 \le y^2 \le x^2 + 2x\epsilon + \epsilon^2 \\
% \Longleftrightarrow  |x^2 - y^2| \le 2 x \epsilon + \epsilon^2
% \end{align*}
% 
% \begin{align*}
% \epsilon \le x^2 \\
% \Longrightarrow \sqrt{\epsilon} \le x \\
% \Longrightarrow 2\epsilon \le 2x\sqrt{\epsilon} \\
% \Longrightarrow x^2 - 2x\sqrt{\epsilon} + \epsilon \le x^2 - \epsilon \\
% \Longrightarrow x - \sqrt{\epsilon} \le \sqrt{x^2 - \epsilon}
% \end{align*}
% 
% \begin{align*}
% \left|x^2 - y^2\right| \le \epsilon \\
% \Longleftrightarrow x^2 - \epsilon \le y^2 \le x^2 + \epsilon \\
% \Longleftrightarrow x - \sqrt{\epsilon} \le \sqrt{x^2 - \epsilon} \le y \le \sqrt{x^2 + \epsilon} \le x + \sqrt{\epsilon} \\
% \Longleftrightarrow |x - y| \le \sqrt{\epsilon}
% \end{align*}

% \begin{align*}
% x^Tx \ge \frac{\|x\|}{\|a\|} a^T x
% \ge \frac{\|x\|}{\|a\|}\left(a^Ta - \|a\|\epsilon\right)
% \ge \|x\|\|a\| - \|x\| \epsilon 
% \ge \|x\|\left(\|a\| - \epsilon\right)
% \end{align*}
% 
% \begin{align*}
% x = x - \frac{a^Tx}{a^Ta} a + \frac{a^Tx}{a^Ta} a \\
% y = y - \frac{a^Ty}{a^Ta} a + \frac{a^Ty}{a^Ta} a
% \end{align*}
% 
% \begin{align*}
% \|x - y\| =
% \left\| x - \frac{a^Tx}{a^Ta} a  \right\|
% \end{align*}

\end{proof}


We define the distance from a point $s$ to a convex set $S$ as
\begin{align*}
D\left(s, S\right) = \inf_{s' \in S} \left\|s - s'\right\|,
\end{align*}
and the distance between any two convex sets $S_1$ and $S_2$ as
% \convexdistance\left(S_1, S_2\right) = \max_{s' \in S_1 \cup S_2} \min_{s \in S_1 \cap S_2} \|s' - s\|. \label{define_a_difference_between_sets}
\begin{align}
\convexdistance\left(S_1, S_2\right) = 
\max \left\{
	\sup_{s  \in S_1} D\left(s , S_2\right),
	\sup_{s' \in S_2} D\left(s', S_1\right)
\right\}. \label{define_a_difference_between_sets}
\end{align}
Note that $D\left(S_1, S_2\right) \ge 0$ for any two convex sets $S_1$, and $S_2$.


\begin{boxedcomment}
Not supposed to worry about $S \cap S' = \emptyset$.
\end{boxedcomment}

\begin{lemma}
\label[lemma]{board_tied_to_sphere}
Let $\convexdistance$ be defined by \cref{define_a_difference_between_sets}.

Let $g \in \Rn$ and let $S, S' \subseteq \Rn$ be convex with $S \cap S' \ne \emptyset$.
If $D\left(S, S'\right) \le 1$, then 
% Suppose that $0 < \epsilon \le 1$, then
% is given with $D\left(S, S'\right) \le \epsilon$.
% Then 
\begin{align*}
\|P_{S}(g) - P_{S'}(g)\| \le 8\left(1 + \|P_{S \cap S'}(g) - g\|\right) \sqrt{\convexdistance\left(S, S'\right)}.
\end{align*}
\end{lemma}
\begin{proof}
% Because $\convexdistance(S, S') \le \epsilon$, we know that $S \cap S' \ne \emptyset$.

% If $S \cap S' = \emptyset$, then $P_{S \cap S'}\left(g\right) = \infty$.

If $g \in S \cap S'$, then $\left\|P_S(g) - P_{S'}(g)\right\| = 0 \le 8\left(1 + \|P_{S \cap S'}(g) - g\|\right) \sqrt{\convexdistance\left(S, S'\right)}$.
Likewise, the left hand side is $0$ whenever $S = S'$.
Otherwise, $\convexdistance\left(S, S'\right) > 0$, and let 
\begin{align*}
\begin{array}{ccc}
x = P_S(g) - g      & u = P_{S \cap S'}(g + x) - g & a = P_{S \cap S'}(g) - g \\
y = P_{S'}(g) - g   & v = P_{S \cap S'}(g + y) - g. &  \\
\end{array}
\end{align*}
Because $S \cap S' \subseteq S$ and $S \cap S' \subseteq S'$ we have
$\|x\| = \|P_S(g) - g\| \le \|P_{S \cap S'} (g) - g\| = \|a\|$ and $\|y\| \le \|a\|$.
Also, because $S \cap S'$ is convex, the optimality conditions for $a$ imply the plane $a^Tx \ge a^Ta$ separates $S \cap S'$ from $g$.
In particular, $a^Tu \ge a^Ta$ and $a^Tv \ge a^Ta$.
Finally, by letting $0 < \epsilon = \convexdistance(S, S') \le 1$, we have $\|x - u\| = \|P_{S \cap S'}\left(P_S(g)\right)\| \le \epsilon$ and $\|y - v\| \le \epsilon$.
Thus, \cref{properties_of_a_circle} informs us
$\|P_S(g) - P_{S'}(g)\| = \|x - y\| \le 8(1 + \|a\|) \sqrt{\epsilon}$.
\end{proof}


\begin{corollary}
Let $\convexdistance$ be defined by \cref{define_a_difference_between_sets}.

Let $g, g' \in \Rn$ and let $S, S' \subseteq \Rn$ be convex with $S \cap S' \ne \emptyset$.
Suppose that $0 < \epsilon \le 1$ is given with $D\left(S, S'\right) \le \epsilon$ and $\|g - g'\| \le \epsilon$.
Then $\|P_S(g) - P_{S'}(g')\| \le 9\left(1 + \|P_{S \cap S'}(g) - g\|\right) \sqrt{\epsilon}$.
\end{corollary}
\begin{proof}
Notice
\begin{align*}
\|P_S(g) - P_{S'}(g')\| 
\le \|P_S(g) - P_{S'}(g)\| + \|P_{S'}(g) - P_{S'}(g')\| \\
\le \|P_S(g) - P_{S'}(g)\| + \|g - g'\| \\
\le 8\left(1 + \|P_{S \cap S'}(g) - g\|\right) \sqrt{\epsilon} + \epsilon \\
\le 9\left(1 + \|P_{S \cap S'}(g) - g\|\right) \sqrt{\epsilon}.
\end{align*}
\end{proof}

\begin{lemma}
\label[lemma]{the_lemma_to_end_all_lemmas}
Let $\convexdistance$ be defined by \cref{define_a_difference_between_sets}.

Let
$A, A' \in \mathbb \mathbb R^{m \times n}$;
$b, b' \in \mathbb \mathbb R^{m}_{\ge 0}$;
$C, C' \in \mathbb \mathbb R^{m' \times n}$;
$d, d' \in \mathbb \mathbb R^{m'}_{\ge 0}$;
unit vector $u \in \Rn$;
and constants
$\delta_l$,
$\delta_u$,
$\delta_r$,
$\delta_{\alpha} > 0$
be given such that
\begin{align*}
\begin{array}{cccc}
\delta_r \|A_i\| \le b_i,			&
Cu \ge \delta_{\alpha} e,			&
\delta_l \le \|A_i\| \le \delta_u	\\
\delta_r \|A_i'\| \le b_i',			&
C'u \ge \delta_{\alpha} e, 			&
\delta_l \le \|A'_i\| \le \delta_u.
\end{array}
\end{align*}
% be given such that $\left\|A_i\right\| > 0$ and $\left\|A'_i\right\| > 0$ for all $i \in [m]$.
% Suppose there exists a unit vector $u \in \Rn$ with $Cu > 0$ and $C'u > 0$.
Given $p \in \Rn$ and $r > 0$, define the sets
\begin{align*}
\begin{array}{ccccc}
S  &= \bigg\{x \in \Rn \bigg|& A (x - p) \le b ,& C (x - p) \le d ,& \|x - p\| \le r \bigg\}, \\
S' &= \bigg\{x \in \Rn \bigg|& A'(x - p) \le b',& C'(x - p) \le d',& \|x - p\| \le r \bigg\}.
\end{array}
\end{align*}
and for any $0 < \epsilon < r$, define the constants
\begin{align*}
\begin{array}{cccccc}
\lambda &=& \delta_{\alpha}^{-1}(r + 1) (\delta_u + 2), &
\gamma &=& 2\lambda(\delta_r \delta_u)^{-1}, \\
\eta    &=& \frac 1 2 \epsilon\delta_{\alpha}(r+2)^{-1}(r + 1)^{-1}, &
\delta &=& \min\bigg\{
1,
\gamma^{-1},
\frac 1 2 {\epsilon} (\gamma r)^{-1},
\eta
\bigg\}.
\end{array}
\end{align*}
If
\begin{align*}
\begin{array}{ccccc}
\|A - A'\|_{\infty} \le \delta,	& \|b - b'\|_{\infty} \le \delta,		& \|C - C'\|_{\infty} \le \delta,	& \textrm{and} & \|d - d'\|_{\infty} \le \delta
\end{array},
\end{align*}
then
$D(S, S') \le \epsilon$.



% \delta_1 \le \delta_2 \|A_i\| \le b_i, 
% \delta_1 \le \delta_2 \|A'_i\| \le b'_i, \\
% \gamma = \frac{2}{\delta_1}\left(1 + \|y\|\right), 
% \epsilon \le \min\left\{\gamma^{-1}\right\}.


\end{lemma}
% y - z  =       A        ^T\mu  + C^T\lambda  \\
% y - z' = \left(A'\right)^T\mu' + C^T\lambda' \\
% t &=  \max_{\substack{i \in [m] \\ A'_i(z - p) > b'_i}} \frac{A'_i (z - p) - b'_i}{A'_i (z - p)} \\

\begin{proof}
% (1 + r)\left[1 + \delta_{\alpha}^{-1} \left(\delta_2 + 1\right)\right]
\begin{boxedcomment}
No longer usable:
Define
\begin{align*}
\begin{array}{cccccc}
\delta_l &=& \min_{1\le i\le m} \min\left\{ \left\|A_i\right\|, \left\|A'_i\right\| \right\},  &
\delta_u &=& \max_{1\le i\le m} \max\left\{ \left\|A_i\right\|, \left\|A'_i\right\| \right\}, \\
\delta_r &=& \min_{1\le i\le m} \min\left\{ \frac{b_i}{\left\|A_i\right\|}, \frac{b_i}{\left\|A'_i\right\|} \right\}, &
\textrm{and} \quad \delta_{\alpha} &=& \max \left\{ \left\|Cu\right\|_{\infty}, \left\|C'u\right\|_{\infty} \right\},
\end{array}
\end{align*}
so that
\begin{align*}
\begin{array}{cccc}
\delta_r \|A_i\| \le b_i,			&
Cu \ge \delta_{\alpha} e,			&
\delta_l \le \|A_i\| \le \delta_u	\\
\delta_r \|A_i'\| \le b_i',			&
C'u \ge \delta_{\alpha} e, 			&
\delta_l \le \|A'_i\| \le \delta_u.
\end{array}
\end{align*}
\end{boxedcomment}

Let $x \in S$, and define
\begin{align*}
\begin{array}{cccccc}
s &=& \delta(r + 1)\delta_{\alpha}^{-1}, &
z &=& x - su, \\
t &=& \max\left\{\gamma \delta, s(r+s)^{-1}\right\}, &
v &=& (1-t)z + t p.
\end{array}
\end{align*}
The construction of $v$ has the following intuition:
to travel to $v$ from $x$, we
\begin{itemize}
\item first travel a distance $s$ along $-u$ to reach $z$ (where we will show $C'(z-p) \le d'$),
\item then travel a fraction $t$ of the distance back to $p$ (where both $A'(v - p) \le b'$ and $\|v-p\| \le r$).
\end{itemize}
Throughout the remainder of this proof, we will show $v \in S'$ and $\|x - v\| \le \epsilon$.
The result then follows from the symmetry of $S$ and $S'$.


For each $i \in [m]$,
\begin{align*}
0 \le C_i'(x-p) - d_i'
\le C_i(x - p) - d_i + (C_i' - C_i)(x-p) + d_i - d_i'  \\
\le 0 + \|C_i' - C_i\| \|x - p\| + \|d_i - d_i'\| 
\le \delta (r+1)
= \delta_{\alpha} s \le sC_i'u
\end{align*}
so that
\begin{align}
-sC_i'u \le d_i' - C_i'(x - p) \Longrightarrow C'(z - p) = C'(x - su - p) \le d'. \label{reun_eqn3}
\end{align}

\begin{boxedcomment}
Explanation for next sentence:
\begin{align*}
s\delta_u + \delta \left(s + r\right) + \delta 
&= \frac{\delta(r + 1)}{\delta_{\alpha}}\delta_u + \delta \left(s + r\right) + \delta \\
&\le \delta \left[(r + 1)\frac{\delta_u}{\delta_{\alpha}} + (r+1)\frac{\delta}{\delta_{\alpha}} + r + 1\right] \\
&\le \delta (r + 1) \left[\frac{\delta_u}{\delta_{\alpha}} + \frac{\delta}{\delta_{\alpha}} + 1\right] \\
&\le \delta (r + 1) \left[\frac{\delta_u}{\delta_{\alpha}} + \frac{1}{\delta_{\alpha}} + \frac{1}{\delta_{\alpha}}\right] \\
&\le \delta \delta_{\alpha}^{-1}(r + 1) (\delta_u + 2)
\end{align*}
\end{boxedcomment}
Also, without loss of generality, $\delta_{\alpha} \le 1$, so that
\begin{align}
A_i'(z - p) - b_i' 
&= - s A_iu + A_i(x - p) - b_i + (A_i' - A_i)(z - x + x - p) + (b_i' - b_i) \nonumber \\
&\le s \|A_i\|\|u\| + 0 + \|A_i' - A_i\|\left[\|z - x\| + \|x - p\|\right] + \|b_i'-b_i\| \nonumber \\
&\le s\delta_u + \delta \left(s + r\right) + \delta \le \lambda \delta \label{reun_eqn1}
\end{align}
so that whenever $A_i'(z - p) > b_i'$,
\begin{align}
\left|A_i'(z - p)\right| 
\ge |b_i'| - \left| A_i'(z - p) - b_i'\right| 
\ge \delta_r\|A_i'\| - \lambda \delta 
\ge \delta_r \delta_u - \lambda \delta \ge \frac 1 2 \delta_r \delta_u. \label{reun_eqn2}
\end{align}
% \Longrightarrow \frac 1 {\left|A_i'(z - p)\right|} \le \frac 2 {\delta_1 \delta_3}. 

% &= A_i(z - p) - b_i + (A_i' - A_i)(z - p) + (b_i' - b_i) \\
% \begin{align*}
% A_i(z - p) - b_i = A_i(x - p) - b_i - s A_iu \le 0 + s \|A_i\| \|u\| \le s\delta_2
% \end{align*}
% so that

% \begin{align*}
% \lambda \delta \le \frac 1 2 \delta_1 \delta_3
% \end{align*}
% \begin{align*}
% (1 - t) A_i'\left[z - p\right] \le b_i' \\
% A_i'\left[(1-t) z + tp - p\right] \le b_i'
% \end{align*}

Dividing \cref{reun_eqn1} by \cref{reun_eqn2}, we find
\begin{align*}
\frac{A'_i (z - p) - b'_i}{A'_i (z - p)} \le \frac{2\lambda}{\delta_r \delta_u}\delta = \gamma \delta \le 1
\end{align*}
which means $t = \max\left\{\gamma \delta, \frac{s}{r+s}\right\} \in [0, 1]$.
Also, note that for each $i \in [m]$, if $A_i'(z - p) > b_i'$, then
\begin{align*}
t \ge \frac{A'_i (z - p) - b'_i}{A'_i (z - p)}
\Longrightarrow (1-t) A'_i (z - p) \le b'_i.
\end{align*}
But if $A_i'(z - p) \le b_i'$, then $t \le 1$ and $b_i' \ge 0$ provide the same identity.
Therefore, 
\begin{align*}
A'(v - p) = A' \bigg[(1-t) z + tp - p\bigg] = (1-t) A' (z - p) \le b'.
\end{align*}
Because $t \in [0, 1]$, \cref{reun_eqn3} and convexity imply $C'(v - p) \le d$.
Lastly, $t \ge \frac{s}{r + s} \Longrightarrow (1 - t) \left(r + s\right) \le r$, so that
\begin{align*}
\|v - p\| 
= \|(1-t)z + t p - p\| 
= (1 - t) \|z - p\| \\
\le (1 - t) \left(\|x - p\| + s\right) 
\le (1 - t) \left(r + s\right) 
\le r
\end{align*}
and $v \in S'$.
\begin{boxedcomment}
Explanation:
\begin{align*}
t \ge \frac{s}{r + s} \\
 tr + ts \ge s \\
- tr - ts \le -s \\
r - tr + s - ts \le r \\
(1 - t) r + (1 - t)s \le r \\
(1 - t) \left(r + s\right) \le r \\
\end{align*}
\end{boxedcomment}

Finally, we will show $\|v - x\| \le \epsilon$.
Notice that $\delta \le \eta = \frac{\epsilon\delta_{\alpha}}{2(r+2)(r + 1)} \Longrightarrow s \le \frac{\epsilon}{4 + 2r - \epsilon}$, which implies two things.
First, it means $s \le \frac{\epsilon}{2r - \epsilon}$ so that
$\frac {s}{1 + s} \le \frac {\epsilon} {2r}. $
Combine this with $\delta \le \frac {\epsilon} {2\gamma r}$ to find that $tr \le \frac 1 2 \epsilon$.
Secondly, it means that
$s \le \frac{\epsilon}{4}$ so that $(1 + t)s \le \frac 1 2 \epsilon$.
Combining these produces,
\begin{align*}
\|x - v\| 
\le \|x - z\| - \|z - v\| 
= s + t \|p - z\| 
\le s + t \left(r + s\right) 
= (1 + t)s + tr \\
\le \frac 1 2 \epsilon + \frac 1 2 \epsilon = \epsilon.
\end{align*}
% = s + \|(1-t) z + tp - z\| 

\begin{boxedcomment}
Explanations:
\begin{align*}
\delta \le \frac{\epsilon\delta_{\alpha}}{2(r+2)(r + 1)}
\le \frac{\epsilon\delta_{\alpha}}{(4 + 2r - \epsilon)(r + 1)} \\
\Longrightarrow \frac{\delta(r + 1)}{\delta_{\alpha}}  \le \frac{\epsilon}{4 + 2r - \epsilon} 
\Longrightarrow s\le \frac{\epsilon}{4 + 2r - \epsilon}
\end{align*}
\begin{align*}
s\le \frac{\epsilon}{(2r - \epsilon)} \Longrightarrow
s(2r - \epsilon)\le \epsilon \Longrightarrow
2rs \le \epsilon(1 + s) \Longrightarrow
\frac {s}{1 + s} \le \frac {\epsilon} {2r}
\end{align*}
\begin{align*}
s \le \frac{\epsilon}{4 + 2r - \epsilon} \le \frac 1 4 \epsilon \Longrightarrow
(1 + t)s \le 2s \le \frac 1 2 \epsilon
\end{align*}
\end{boxedcomment}
\end{proof}

\subsubsection{Simplifed Results}
\label{simplifed_bounded_projection}



% We prove the convergence results found within \cref{limit_of_true_criticallity} by contradiction.
% Namely, we assume the existence of an $\epsilon_{\textrm{lb}} > 0$ within \cref{the_contradiction}, 
% and within \cref{lim_chi_to_zero}, we show that $\epsilon_{\textrm{lb}}$ cannot exist.
% To accomplish this, we define a certain set of pairs of iterates within \cref{define_slb}.
% We generalize this set within following definition.

% \begin{definition}
% \label{criteria_from_contradiction}
% A sequence $S \subseteq\naturals \times \naturals$ satisfies this definition if
% is called a \emph{Cauchy pair-subsequence} if
% Let $l, k \in S \subseteq \naturals$, where $S$ is such that the subsequence $\{\xk\}_{k \in S}$ is Cauchy.
% for any $\epsilon > 0$, there exists $k_0 \in \naturals$ such that if $(k, l) \in S$ with $k, l \ge k_0$, 
% then 
% $\left\|\xk - \xl \right\| \le \epsilon$.
% There exists $x^{\star}$ such that $\lim_{k\to\infty} \xk = x^{\star}$.
% \end{definition}



\begin{lemma}
\label[lemma]{model_gradients_are_cauchy}
Let $\ulb$ and $\slb$, be defined by \cref{define_the_finite_set} and \cref{define_slb}.

\input{assumptions.d/model_gradients_are_cauchy}

If there exists $\elb > 0$ such that $\ulb$ is infinite, 
then for each $i \in [m]$ and any $\epsilon > 0$, there is a $k_0 \in \naturals$
such that if $(k, l) \in \slb$ and $k, l \ge k_0$, then $\left\|\gmcik - \gmcil \right\| \le \epsilon$.
\end{lemma}
\begin{proof}
Let $\epsilon > 0$ be arbitrary.
By \cref{contradiction_portion}, we can choose $k_1 \in \naturals$ such that if $k, l \ge k_1$ and
$k, l \in \slb$ then $\lipgrad \left\|\xk - \xl \right\| \le \frac 1 3 \epsilon$.
By \cref{delta_to_zero}, we can choose an $k_2 \in \naturals$ such that $\kappa_g \dk^2 \le \frac 1 3 \epsilon $ whenever $k \ge k_2$.
Choose $k_0 = \max\{k_1, k_2\}$, and let $d, k \ge k_2$.
By the triangle inequality, \cref{accuracy_is_satisfied}, and \cref{lipschitz_gradients_assumption}:
\begin{align*}
\left\|\nabla \mcik\left(\xk\right) - \nabla \mcil\left(\xl\right) \right\| \\
\le 
\left\|\nabla \mcik\left(\xk\right) - \nabla c_i\left(\xk\right) \right\|
+ \left\|\nabla c_i\left(\xk\right) - \nabla c_i\left(\xl\right)\right\| \\
+ \left \| \nabla c_i\left(\xl\right) - \nabla \mcil\left(\xl\right) \right \| \\
\le \kappa_g \left(\dk^2 + \dl^2\right) + \lipgrad \left\|\xk - \xl \right\|
\le \frac 1 3 \epsilon + \frac 1 3 \epsilon + \frac 1 3 \epsilon = \epsilon.
\end{align*}
\end{proof}
% \left\|\mcik\left(\xk\right) - \mcik\left(\xl\right) \right\| + \left \| \mcik\left(\xl\right) - \mcil\left(\xl\right) \right\|





Let $\minangledelta$ be defined by \cref{minangleassumption_alt} and define
% such that if $\dk \le \minangledelta$ and
\begin{align}
\activeindicesk = \left\{i \in [m] \bigg| -c_i\left(\xk\right) \le \minangledelta \left\|\gmcik\right\| \right\}. \label{define_activeindicesk}
\end{align}


% \begin{align*}
% ? =  \\
% c_i\left(\xk\right) + \gmcik^T \gk
% \end{align*}

The following observation follows directly from the definition:
\begin{lemma}
\label[lemma]{doesntreallyneedtobesaid}
Let $\activeindicesk$, $\zik$, $\minangledelta$ be defined by \cref{define_activeindicesk}, \cref{define_z}, and \cref{minangleassumption_alt} respectively.

If $i \in \activeindicesk$, then $\zik \in B_{\infty}\left(\xk, \minangledelta \right)$.
\end{lemma}
\begin{proof}
If $c_i\left(\xk\right) = 0$, then $\zik = \xk \in B_{\infty}\left(\xk, \minangledelta \right)$.
Otherwise, $\left\|\gmcik\right\| \ge \frac{-c_i\left(\xk\right)}{\minangledelta} \ge 0$, so
\begin{align*}
\zik = \xk - \frac{m^{(k)}_{c_i}\left(\xk\right)}{\left\|\gmcik\right\|^2} \gmcik \\
\Longrightarrow \left\|\xk - \zik \right\|= -\frac{c_i\left(\xk\right)}{\left\|\gmcik\right\|} \le \minangledelta.
\end{align*}
\end{proof}

\begin{lemma}
\label[lemma]{underbound}
Let $\activeindicesk$ be defined by \cref{define_activeindicesk}, and $\mingradepsilon$ and $\mingrad$ be defined by \cref{mingradassumption}.

\input{assumptions.d/underbound}

There exists $k_0 \in \naturals$ such that if $k \ge k_0$, then
for all $i \in \activeindicesk$,
\begin{align*}
\left\|\gmcik\right\| \ge \activegradmin
\end{align*}
where
\begin{align}
\label{define_activegradmin}
\activegradmin = \min\left\{\mingrad, \frac{\mingradepsilon}{\sqrt{n} \minangledelta}\right\}.
\end{align}
\end{lemma}
\begin{proof}

By \cref{mingradassumption}, if $-c_i\left(\xk\right) \le \mingradepsilon$, then $\left\|\nabla c_i\left(\xk\right)\right\| \ge \mingrad \ge \activegradmin$.
On the other hand, if $-c_i\left(\xk\right) \ge \mingradepsilon$ and $i \in \activeindicesk$, by \cref{define_z}
\begin{align*}
\sqrt{n} \minangledelta \ge \left\|\xk - \zik \right\|
= \frac{-c_i\left(\xk\right)}{\left\|\gmcik\right\|}
\ge \frac{\mingradepsilon}{\left\|\gmcik\right\|} \\
\Longrightarrow
\left\|\gmcik\right\| \ge \frac{\mingradepsilon}{\sqrt{n} \minangledelta} \ge \activegradmin.
\end{align*}
\end{proof}


\begin{lemma}
\label[lemma]{yet_another_bound_on_a_u}

Let $\ulb$, $\slb$, and $\activeindicesk$ be defined by \cref{define_the_finite_set}, \cref{define_slb}, and \cref{define_activeindicesk} respectively.

\input{assumptions.d/yet_another_bound_on_a_u}

There exists $k_0 \in \naturals$, scalar $\minactivealpha > 0$, and unit vector $\activedirk \in \Rn$ such that if 
$k \ge k_0$, then $\forall i \in \activeindicesk$:
\begin{align*}
\gmcik^T \activedirk \ge \minactivealpha
\quad \textrm{and} \quad
\nabla c_i\left(\xk\right)^T \activedirk \ge \minactivealpha.
\end{align*}

Furthermore, if there is an $\elb > 0$ such that $\ulb$ is infinite, then for any $(k, l) \in \slb$, $k,l \ge k_0$, and $\forall i \in \activeindicesk$
\begin{align*}
\gmcil^T \activedirk \ge \minactivealpha.
\end{align*}
\end{lemma}
\begin{proof}

Let 
$\minanglealpha$ and $\minangledelta$ be defined by \cref{minangleassumption_alt};
$\kappa_g$ be defined by \cref{accuracy_is_satisfied}; 
and $\activegradmin$ be defined by \cref{define_activegradmin}.

Define 
\begin{align}
\minactivealpha = \frac 1 2 \minanglealpha \activegradmin. \label{define_minactivealpha}
\end{align}

By \cref{delta_to_zero}, we know there exists $k_1 \in \naturals$ such that if $k \ge k_1$, then
\begin{align*}
\dk \le \minangledelta \quad \textrm{and} \quad  \dk \le \sqrt{\frac {\minanglealpha \activegradmin} {2 \kappa_g}}.
\end{align*}
By \cref{doesntreallyneedtobesaid}, we have $\zik \in B_{\infty}\left(\xk, \minangledelta \right)$ for all $i \in \activeindicesk$.
Thus, \cref{minangleassumption_alt} provides a unit vector $\minangledirk\in\Rn$ such that
\begin{align*}
-\frac {\gmcik}{\left\|\gmcik\right\|} ^T\minangledirk \ge \minanglealpha.
\end{align*}
Combining this with \cref{underbound}, we see that there exists and $k_2 \in \naturals$ such that for all $i \in \mathbb I^{(k)}$, $k \ge k_2$:
\begin{align*}
 \gmcik^T\left(-\minangledirk\right) \ge \minanglealpha \activegradmin
\end{align*}
Also, by \cref{accuracy_is_satisfied}, we have
\begin{align*}
\left\|\gmcik - \nabla c_i\left(\xk\right) \right\| \le \kappa_g \dk^2 \le \frac 1 2 \minanglealpha \activegradmin
\end{align*}
so that
\begin{align*}
\nabla c_i\left(\xk\right)^T \left(-\minangledirk\right) \\
= \gmcik^T\left(-\minangledirk\right) + \left(\gmcik - \nabla c_i\left(\xk\right) \right)^T\left(-\minangledirk\right) \\
\ge \minanglealpha \activegradmin - \left\|\gmcik - \nabla c_i\left(\xk\right) \right\|\left\|\minangledirk\right\|
\ge \minanglealpha \activegradmin - \frac 1 2 \minanglealpha \activegradmin = \minactivealpha.
\end{align*}

Furthermore, if there is a $\elb > 0$ such that $\ulb$ is infinite,
then \cref{model_gradients_are_cauchy} provides a $k_3 \in \naturals$ such that if $(k, l) \in \slb$ and $k, l \ge k_3$
\begin{align*}
\left\|\gmcik - \gmcil\right\| \le \frac {\minanglealpha \activegradmin} {2}.
\end{align*}
Thus,
\begin{align*}
\left\|\gmcil \left(-\minangledirk\right) \right\| \\
= \left\|\gmcik^T\left(-\minangledirk\right) + \left(\gmcik - \gmcil \right)^T\left(-\minangledirk\right)\right\| \\
\ge \minanglealpha \activegradmin - \left\|\gmcik - \gmcil \right\|\left\|\minangledirk\right\|
\ge \minanglealpha \activegradmin - \frac {\minanglealpha \activegradmin} {2} = \minactivealpha.
\end{align*}
Thus, we must only set $k_0 = \max \left\{k_1, k_2, k_3\right\}$ and $\activedirk=-\minangledirk$.
\end{proof}




% 
% 
% 
% \begin{align*}
% \left\|z - p\right\| = r \\
% r \le \left\|z' - p\right\| = r + \epsilon \\
% (z - p)^T (z' - p) \ge \|z - p\|^2
% \end{align*}
% 
% \begin{align*}
% \left\|z - z'\right\|
% \end{align*}
% 
% 
% 


% \begin{lemma}
% aabbcc
% \end{lemma}
% \begin{proof}
% By the mean value theorem, there exists $t \in [0, 1]$ such that
% \begin{align*}
% \end{align*}
% \end{proof}

Define
\begin{align}
\truefeasiblek &= \left\{ x \in \Rn \bigg| c_i\left(\xk\right) + \nabla c_i\left(\xk\right)^T \left(x - \xk\right) \le 0 \; \forall i \in [m] \right\}. \label{define_truefeasiblek}
\end{align}

% \begin{boxedcomment}
% Use a superscript of $(k, l)$
% \end{boxedcomment}


% https://mirror.las.iastate.edu/tex-archive/info/symbols/comprehensive/symbols-a4.pdf#page=123

Let $\feasiblek$, $\truefeasiblek$ be defined by \cref{define_feasiblek} and \cref{define_truefeasiblek} respectively, 
and for any fixed $k$, define

% \Amk &= \left\{ i \in [m] \bigg | c_i\left(\xk\right) + \gmcik^T\left( P_{\feasiblek}\left(\xk - \gk\right) - \xk\right) = 0\right\}, \label{tbbt_a1} \\
\begin{align}
\Alk &= \left\{ i \in [m] \bigg | c_i\left(\xl\right) + \gmcil^T\left( P_{\feasiblel}\left(\xk - \gk\right) - \xl\right) = 0\right\}, \label{tbbt_a2} \\
\Ack &= \left\{ i \in [m] \bigg | c_i\left(\xk\right) + \nabla c\left(\xk\right)^T\left( P_{\truefeasiblek}\left(\xk - \gk\right) - \xk\right) = 0\right\}. \label{tbbt_a3}
\end{align}

\begin{lemma}
\label[lemma]{norm_of_active_constraints_bounded_below}
Let $\Alk$ and $\Ack$ be defined by \cref{tbbt_a2} and \cref{tbbt_a3}.
Let $\ulb$ and $\slb$, be defined by \cref{define_the_finite_set} and \cref{define_slb}.

\input{assumptions.d/norm_of_active_constraints_bounded_below}

There exists $\deltalb > 0$ and $k_0 \in \naturals$ such that,
\begin{itemize}
\item if $k \ge k_0$ and $i \in \Amk \cup \Ack$, then
\begin{align*}
\left\|\gmcik\right\| \ge \deltalb \quad \textrm{and} \quad
\left\|\nabla c_i\left(\xk\right) \right\| \ge \deltalb.
\end{align*}
\item if there exists an $\elb > 0$ such that $\ulb$ is infinite
$k, l \ge k_0$, $(k, l) \in \slb$, and $i \in \Amk \cup \Alk$, then
\begin{align*}
\left\|\gmcil\right\| \ge \deltalb.
\end{align*}
\end{itemize}
\end{lemma}
\begin{proof}

Let $\mingradepsilon > 0$ and $\mingrad > 0$ be defined by \cref{mingradassumption}
and $\maxmodelgrad \ge 1$ be defined by \cref{i_thought_i_proved_this_already}.
% and $\maxgrad$ be defined by \cref{bounded_gradients_lemma}.
Define
\begin{align*}
\deltalb = \min\left\{\frac 1 3 \mingrad, \frac 1 {6} \frac{\mingradepsilon}{\maxmodelgrad} \right\}.
\end{align*}

Before going through several cases, we make four observations.
First, \cref{i_thought_i_proved_this_already} and \cref{delta_to_zero} imply the existence of $k_1 \in \naturals$
such that if $k \ge k_1$, then as $\xk \in \feasiblek$ which is convex,
\begin{align*}
\left\|P_{\feasiblek}\left(\xk - \gk\right) - \xk\right\| 
\le \left\|\xk - \gk - \xk \right\| \le \maxmodelgrad.
\end{align*}
Secondly, \cref{accuracy_is_satisfied} and \cref{delta_to_zero} imply the existence of $k_2 \in \naturals$ such that if $k \ge k_2$, then
\begin{align}
\left\|\gmcik - \nabla c_i\left(\xk\right)\right\| \le \kappa_g \dk^2 \le \deltalb. \label{hmoydihtp_eqn1}
\end{align}
Similarily, \cref{model_gradients_are_cauchy} implies the existence of $k_3 \in \naturals$ such that if $k, l \ge k_3$, then 
\begin{align}
\left\|\gmcik - \gmcil\right\| \le \deltalb. \label{hmoydihtp_eqn2}
\end{align}
% \min\left \{\frac 1 2 \mingrad, \deltalb \right\}
Lastly, by the mean value theorem, for each $i \in [m]$, there exists $t_i \in [0, 1]$ such that
\begin{align*}
c_i\left(\xl\right) = c_i\left(\xk\right) + \nabla c_i\left((1 - t_i)\xk + t_i \xl\right)^T\left(\xl - \xk\right).
\end{align*}
Thus, if $(k, l) \in \slb$, \cref{contradiction_portion} and \cref{bounded_gradients_lemma}
imply the existence of $\maxgrad > 0$ and $k_4 \in \naturals$ such that if $k, l \ge k_4$, 
then
\begin{align}
\left\|c_i\left(\xk\right) - c_i\left(\xl\right) \right\| \le \maxgrad \left\|\xl - \xk\right\| \le \deltalb. \label{hmoydihtp_eqn3}
\end{align}
We choose $k_0 = \max\left\{k_1, k_2, k_3, k_4\right\}$, and assume $k, l \ge k_0$.

\paragraph*{Case 1}
If $-c_i\left(\xl\right) \le \mingradepsilon$, then \cref{mingradassumption} implies $\left\| \nabla c_i\left(\xl\right) \right\| \ge \mingrad \ge 3 \deltalb$.

\paragraph*{Case 2}
Suppose that $-c_i\left(\xk\right) > \mingradepsilon$ and $i \in \Ack$.
Then
\begin{align*}
\left\|\nabla c_i\left(\xk\right)  \right\| \maxmodelgrad
\ge \nabla c_i\left(\xk\right) ^T\left(P_{\truefeasiblek}\left(\xk - \gk\right) - \xk\right) \\
= -c_i\left(\xk\right) > \mingradepsilon \ge 6 \deltalb \maxmodelgrad.
\end{align*}
and $\left\|\nabla c_i\left(\xk\right)  \right\| \ge 6 \deltalb$.

% (1 - x) = 4x

\paragraph*{Case 3}
Suppose that $-c_i\left(\xk\right) > \mingradepsilon$ and $i \in \Amk$.
Then
\begin{align*}
\left\|\gmcik\right\| \maxmodelgrad
\ge \gmcik^T\left[P_{\feasiblek}\left(\xk - \gk\right) - \xk\right] \\
= -c_i\left(\xk\right) > \mingradepsilon \ge 6 \deltalb \maxmodelgrad.
\end{align*}
so that $\left\|\gmcik\right\| \ge 6 \deltalb$, and \cref{hmoydihtp_eqn1} implies $\| \nabla c_i\left(\xl\right) \| \ge 5 \deltalb$.

\paragraph*{Case 4}
Suppose that $-c_i\left(\xk\right) > \mingradepsilon$ and $i \in \Alk$.
Then \cref{hmoydihtp_eqn3} implies that $-c_i\left(\xl\right) > \mingradepsilon - \deltalb \ge (6\maxmodelgrad - 1) \deltalb \ge 5 \deltalb$, so that
\begin{align*}
\left\|\gmcil\right\| \maxmodelgrad
\ge \gmcil^T\left(P_{\feasiblek}\left(\xl - \gk\right) - \xl\right) \\
= -c_i\left(\xl\right) > 5 \deltalb \maxmodelgrad.
\end{align*}
Hence, $\left\|\gmcil\right\| \ge 5 \deltalb$.
Then by \cref{hmoydihtp_eqn2}, $\left\|\gmcik\right\| \ge 4 \deltalb$, and by \cref{hmoydihtp_eqn1} $\left\|\nabla c_i\left(\xk\right) \right\| \ge 3 \deltalb$.


% Without loss of generality, we may assume that $\maxmodelgrad \ge 1$, so that 



Throughout the cases, we have established if $i \in \Amk \cup \Ack \cup \Alk$, then $\left\| \nabla c_i\left(\xk\right) \right\| \ge 3 \deltalb$.
Then \cref{hmoydihtp_eqn1} implies $\left\|\gmcik\right\| \ge 2 \deltalb$, 
and \cref{hmoydihtp_eqn2} implies $\left\|\gmcil\right\| \ge \deltalb$.
\end{proof}

% 
% 
% \begin{boxedcomment}
% Use a superscript of $(k, l)$
% \end{boxedcomment}

With $\feasiblek$, $\truefeasiblek$ be defined by \cref{define_feasiblek} and \cref{define_truefeasiblek} respectively, define
\begin{align}
\zlk =& P_{\feasiblel}\left(\xk - \gk\right), \label{define_zlk} \\
\zck =& P_{\truefeasiblek}\left(\xk - \gk\right). \label{define_zck}
\end{align}

% \begin{assumptions}
% Suppose that 
% \cref{bounded_gradients_lemma}
% and the assumptions for
% \cref{accuracy_is_satisfied},
% \cref{i_thought_i_proved_this_already},
% \cref{delta_to_zero},
% \cref{board_tied_to_sphere},
% \cref{the_lemma_to_end_all_lemmas},
% \cref{model_gradients_are_cauchy},
% \cref{yet_another_bound_on_a_u},
% and \cref{norm_of_active_constraints_bounded_below}
% are satisfied.
% \end{assumptions}


\begin{theorem}
\label{bounded_projection_theorem}
Let $\feasiblek$, $\truefeasiblek$ be defined by \cref{define_feasiblek} and \cref{define_truefeasiblek} respectively.
Let $\zlk$ and $\zck$ be defined by \cref{define_zlk} and \cref{define_zck} respectively.
Let $\ulb$ and $\slb$, be defined by \cref{define_the_finite_set} and \cref{define_slb}.

\input{assumptions.d/bounded_projection_theorem}


For any $\epsilon > 0$, there exists $k_0 \in \naturals$ such that
\begin{itemize}
\item if $k \ge k_0$, then
$
\left\|\zmk - \zck\right\| \le \epsilon
$
\item if there exists an $\elb > 0$ such that $\ulb$ is infinite; $k, l \ge k_0$; and $(k, l) \in \slb$, then 
$
\left\|\zmk - \zlk\right\| \le \epsilon.
$
\end{itemize}
\end{theorem}
\begin{proof}

For any $\mathcal U \subseteq [m]$, define
\begin{align*}
\feasiblek(\mathcal U)  = \left\{x \in \Rn \bigg| c_i\left(\xk\right) + \gmcik^T\left(x - \xk\right) \le 0 \; \forall i \in \mathcal U \right\} \\
\truefeasiblek(\mathcal U)  = \left\{x \in \Rn \bigg| c_i\left(\xk\right) + \nabla c_i\left(\xk\right)^T\left(x - \xk\right) \le 0 \; \forall i \in \mathcal U \right\}.
\end{align*}
Notice that if $\Alk$, $\Ack$
are defined by
\cref{tbbt_a2}, and \cref{tbbt_a3},
then $\Amk$ is the set of indices for which 
\begin{align*}
c_i\left(\xk\right) + \gmcik^T\left(\zmk - \xk\right) = 0.
\end{align*}
Thus, $\zmk = P_{\feasiblek(\mathcal U)}\left(\xk - \gk\right)$ for any $\mathcal U \subseteq [m]$ with $\Amk \subseteq \mathcal U$.
In particular,
\begin{align*}
\zmk =& P_{\feasiblek\left(\Amk \cup \Ack\right)}\left(\xk - \gk\right) \\
=& P_{\feasiblek\left(\Amk \cup \Alk\right)}\left(\xk - \gk\right)\\
\zlk =& P_{\feasiblel\left(\Amk \cup \Alk\right)}\left(\xk - \gk\right) \\
\zck =& P_{\truefeasiblek\left(\Amk \cup \Ack\right)}\left(\xk - \gk\right).
\end{align*}

Further, notice that by \cref{i_thought_i_proved_this_already} and \cref{delta_to_zero}, there exists $k_1 \in \naturals$ such that if $k, l \ge k_1$, 
then
$\left\|\gk\right\| \le \maxmodelgrad$
% and $\left\|\gl\right\| \le \maxmodelgrad$
.
Combining this with $\xk \in \feasible \subseteq \feasiblel\left(\Amk \cup \Alk\right)$, we have 
\begin{align}
\begin{array}{ccc}
\zmk &=& P_{\feasiblek\left(\Amk \cup \Ack\right) \cap B_2\left(\xk, \maxmodelgrad\right)}\left(\xk - \gk\right)  \\
     &=& P_{\feasiblek\left(\Amk \cup \Alk\right) \cap B_2\left(\xk, \maxmodelgrad\right)}\left(\xk - \gk\right), \\
\zlk &=& P_{\feasiblel\left(\Amk \cup \Alk\right) \cap B_2\left(\xk, \maxmodelgrad\right)}\left(\xk - \gk\right), \\
\zck &=& P_{\truefeasiblek\left(\Amk \cup \Ack\right) \cap B_2\left(\xk, \maxmodelgrad\right)}\left(\xk - \gk\right).
\end{array}\label{cbpt_eqn1}
\end{align}

We wish to apply \cref{the_lemma_to_end_all_lemmas} by assigning values to
$A$, $A'$, $b$, $b'$, $C$, $C'$, $d$, $d'$, $u$, $\delta_l$, $\delta_u$, $\delta_{\alpha}$, and $\delta_r$.
We can let $\activeindicesk$ be defined by \cref{define_activeindicesk}, and partition either

\paragraph*{Case 1}
\begin{align*}
\begin{array}{cclccl}
C  &\gets& \left[\nabla m^{(k)}_c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Ack\right)}, &
C' &\gets& \left[\nabla c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Ack\right)}, \\
A  &\gets& \left[\nabla m^{(k)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Ack\right)}, &
A' &\gets& \left[\nabla c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Ack\right)} \\
d  &\gets& \left[-m^{(k)}_c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Ack\right)}, &
d' &\gets& \left[-c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Ack\right)}, \\
b  &\gets& \left[-m^{(k)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Ack\right)}, &
b' &\gets& \left[-c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Ack\right)} \\
\end{array}
\end{align*}
or
\paragraph*{Case 2}
\begin{align*}
\begin{array}{cclccl}
C  &\gets& \left[\nabla m^{(k)}_c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Alk \right)}, &
C' &\gets& \left[\nabla m^{(l)}\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Alk \right)}, \\
A  &\gets& \left[\nabla m^{(k)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Alk \right)}, &
A' &\gets& \left[\nabla m^{(l)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Alk \right)} \\
d  &\gets& \left[-m^{(k)}_c\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Alk \right)}, &
d' &\gets& \left[-m^{(l)}\left(\xk\right)\right]_{\activeindicesk \cap \left(\Amk \cup \Alk \right)}, \\
b  &\gets& \left[-m^{(k)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Alk \right)}, &
b' &\gets& \left[-m^{(l)}_c\left(\xk\right)\right]_{\left([m] \setminus \activeindicesk\right) \cap \left(\Amk \cup \Alk \right)}.
\end{array}
\end{align*}
We see from \cref{define_activeindicesk}, that in either case, with the assignment $\delta_r = \minangledelta$, we immediately have 
$\delta_r \|A_i\| \le b_i$ and $\delta_r \|A_i'\| \le b_i'$.
Also, because $\xk$ and $\xl$ are both feasible, $b, b', d, d' \ge 0$.
By \cref{norm_of_active_constraints_bounded_below}, there exists $\delta_l = \deltalb > 0$ and $k_2 \in \naturals$ such that if $k, l \ge k_2$, then
$\delta_l \le \|A_i\|$ and $\delta_l \le \|A'_i\|$.
By \cref{i_thought_i_proved_this_already} and \cref{delta_to_zero}, there exists $k_3 \in \naturals$ and $\delta_u = \maxmodelgrad > 0$ such that if $k, l \ge k_3$,
then
$\|A_i\| \le \delta_u$ and $\|A_i'\| \le \delta_u$.
Also, by \cref{yet_another_bound_on_a_u}, there exists $k_4 \in \naturals$, unit vector $u = \activedirk$, and $\delta_{\alpha} = \minactivealpha > 0$ 
such that if $k, l \ge k_4$, then
$Cu \ge \delta_{\alpha} e$ and $C'u \ge \delta_{\alpha} e$.
By the mean value theorem, for each $i \in [m]$, there exists $t_i \in [0, 1]$ such that
\begin{align*}
c_i\left(\xl\right) = c_i\left(\xk\right) + \nabla c_i\left((1 - t_i)\xk + t_i \xl\right)^T\left(\xl - \xk\right).
\end{align*}
Thus, for any $\delta > 0$, we have
$(k, l) \in \slb$, \cref{contradiction_portion}, and
\cref{bounded_gradients_lemma}, there exists $\maxgrad > 0$ and $k_5 \in \naturals$ such that if $k, l \ge k_5$, 
then
\begin{align}
\left\|c_i\left(\xk\right) - c_i\left(\xl\right) \right\| \le \maxgrad \left\|\xl - \xk\right\| \le \delta. \label{cbpt_eqn2}
\end{align}
Also, by \cref{delta_to_zero}, \cref{accuracy_is_satisfied}, and \cref{model_gradients_are_cauchy}, 
there exists $k_6 \in \naturals$ such that if $k, l \ge k_6$, then
\begin{align}
\left\|\gmcik - \nabla c_i\left(\xk\right)\right\| \le \delta \quad \textrm{and} \quad
\left\|\gmcik - \gmcil\right\| \le \delta. \label{cbpt_eqn3}
\end{align}
Now, \cref{cbpt_eqn2}, \cref{cbpt_eqn3}, and $m_{c_i}^{(k)}\left(\xk\right) = c_i\left(\xk\right) \forall i \in[m], \forall k \ge \max\left\{k_5, k_6\right\}$ imply
\begin{align*}
\begin{array}{cccc}
\|A - A'\|_{\infty} \le \delta,	& \|b - b'\|_{\infty} \le \delta,		& \|C - C'\|_{\infty} \le \delta,	& \|d - d'\|_{\infty} \le \delta.
\end{array}
\end{align*}
Finally, we assign $\epsilon' = \left(\frac {\epsilon}{8\left(1 + \maxmodelgrad\right)}\right)^2$,
$r \gets \maxmodelgrad$ and $p \gets \xk$ and use \cref{the_lemma_to_end_all_lemmas}
to conclude that if $k, l \ge k_0 = \max\left\{k_1, k_2, k_3, k_4, k_5, k_6\right\}$ and
\begin{align*}
\begin{array}{ccccc}
\mathcal S  &= \bigg\{x \in \Rn \bigg|& A (x - p) \le b ,& C (x - p) \le d ,& \|x - p\| \le r \bigg\}, \\
\mathcal S' &= \bigg\{x \in \Rn \bigg|& A'(x - p) \le b',& C'(x - p) \le d',& \|x - p\| \le r \bigg\}
\end{array}
\end{align*}
then $\convexdistance(\mathcal S, \mathcal S') \le \epsilon'$ where $\convexdistance$ is defined by \cref{define_a_difference_between_sets}.
By using $g \gets \xk - \gk$, \cref{board_tied_to_sphere} implies
\begin{align*}
\left\|P_{\mathcal S}(g) - P_{\mathcal S'}(g)\right\| 
\le 8\left(1 + \left\|P_{\mathcal S \cap \mathcal S'}(g) - g\right\|\right) \sqrt{\epsilon'} 
\le 8\left(1 + \left\|\gk\right\|\right) \sqrt{\epsilon'} 
\le \epsilon.
\end{align*}
Notice that in Case 1, we have 
\begin{align*}
\mathcal S  = \feasiblek\left(\Amk \cup \Ack \right) \cap B_2\left(\xk, \maxmodelgrad\right) \\ 
\textrm{and} \quad \mathcal S' = \truefeasiblek\left(\Amk \cup \Ack \right) \cap B_2\left(\xk, \maxmodelgrad\right), 
\end{align*}
while in Case 2 we have 
\begin{align*}
\mathcal S  = \feasiblek\left(\Amk \cup \Alk\right) \cap B_2\left(\xk, \maxmodelgrad\right) \\
\textrm{and} \quad \mathcal S' = \feasiblel\left(\Amk \cup \Alk \right) \cap B_2\left(\xk, \maxmodelgrad\right).
\end{align*}
Therefore, by \cref{cbpt_eqn1}, $\|\zmk - \zck\| \le \epsilon$ and $\|\zmk - \zlk\| \le \epsilon$.
\end{proof}


\subsection{Convergence}
\label{convergence_section}


\subsubsection{Criticallity Goes to Zero, Part 2}
\label{limit_of_criticallity_to_zero}


\begin{lemma}
\label[lemma]{lim_chi_to_zero}
Let $\chik$ be defined by \cref{define_criticality_measure}.

\input{assumptions.d/lim_chi_to_zero}

If $\gammasm > 0$, then $\lim_{k\to\infty}\chik=0$.
\end{lemma}


\begin{proof}
Let $\ulb$, $\slb$, $\reduceiterates$, and $\miniterates$ be defined by 
\cref{define_the_finite_set}, \cref{define_slb}, \cref{define_reduceiterates}, and \cref{define_miniterates}.
Suppose for a contradiction that there exists a $\elb$ such that $\ulb = \left\{k \in \naturals | \chik \ge \elb \right\}$ is infinite.
By \cref{delta_to_zero}, there exists $k_1 \in \naturals$ such that if $k \ge k_1$, then $\dk \le \dlb$ as defined in \cref{define_delta_lb}.
If $k \in \ulb$ with $k \ge k_0$, then $k \in \reduceiterates \subset \miniterates$ by \cref{mathcal_k_subset_bar_s}.
% \begin{align*}
% \dk \le \min\{\frac{\chik}{\maxhessian}, \frac{(1-\gammasm)\chik}{c}, \left(\frac 1 {\kappa_{\chi}}  \epsilon \right)^{\frac 1 {p_{\Delta}}}, 1\}
% \end{align*}
% and therefore $k \in \miniterates \subset S$.

Let $(k, l) \in \slb$ be given with $k \ge k_1$.
This means that $\chik - \chi^{(l)} \ge \frac {\epsilon} 2 $.
We know by \cref{lipschitz_gradients_assumption} and \cref{accuracy_is_satisfied} that there is a $\kappa_g > 0$ and $\lipgrad>0$ such that
\begin{align}
\left\|\gk - \gl\right\| \nonumber 
\le \left\|\gk - \gradf\left(\xk\right)\right\| \\
+ \left\|\gradf\left(\xk\right) - \gradf\left(\xl\right)\right\| 
+ \left\|\gradf\left(\xl\right) - \gl \right\| \nonumber \\
\le \kappa_g \left(\dk^2 + \dl^2\right) + \lipgrad \left\|\xk - \xl \right\| \label{chi2zero2_comp1}
\end{align}


% Let $\zmk$, $\zlk$, $\zck$ be defined by \cref{define_zmk}, \cref{define_zlk}, \cref{define_zck} respectively.
Let $\zlk$ be defined by \cref{define_zlk}.
Using the triangle inequality, the contraction property of projections, 
\cref{define_criticality_measure}, and \cref{chi2zero2_comp1} we have 
\begin{align*}
\frac{\epsilon}{2} \le \chik - \chi^{(l)} \\
=     \left\|P_{\feasiblek}\left(\xk - \gk\right) - \xk\right \| 
    - \left\|P_{\feasiblel}\left(\xl - \gl\right) - \xl\right \| \\
\le   \left\|P_{\feasiblek}\left(\xk - \gk\right) - \xk
    -  P_{\feasiblel}\left(\xl - \gl\right) + \xl\right \| \\
 \le  \left\|\xk - \xl\right\|
 + \left\|P_{\feasiblek}\left(\xk - \gk\right) -  P_{\feasiblel}\left(\xk - \gk\right) \right\| \\
 + \left\|P_{\feasiblel}\left(\xk - \gk\right) -  P_{\feasiblel}\left(\xl - \gl\right) \right\| \\
\le \left\|\xk - \xl\right\| + \left\|\zmk - \zlk\right\| 
+ \left\| \xk - \gk - \xl + \gl  \right\| \\
\le \left(2 + \lipgrad\right) \left\|\xk - \xl\right\| 
+ \kappa_g \left(\dk^2 + \dl^2\right)
+ \left\|\zmk - \zlk\right\|
\end{align*}
        
% \begin{align*}
% \le\left\|\xk - x^{(l_k)}\right\| + \| p^{(k\to k)} - p^{(k\to l)}\| + \left\|\xk - \gk - x^{(l_k)} - {\nabla m_f^{(l_k)}\left(x^{(l_k)}\right)}\right\| \\
% \le 2\left\|\xk - x^{(l_k)}\right\| + \| p^{(k\to k)} - p^{(k\to l)}\| + \left\|\gk - {\nabla m_f^{(l_k)}\left(x^{(l_k)}\right)}\right\|\\
% =   (2 + \lipgrad) \left\|\xk - x^{(l_k)}\right\| + \| p^{(k\to k)} - p^{(k\to l)}\| + \kappa_g \left(\dk^2 + \Delta_{l_k}^2\right)
% \end{align*}


% \le 2\left\|\xk - x^{(l_k)}\right\| + \| p^{(k)} - p^{(k_l)}\| + \left\|\gk - \gradf(\xk)\right\| + \left\|\gradf(\xk) - \gradf(x^{(l_k)})\right\| + \left\|\gradf(x^{(l_k)}) - {\nabla m_f^{(l_k)}\left(x^{(l_k)}\right)}\right\| \\
% \le \left(2 + \lipgrad\right)\left\|\xk - x^{(l_k)}\right\| + \| p^{(k)} - p^{(k_l)}\| + \left\|\gk - \gradf(\xk)\right\| + \left\|\gradf(x^{(l_k)}) - {\nabla m_f^{(l_k)}\left(x^{(l_k)}\right)}\right\|.
% \le\left\|\xk - x^{(l_k)}\right\| +  \left\|P_{\Omega_1}\left(\xk - \gk\right) - P_{\Omega_2}\left(x^{(l_k)} - g^{(l_k)}\right)\right\| \\

So that
\begin{align}
\frac{\epsilon_{lb}} 2 \le \left(2 + \lipgrad\right) \left\|\xk - \xl\right\| 
+ \kappa_g \left(\dk^2 + \dl^2\right)
+ \left\|\zmk - \zlk\right\|
\label{chi2zero2_conv}.
\end{align}

% Because we have assumed \cref{the_contradiction}, we can apply \cref{contradiction_portion} so that 
% $S_{lb}$ defined \cref{define_slb} satisfies \cref{criteria_from_contradiction}.
We can then apply \cref{bounded_projection_theorem} and \cref{delta_to_zero} to conclude that the entire right hand side of \cref{chi2zero2_conv} goes to zero.
This is a contradiction, and there is no such $\elb > 0$.
\end{proof}

\subsubsection{Convergence of Criticality Measure}
\label{limit_of_true_criticallity}


\begin{lemma}
\label[lemma]{the_convergence_lemma}

Let
$\truefeasiblek$
be defined by
\cref{define_truefeasiblek}.

\input{assumptions.d/the_convergence_lemma}

If $\gammasm = 0$, then
\begin{align*}
\liminf_{k\to\infty} \left\|P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| = 0.
\end{align*}

If $\gammasm > 0$, then
\begin{align*}
\lim_{k\to\infty} \left\|P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| = 0.
\end{align*}

\end{lemma}

\begin{proof}
Let 
$\feasiblek$, $\zlk$, $\zck$
be defined by 
\cref{define_feasiblek}, \cref{define_zlk}, \cref{define_zck}
respectively.
Let $\epsilon > 0$ be fixed.
By the triangle inequality, and the contraction property of projections, we have
\begin{align}\left \|
 P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right)
-P_{\feasiblek}\left(\xk - \gk \right)
\right\| \nonumber \\
\le 
\bigg \|
 P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right) 
-P_{\truefeasiblek}\left(\xk - \gk \right) \nonumber \\
+P_{\truefeasiblek}\left(\xk - \gk \right)
-P_{\feasiblek}\left(\xk - \gk \right)
\bigg\| \nonumber \\
\le \left\|
\xk - \gradf\left(\xk\right) - \xk + \gk
\right\| + \left\|\zck - \zmk\right\| \nonumber \\
= \left\|\gk - \gradf\left(\xk\right)\right\| + \left\|\zck - \zmk\right\| \label{single_star}
\end{align}
With $\chik$ defined by \cref{define_criticality_measure}, the triangle inequality implies
\begin{align*}
\left\|P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| \\
= \bigg\|
 P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right)
-P_{\feasiblek}\left(\xk - \gk \right) \\
+P_{\feasiblek}\left(\xk - \gk \right)
- \xk\bigg\| \\
\le \left\|\gk - \gradf\left(\xk\right)\right\| + \left\|\zck - \zmk\right\| + \chik
\end{align*}
by \cref{single_star}.


Using \cref{delta_to_zero}, \cref{accuracy_is_satisfied}, \cref{bounded_projection_theorem}, and \cref{liminf_chi_to_zero},
for any $\epsilon > 0$, there is a $k_0 \in \naturals$ such that if $k \ge k_0$, then
\begin{align*}
\left\|\gk - \gradf\left(\xk\right)\right\| \le \kappa_g \dk^2 \le \frac 1 3 \epsilon,
\quad
\left\|\zck - \zmk\right\| \le \frac 1 3 \epsilon,
\quad \textrm{and} \quad
\chik \le \frac 1 3 \epsilon.
\end{align*}
Because $\epsilon$ was arbitrary, it follows that
\begin{align*}
\lim_{k\to\infty} \left[\left\|\gk - \gradf\left(\xk\right)\right\| + \left\|\zck - \zmk\right\|\right] = 0.
\end{align*}
\end{proof}


\section{Convex Constraints}
When the constraints are convex, the analysis becomes simpler.
Namely, we can use \cref{restore_feasible_ellipsoid_convex} to construct an algorithm satisfying \cref{restorability_assumption}.
To this end, we replace \cref{restorability_assumption} with \cref{constraints_are_convex}.
Within \cref{the_convergence_theorem} we extend \cref{the_convergence_lemma} to the common criticality measure used for convex constraints.
Namely, for convex constraints, the projection of $\xk - \nabla f\left(\xk\right)$ onto the feasible set is well defined.


\begin{assumption}
\label{constraints_are_convex}
The functions $c_i$ for all $i \in [m]$ are convex in $ \domain $. That is, 
% c_i(x) \le 0 \wedge c_i(y) \le 0 \Longrightarrow
\begin{align*}
c_i(\zeta x + (1 - \zeta) y) \le 0 \quad \forall x, y \in \domain, \zeta \in [0, 1].
\end{align*}
\end{assumption}


\subsection{Convex Restoration}
\label{convex_restoration}
When the constraints are convex, the method for constructing a new sample set near our current iterate becomes easier.
We know that during the initialization, we began with a set of sample points from a feasible, non-empty ellipsoid.
This means that the constructing convex hull of previously evaluated points has a non-empty interior.

% We can then construct a feasible ellipsoid within the convex hull of the previous feasible points and the current iterate.
Note that although constructing the convex hull of an an arbitrary set of points is computationally expensive,
in practice it is possible to do with a sufficiently small set of sample points.
% In dimension $n$, the number of sample points $p_1$ required to construct a quadratic model is $O(n^2)$.
For example, in dimension $n$, if a subset of previously evaluated points of size $n+1$ is found, then not all hyperplanes of the convex hull need to be enumerated.
Only those that run through the point $\xk$.
As it takes $n$ points to construct a hyperplane, and one of them is $\xk$, we must only consider the subsets of size $n-1$ of the of the $n+1$ sample points.
Thus, to construct a hyperplane used in the construction of $\pi^{\textrm{hull}}$,
we consider the hyper-planes running through $\xk$ to any other subset of size $n - 1$ of these points,
which makes for $n+1$ choose $n-1$ points, or:
% In dimension $n$, the number of sample points $p_1$ required to construct a linear model of the constraints $n$.
% Choose a subset of $n+1$ points in the last lambda poised set that have a non-empty interior.
\begin{align*}
\frac{p_1!}{p_1!(p_1 - n)!} \\
\frac{(n+1)!}{(n+1)!(n-1)!} = \frac{(n+1)!}{(n-1)!\left(n+1 - (n-1)\right)!} = \frac{n(n+1) }{2}
\end{align*}

% \begin{algorithm}[H]
%     \caption{Restore a feasible ellipsoid}
%     \label{restore_feasible_ellipsoid}
%     \begin{itemize}
%         \item[\textbf{Step 0}] \textbf{(Initialization)} \\
%             Feasible ellipsoid, current iterate
%             
%         \item[\textbf{Step 1}] \textbf{(Construct Ellipsoid within the convex hull)} \\
%         	A sphere works.
%     \end{itemize}
% \end{algorithm}
% 
% \begin{boxedcomment}
% Fill this in.
% \end{boxedcomment}
% This is why we require an initial feasible ellipsoid.

% \begin{align}
% \label{construct_ellipsoid_in_hull}
% \begin{array}{ccc}
% \max_{} & t & \\
% \textrm{s.t.} & & \\
% \end{array}
% \end{align}


To do this, we first construct the convex hull of the points 
Let $Y = y^{(0)}, y^{(1)}, \ldots, y^{(p)}$ be the set of sample points used in the previous iteration.

\begin{algorithm}[H]
    \caption{Restore a feasible ellipsoid with convex constraints}
    \label{restore_feasible_ellipsoid_convex}
    \begin{itemize}
        \item[\textbf{Step 0}] \textbf{(Initialization)} \\
            $P_{\textrm{hull}} = \emptyset$
            
        \item[\textbf{Step 1}] \textbf{(Potentially add hyperplane)} \\
	    For each subset $S \subseteq Y \cup \{x^{(k-1)}\}$ with $|S| = n - 1$, construct the hyplerplane $ax\le b$ running through the points $S \cup \xk$.
	    If the inequality $ap \le b$ is valid for each $p \in Y \cup \{x^{(k-1)}\}$, add the pair $(a, b)$ to $P_{\textrm{hull}}$.
	
	\item[\textbf{Step 1}] \textbf{(Construct ellipsoid)} \\
	   Construct an ellipsoid within $P_{\textrm{hull}} \cap \tr$ \\
    \end{itemize}
\end{algorithm}

The final step of constructing the ellipsoid within the vertex of the polyhedron is done several times throughout this thesis.
For completeness, we describe the process here.

Let $P_{\textrm{hull}}$ be described with constraints:
\begin{align*}
P_{\textrm{hull}} = \left\{x \in \Rn \bigg | A^{\textrm{hull}} x \le b^{\textrm{hull}}, \left\|A_i^{\textrm{hull}}\right\| = 1 \forall i \right\}
\end{align*}
and let $\mathcal A^{\textrm{hull}}$ be the set of active constraints:
\begin{align*}
\mathcal A^{\textrm{hull}} = \left\{i \in [m] \bigg | A_i^{\textrm{hull}} \xk = b_i^{\textrm{hull}} \right\}
\end{align*}

If $\xk$ is not on the boundary of $P_{\textrm{hull}}$, then $\mathcal A^{\textrm{hull}} = \emptyset$ and we are free to construct the ellipsoid
\begin{align*}
\Delta = \min_{i} b_i^{\textrm{hull}} - A_i^{\textrm{hull}} \xk, \\
\left\{x \in \Rn \bigg | \left\|x - \xk\right\|^2 \le \Delta^2\right\}.
\end{align*}
% 
% \begin{boxedcomment}
% Know that $\pi > 0$ because the polyhedron has an interior...
% \end{boxedcomment}

Otherwise, we can let
\begin{align*}
u^{\textrm{hull}} = \argmax_{\|u\| = 1} \min_{i \in \mathcal A^{\textrm{hull}}} -u^TA_i^{\textrm{hull}} \\
0 < \pi^{\textrm{hull}} = \max_{\|u\| = 1} \min_{i \in \mathcal A^{\textrm{hull}}} -u^TA_i^{\textrm{hull}} \\
\beta^{\textrm{hull}} = \sqrt{1 - \left(\pi^{\textrm{hull}}\right)^2} \\
R^{\textrm{hull}} = 2\frac{(e_1 + u^{\textrm{hull}})(e_1 + u^{\textrm{hull}})^T}{(e_1 + u^{\textrm{hull}})^T(e_1 + u^{\textrm{hull}})} - \boldsymbol I
\end{align*}
so that the cone
\begin{align*}
C^{\textrm{hull}} = \left\{x \in \Rn | \; x = \xk + s, s^Tu^{\textrm{hull}} \ge \beta^{\textrm{hull}} \left\|s\right\| \right\}
\end{align*}
is contained within $P_{\textrm{hull}}$ near $\xk$.
To see this, consider constraint $j$ and apply \cref{cone_subset_cone} with
$u \gets u^{\textrm{hull}}$, 
$v \gets -A_j^{\textrm{hull}}$, 
$\beta \gets 0$, 
and $\gamma \gets \left(u^{\textrm{hull}} \right)^T \left( -A_j^{\textrm{hull}}\right) := \pi_i \ge \pi^{\textrm{hull}}$
to find
\begin{align*}
C^{\textrm{hull}} = \left\{x \in \Rn \bigg | x^T u^\textrm{hull} \ge \|x\| \sqrt{1 - \left(\pi^{\textrm{hull}}\right)^2} \right\} \\
\subseteq \left\{x \in \Rn \bigg | x^T u^\textrm{hull} \ge \|x\| \sqrt{1 - \pi_i^2} \right\} 
\subseteq \left\{s \in \Rn \bigg| A_j^{\textrm{hull}} s \le 0 \Longleftrightarrow s^T\left(-A_j^{\textrm{hull}}\right) \ge 0 \right\}
\end{align*}
% Note that 
% \begin{align*}
% R^{\textrm{hull}}e_1 = u^{\textrm{hull}} \\
% \end{align*}
With $f_e$ defined by \cref{define_ellipse_function}, we have by \cref{ellipse_in_cone} that
for any $t > 0$, the ellipsoid
\begin{align*}
\left\{x \in \Rn \bigg | f_e\left(t, t,  \beta^{\textrm{hull}}; x \right) \le 0\right\} \subseteq \left\{ts \in \Rn \bigg | e_1^T s \ge \beta^{\textrm{hull}}, \|s\| = 1, t > 0\right\} \\
\Longrightarrow \left\{x \in \Rn \bigg | f_e\left(t, t,  \beta^{\textrm{hull}};  R^{\textrm{hull}}\left(x - \xk\right) \right) \le 0\right\}
\subseteq \left\{x \in \Rn \bigg | x = \xk + s^Tu^{\textrm{hull}} \ge \beta^{\textrm{hull}}\|s\| \right\}.
\end{align*}

\begin{itemize}
\item We have that \cref{define_ellipse_function} ensures adjacency.
\item Because the sample set had a non-empty interior, $ \pi^{\textrm{hull}} > 0$, so the ellipsoid is also non-empty
\item Because $t > 0$ is arbitrary, we can ensure that it is trusted.
\end{itemize}

\begin{boxedcomment}
Choose a value of $t$.
\end{boxedcomment}

% = \left\{s \in \Rn \bigg | e_1^T s \ge \beta^{\textrm{hull}}\|s\| \right\} \\

% 
% by defining
% \begin{align}
% \gamma &= 1 + \frac 1 {\sqrt{2}} \\
% \bs &= \max\left\{\frac 1 2 , \bsk\right\}  \\
% \sampletrkpo &= \left\{x \in \Rn | f_e\left(\frac 1 {2\gamma} \dkpo, \frac 1 {2\gamma} \dkpo,\bs; \rotk(x - \xkpo)\right) \le 0\right\}.
% \end{align}
% We have defined the components $\qkpo$, $\ckpo$, $\sdkpo$ as
% \begin{align*}
% \begin{array}{ccc}
% \qkpo &=& \left(\rotk\right)^T \begin{pmatrix}
% 1 & \boldsymbol0^T \\
% \boldsymbol 0 & \frac{\bs^2}{1 - \bs^2} \boldsymbol I \\
% \end{pmatrix} \rotk, \\
% \ckpo &=& \xkpo + \frac 1 {2\gamma} \dkpo \huk, \\
% \sdkpo &=& \frac 1 {2\gamma} \dkpo. 
% \end{array}
% \end{align*}
% 

\begin{align*}
A_{\mathcal A^{\textrm{hull}}}^{\textrm{hull}}
\end{align*}

\subsection{Convex Convergence}
% 
% \begin{lemma}
% \label{recoverable}
% Suppose that \cref{constraints_are_convex} holds.
% 
% The ellipsoid output from \cref{restore_feasible_ellipsoid_convex} is conditioned, trusted, adjacent, and non-empty according to \cref{ellipsoids_notation_definitions}.
% \end{lemma}
% \begin{proof}
% \end{proof}




\begin{theorem}
\label{the_convergence_theorem}

Let $\feasible$ be defined by \cref{define_feasible}.

\input{assumptions.d/the_convergence_theorem}

If $\gammasm = 0$, then
\begin{align*}
\liminf_{k\to\infty} \chi\left(\xk\right) = \liminf_{k\to\infty} \left\|P_{\feasible}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| = 0.
\end{align*}

If $\gammasm > 0$, then
\begin{align*}
\lim_{k\to\infty} \chi\left(\xk\right) = \lim_{k\to\infty} \left\|P_{\feasible}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| = 0.
\end{align*}
\end{theorem}


\begin{proof}
Let $\truefeasiblek$ be defined by \cref{define_truefeasiblek}.
By convexity of the constraints from \cref{constraints_are_convex}, $\feasible \subseteq \truefeasiblek$, so that
\begin{align*}
\left\|P_{\feasible}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\| 
\le \left\|P_{\truefeasiblek}\left(\xk - \gradf\left(\xk\right)\right) - \xk \right\|.
\end{align*}
The result then follows from \cref{the_convergence_lemma};

\end{proof}



% \subsection{Bounded Level Sets}
% \label{bounded_level_sets_section}
% 
% We would have been able to make a number of simplifications if we assume bounded level sets.
% 
% 
% \begin{assumption}
% \label{bounded_level_sets}
% All level sets of the function $f$ are bounded. That is, for each $r \in \reals$, there exists an $M$ such that 
% \begin{align*}
% \|y\| \le M \quad \forall y \in \Omega \quad \textrm{s.t.} \quad f(y) = r.
% \end{align*}
% \end{assumption}

\section{Numerical Results}


\begin{align*}
\left(\wik - \xk\right)^T \left(x - \frac{\left(x^T\left(\wik - \xk \right)\right)\left(\wik - \xk\right)}{\left\|\wik - \xk\right\|^2 }\right)= 0 \\
\left(I - \right) \\
\left\{x \in \Rn \bigg | \left\|x - \wik \right\|^2 \le \left\|\right\| \right\}
\end{align*}




\section{Weakening an Assumption}
\label{alternative_assumptions_section}

We have made \cref{minangleassumption_alt}, which is an assumption about the constraint's models.
We would have preferred to make an assumption about the true constraints: \cref{minangleassumption_alt_func}.
However, the boundededness of the ellipsoids according to \cref{ellipsoids_notation_definitions} shown in \cref{bounded_condition_numbers}
is required to show prove \cref{accuracy_is_satisfied}.
Yet, \cref{accuracy_is_satisfied} is key to applying results about true constraints to results about their models.
In this section, we discuss some progress towards replacing \cref{minangleassumption_alt} with \cref{minangleassumption_alt_func}.
In the process, we are also able to remove \cref{mingradassumption}.






\begin{definition}
Let $\epsilon > 0$ be a given constant.
A constraint $c_i(x) \le 0$ is said to be $\epsilon$-nearly active at $x \in \feasible$ if $|c_i(x)| \le \epsilon$.
\end{definition}

\begin{definition}
Let $\epsilon > 0$ be given.
Define the set of indices of $\epsilon$ active constraints at a point $x \in \feasible$ by
\begin{align}
\epsactive(x; \epsilon) = \bigg\{ i \in [m] \bigg | |c_i(x)| \le \epsilon \bigg\} \label{define_epsactive}
\end{align}
\end{definition}

\begin{definition}
Let $\epsilon > 0$ be given.
Define the set of indices of $\epsilon$ active model constraints at a point $x \in \feasible$ by
\begin{align}
\epsactivemodels(x; \epsilon) = \bigg\{ i \in [m] \bigg | |m_{c_i}(x)| \le \epsilon  \bigg\} \label{define_epsactivemodels}
\end{align}
\end{definition}

\begin{assumption}
\label{minangleassumption_alt_func}
There exists $\minanglealpha$ and $\epsilon > 0$ such that for every $x \in \feasible$, 
there is a unit vector $\minanglediralt(x)$ such that
\begin{align*}
\nabla c_i(x)^T\minanglediralt(x) > \minanglealpha\quad \forall i \in \epsactive(x; \epsilon).
\end{align*}
\end{assumption}


\begin{lemma}
\label[lemma]{mingradlemma}
Suppose that 
\cref{minangleassumption_alt_func} holds.
There exist $\mingradepsilon > 0$ and $\mingrad > 0$ such that for each $x \in \Omega$ we have
\begin{align*}
\| \nabla c_i(x) \| \ge \mingrad \quad \forall i \in \epsactive(x; \mingradepsilon).
\end{align*}
\end{lemma}
\begin{proof}
Let $x \in \Omega$ be arbitrary.
By \cref{minangleassumption_alt_func} we know there are some constants $\minanglealpha>0$ and $\epsilon>0$ and unit vector $\minanglediralt(x)$
such that for all $i \in \epsactive(x; \epsilon)$,
$\nabla c_i(x)^T\minanglediralt(x) \ge \minanglealpha$.
Letting $\mingradepsilon = \epsilon$ and $\mingrad = \minanglealpha$, we see that for all $i \in \epsactive(x; \mingraddelta)$,
\begin{align*}
\|\nabla c_i(x)\| = \|\nabla c_i(x)\| \|\minanglediralt(x)\| \ge \nabla c_i(x)^T\minanglediralt(x) \ge \minanglealpha = \mingrad.
\end{align*}
\end{proof}

The following result is close to showing a bound the condition numbers of $\qk$.
However, it relies on $\dk \to 0$.
\begin{lemma}
Suppose \cref{minangleassumption_alt_func} and \cref{bounded_gradients_lemma} are satisfied as well as the assumptions for \cref{accuracy_is_satisfied_lemma}.

There exists $\dacc > 0$ and $\sigma_0 \ge 1$ such that if $\dk \le \dacc$, and
\begin{align}
\condition \left( Q^{(k-1)} \right) \le \sigma_0 \label{idk_alt_proof_statement1}
\end{align}
then 
\begin{align}
\condition \left( \qk \right) \le \sigma_0. \label{idk_alt_proof_statement2}
\end{align}
\end{lemma}


\begin{proof}
Let $\epsilon, \minanglealpha > 0$ be defined by \cref{minangleassumption_alt_func}, and $\maxgrad$ be defined by \cref{bounded_gradients_lemma}.
Define
\begin{align}
&M = \frac 1 {\maxgrad + \frac 1 2 \minanglealpha} & \\
&\textrm{and} \quad \sigma_0 = \max\left\{1, \frac{48}{M^2\left(\minanglealpha\right)^2}\right\}.&
\end{align}
By \cref{boundbeta}, there exists $\dacco \left(\frac {M \minanglealpha} 2\right) > 0$ such that if
$\thetamink \ge \frac {M \minanglealpha} 2$ and $\dk \le \dacco \left(\frac {M \minanglealpha} 2\right)$,
then $\sigma\left(\qk\right) \le \frac{12}{\left(\frac{M\minanglealpha}{2}\right)^2} = \sigma_0$.
Define 
\begin{align}
\dacc = \min\left\{
\dacco \left(\frac {M \minanglealpha} 2\right), 
\sqrt{\frac {\minanglealpha}{2\kappa_g\sqrt{\sigma_0}}},
\frac{M \epsilon}{\sqrt{n}}
\right\} \label{define_delta_accuracy2}
\end{align}
and suppose that \cref{idk_alt_proof_statement1} and $\dk \le \dacc$ hold.
By \cref{boundbeta}, \cref{idk_alt_proof_statement2} follows from $\thetamink \ge \frac {M \minanglealpha} 2$.

% then
% \begin{align}
% \sqrt{\sigma\left(\qk \right)} \le \frac{48}{M^2\left(\minanglealpha\right)^2}
% \end{align}
% Suppose that $\epsactive(\xk; \epsilon) = \emptyset$.
% for all $y \in \tr$, we have
% \begin{align*}
% \left\|\gradf\left(y\right) - \nabla \mfk\left(y\right) \right\| \le \kappa_g \sqrt{\sigma\left( Q^{(k-1)} \right)}\dk^2.
% \end{align*}
% In particular, there exists $u \in \Rn$ with $\|u\| \le 1$ such that

By \cref{accuracy_is_satisfied_lemma}, there exists $\kappa_{g} > 0$ and $u\in\Rn$ with $\|u\|\le 1$ such that
\begin{align}
\nabla c_i\left(\xk\right) = \gmcik + \kappa_g \sqrt{\sigma\left( Q^{(k-1)} \right)}\dk^2 u. \label{idk_alt_proof_eqn2}
\end{align}
Taking the square root of \cref{idk_alt_proof_statement1} and multiplying by $\dk^2 \le \frac {\minanglealpha}{2\kappa_g\sqrt{\sigma_0}}$, we see
\begin{align}
\sqrt{\sigma\left( Q^{(k-1)} \right)} \dk^2 \le \frac {\minanglealpha} {2\kappa_g}. \label{idk_alt_proof_ass1}
\end{align}
We see that by the triangle inequality, \cref{bounded_gradients_lemma}, \cref{idk_alt_proof_eqn2}, and \cref{idk_alt_proof_ass1} that
\begin{align}
\left\|\gmcik\right\| \le \left\| \nabla c_i\left(x\right)\right\| + \kappa_g \sqrt{\sigma\left( Q^{(k-1)} \right)} \dk^2 \le \maxgrad + \frac 1 2 \minanglealpha = \frac 1 M \nonumber \\
\Longrightarrow \frac 1 {\left\|\gmcik\right\|} \ge M. \label{idk_alt_proof_eqn4}
\end{align}

Let $\activeconstraintsk$ and $\epsactive(x; \epsilon)$
% and $\epsactivemodels(x; \epsilon)$
be defined by
\cref{define_activeconstraints} and \cref{define_epsactive}
% , and \cref{define_epsactivemodels}
respectively.
If $\activeconstraintsk = \emptyset$, then $\sigma\left(\qk\right) = 1$.
Otherwise, let $i \in \activeconstraintsk$ be arbitrary,
so that $\zik \in \tr$. By \cref{define_z}
\begin{align*}
M \left|m_{c_i}\left(\xk\right)\right| \le \frac{\left|m_{c_i}\left(\xk\right)\right|}{\left\|\gmcik\right\|} \le \sqrt{n} \dk \Longrightarrow
\left|c_i\left(\xk\right)\right| \le \frac{\sqrt{n}\dk}{M} \le \epsilon
\end{align*}
by \cref{define_delta_accuracy2}.
This means that $i \in \epsactive(\xk; \epsilon)$, so that by \cref{minangleassumption_alt} there exists a unit vector $\minanglediralt(\xk)$ with
\begin{align}
\nabla c_i\left(\xk\right)^T\minanglediralt\left(\xk\right) > \minanglealpha \label{idk_alt_proof_eqn1}
\end{align}
Substituting \cref{idk_alt_proof_eqn2} into \cref{idk_alt_proof_eqn1} yields:
\begin{align*}
\left(\gmcik + \kappa_g \sqrt{\sigma\left( Q^{(k-1)} \right)}\dk^2 u\right)^T \minanglediralt(x) > \minanglealpha
\end{align*}
Using \cref{idk_alt_proof_ass1}, $\|u\|\le 1$, and $\|\minanglediralt(x)\| = 1$, this becomes
\begin{align}
\gmcik^T\minanglediralt(x) > \minanglealpha - \kappa_g \sqrt{\sigma\left( Q^{(k-1)} \right)}\dk^2 u^T\minanglediralt(x) \ge \frac 1 2 \minanglealpha \label{idk_alt_proof_eqn3}
\end{align}
Multiplying \cref{idk_alt_proof_eqn3} and \cref{idk_alt_proof_eqn4} yields
\begin{align}
\frac{\gmcik}{\left\|\gmcik\right\|}^T  \minanglediralt(x) \ge \frac M 2 \minanglealpha. \label{idk_alt_proof_eqn5}
\end{align}
Let $\huk$ be defined by \cref{define_u}, 
% \begin{align*}
% \huk \in -\argmin_{\|u\| = 1} \max_{i \in \activeconstraintsk} u^T \frac{\gmcik}{\left\|\gmcik\right\|}
% \end{align*}
and $\thetamink$ be defined by \cref{define_thetamink}.
Then \cref{idk_alt_proof_eqn5} tells us 
\begin{align*}
\thetamink = \min_{i \in \activeconstraintsk} \left(-\hgik\right)^T \huk \ge \frac{\gmcik}{\left\|\gmcik\right\|}^T \minanglediralt(x) \ge \frac M 2 \minanglealpha.
\end{align*}
% Then by \cref{boundbeta}, there exists $\dacc(\epsilon) > 0$ such that if $\thetamink >$
% \begin{align*}
% \sigma\left(\qk\right) \le \frac{48}{M^2\left(\minanglealpha\right)^2}
% \end{align*}
% Now,
% \begin{align*}
% \sigma_2 = \frac{\max\left\{1, \frac{\beta^2}{1 - \beta^2}\right\}}{\min\left\{1, \frac{\beta^2}{1 - \beta^2}\right\}}
% \end{align*}

% We know that the next trial point $\xkpo$ must also lie within the trust region, so that we have both
% \begin{align*}
% \left\|\gradf\left(\xk\right) - \nabla \mfk\left(\xk\right) \right\| \le \kappa_g \sqrt{\sigma_1}\dk^2 \\
% \left\|\gradf\left(\xkpo\right) - \nabla \mfk\left(\xkpo\right) \right\| \le \kappa_g \sqrt{\sigma_1}\dk^2 \\
% \end{align*}
% 
% \begin{align*}
% \nabla \mfk\left(\xk\right)  = \gradf\left(\xk\right) + \kappa_g \sqrt{\sigma_1}\dk^2 \\
% \end{align*}
% 
% 
% 
% Let $\bsk$ be defined by \cref{define_bsk}.
% Let $\alpha$ and $\beta$ be defined by \cref{define_alpha_beta}.
% Let $p_{\alpha}$ and $p_{\beta}$ be defined by \cref{define_p_alpha} and \cref{define_p_beta} respectively.
% \begin{align}
% \end{align}
% 
% \begin{align*}
% \huk \in -\argmin_{\|u\| = 1} \max_{i \in \activeconstraintsk} u^T \frac{\gmcik}{\left\|\gmcik\right\|} \\
% \bsk = \beta\dk^{p_{\beta}} + \sqrt{\left(1 - \left(\beta\dk^{p_{\beta}}\right)^2\right)\left(1 - \left(\thetamink\right) ^2\right)} \\
% \end{align*}
\end{proof}

\section{Deleted Stuff}
\subsection{Linear Constraints}
\label{handling_linear_constraints_within_ellipsoid_programs}
\sbnote{I am confused by the following discussion.   You seem to be implying that you are constructing the ellipsoid to lie within a polyhedral region (in the case, the $L_\infty$ ball.  But this doesn't account for requirement that the ellipsoid also lies within the cap cone)}
Throughout discussing strategies for constructing an ellipsoid within the buffering cones, we must also require that the ellipsoid is contained within the trust region.
Because our trust region is $\tr$, we can satisfy the trust region constraints by including linear constraints of the form $\xk_i - \dk \le x_i \le \xk_i + \dk$.
As done in \cref{chap:linear},  using the work of \cite{Khachiyan1993}, we handle the general constraints $Ax \le b$ for some $m\times n$ matrix $A$ and $b \in \Rm$.
That is, given a polyhedron $P = \{ x \in \Rn\; | \;  Ax \le b \}$ defined by an $m \times n$ matrix $A$, and $b \in \Rm$,
we wish to find the maximum-volume ellipsoid $E \subset P$ centered at a point $\mu \in P$.

Let $\bar{b} = b - A\mu$ and $d = x - \mu$ so that the polyhedron becomes
\begin{align*}
P = \left\{ \mu + d \in \Rn \; \bigg | \;  Ad \le \bar{b} \right\}
\end{align*}
Then, the ellipsoid can then be centered at zero, and defined by a symmetric positive definite matrix $Q \succ 0$:
\begin{align*}
E =\left \{ d \in \Rn \; \bigg | \; \frac 1 2 d^T Q d \le 1 \right\}.
\end{align*}
Our goal is to determine $Q$ to maximize the volume of $E$ such that $\mu + E \subset P$.
Define the auxiliary function $f(d) = \frac 1 2 d^T Q d$ so that $E = \{ d \in \Rn\; | \; f(d) \le 1 \}$.

% Because $Q$ is positive definite, $f$ has a unique minimum on each hyper-plane $A_i d = b_i$.
% Let this minimum be $d^{(i)} = \argmin_{A_id =\bar{b}_i} f(d)$ for $i \in [m]$.
% By the first order optimality conditions, there exists a $\lambda \in \Rm$ such that
% \begin{align*}
% \gradf(d^{(i)}) = Q d^{(i)} = \lambda_i A_i 
% \Longrightarrow d^{(i)} = \lambda_i Q^{-1}A_i \quad \forall i \in [m]
% \end{align*}
% We also know that
% \begin{align*}
% A_i^T d^{(i)} = \bar{b_i} \Longrightarrow
% A_i^T \lambda_i Q^{-1}A_i = \bar{b_i} \Longrightarrow
% \lambda_i = \frac {\bar{b_i}}{A_i^T  Q^{-1}A_i}
% \end{align*}
% so that
% \begin{align*}
% d^{(i)} = \lambda_i Q^{-1}A_i = \frac {\bar{b_i}}{A_i^T  Q^{-1}A_i}  Q^{-1}A_i \quad \forall i \in [m].
% \end{align*}
% 
% Because $E \subset P$, we also know that $f(d^{(i)}) \ge 1$ for each $i$. Thus,
% \begin{align*}
% \frac 1 2 (d^{(i)})^{T} Q d^{(i)} \ge 1 \\
% \Longrightarrow \frac 1 2 \bigg(\frac {\bar{b}_i}{A_i^T  Q^{-1}A_i}  Q^{-1}A_i\bigg)^{T} Q \frac {\bar{b}_i}{A_i^T  Q^{-1}A_i}  Q^{-1}A_i \ge 1 \\
% \Longrightarrow \frac 1 2 \frac {1}{A_i^T  Q^{-1}A_i}  \bar{b_i} A_i^T Q^{-1} Q \frac {\bar{b_i}}{A_i^T  Q^{-1}A_i}  Q^{-1}A_i \ge 1 \\
% \Longrightarrow \frac 1 2 \frac {1}{A_i^T  Q^{-1}A_i}  \frac {\bar{b_i}^2}{A_i^T  Q^{-1}A_i}  A_i^T Q^{-1}A_i \ge 1 \\
% \Longrightarrow \frac 1 2  \frac {\bar{b_i}^2}{A_i^T  Q^{-1}A_i} \ge 1 \\
% \Longrightarrow \frac 1 2 \bar{b_i}^2\ge A_i^T  Q^{-1}A_i \\
% \Longrightarrow A_i^T  Q^{-1}A_i \le \frac 1 2 \bar{b_i}^2.
% \end{align*}

As shown in \cref{ellipse_optimization} these constraints can be simplified to requiring
\begin{align*}
A_i^T  Q^{-1}A_i \le \frac 1 2 \bar{b_i}^2 \forall i \in [m].
\end{align*}

For the trust region, this is can be written as
\begin{align*}
\tr = \left\{ x \in \Rn | \atr x\le \btr\left(\xk, \dk\right) \right\}
\end{align*}
where
\begin{align}
\atr = \begin{pmatrix}
 1 &  0 & 0      & \ldots &  0 \\
-1 &  0 & 0      & \ldots &  0 \\
 0 &  1 & 0      & \ldots &  0 \\
 0 & -1 & 0      & \ldots &  0 \\
   &    & \vdots &        &    \\
 0 &  0 &      0 & \ldots &  1 \\
 0 &  0 &      0 & \ldots & -1 \\
\end{pmatrix} \quad \textrm{and} \quad
\btr\left(\xk, \dk\right) = \atr \xk + \dk. \label{define_atr}
\end{align}
In row form, this is
\begin{align*}
e_i ^T x \le e_i^T\xk + \dk, \quad \textrm{and} \quad
-e_i ^T x \le -e_i^T\xk + \dk \quad \forall i \in [n],
\end{align*}
to ensure $\unshiftedellipsoid \subseteq \tr$, we must only constrain the diagonal elements of the inverse of $\qk$:
\begin{align}
e_i^T\left(\frac{\qk}{\frac 1 2 \sdk^2}\right)^{-1} e_i \le \left[e_i^T\left(\xk - \ck\right) \pm \dk \right]^2 \quad \forall i \in [n].
\label{ellipsoids_trust_region_constraints}
\end{align}

