
% \section{Derivative Free Optimization}

Derivative free optimization (DFO) refers to mathematical programs involving functions for which derivative information is not explicitly available.
Such problems arise, for example, when the functions are evaluated by simulations or by laboratory experiments.
Applications of DFO appear in many fields, including photo-injector optimization \cite{1742-6596-874-1-012062},
circuitry arrangements \cite{PLOSKAS201816}, machine learning \cite{KS2018}, volume optimization \cite{Cheng2017}, and reliability-based optimization \cite{Gao2017}.
In such applications, function evaluations are typically expensive, so it is sensible to invest significant computational resources to minimize the number of function evaluations.

This work is ultimately aimed at developing algorithms to solve constrained optimization problems of the form 
\begin{align}
\begin{array}{ccl} \min_{x \in \Rn} & f(x) \\
\mbox{subject to} \quad & c_i(x) \le 0, &\quad \forall 1 \le i \le m,
\end{array}
\label{the_dfo_problem}
\end{align}
where 
% $\domain$ is a subset of $\Rn$, and
$f$ and $c_i, \forall 1 \le i \le m$ are real-valued functions on $\Rn$ with at least one of these functions being a {\em black-box} function, meaning that derivatives cannot be evaluated directly.

We will let the feasible set be represented as 
\begin{align}
\feasible = \{x \in \Rn | c_i(x) \le 0, \; \forall 1 \le i \le m \}.
\end{align}

We are interested in developing {\em model-based} {\em trust-region} algorithms for solving these problems.
Model-based methods work by constructing model functions to approximate the black box functions at each iteration.
The model functions are determined by fitting previously evaluated function values on a set of sample points.
In trust-region methods, the model functions are used to define a trust-region subproblem whose solution determines the next iterate.
For example, the trust-region subproblem might have the form
\begin{align*}
\begin{array}{ccl} \min_{\|s\| \le \dk}
 & \mfk \left(\xk+s\right) \\
\mbox{subject to} \quad & \mcik\left(\xk + s\right) \le 0 & \quad 1 \le i \le m, \\
& \|s\| \le \dk \\
\end{array}
\end{align*}
where $\xk$ is the current iterate, $\mfk$ is the model function approximating $f$, 
and $\mcik$ are the model functions approximating the constraint functions $c_i, \forall 1 \le i \le m$, and $\dk$ is the radius of the trust-region.
The key differences between the trust-region subproblem problem and the original are that all functions are replaced with their model functions, and a trust region constraint $\|s\| \le \dk$ has been added.

%Note that we are using models of the constraint functions to approximate the feasible region during each iteration.  In particular, we define the {\em model feasible region} by
%\begin{align}
%\feasiblek = \left\{x \in \Rn \big| \mcik(x) \le 0 \quad \forall 1 \le i \le m \right\}.  \label{define_feasiblek}
%\end{align}



Conceptually, the model functions are ``trusted'' only within a distance $ \dk $ of the current iterate $\xk$; so the trust-region subproblem restricts the length of step $s$ to be no larger than $\dk$.
To ensure that the model functions are good approximations of the true functions over the trust region, the sample points are typically chosen to lie within the trust-region.

We are specifically interested in applications where some of the black box functions cannot be evaluated outside the feasible region.
As in \cite{digabel2015taxonomy}, {\em quantifiable} means the functions can be evaluated at any point in $\feasible$ and that the values returned for the constraint functions provide meaningful information about how close the point is to a constraint boundary.
For our work, we assume that the black-box functions return meaningful numerical values \emph{only} when evaluated at feasible points.
In this case,  the constraints are called {\em partially quantifiable}.    
\sbnote{Can we find an example of a partially quantifiable constraint?}   
For problems involving partially quantifiable constraints,  numerical function values at infeasible points (if obtained) are likely to be unreliable.
We, therefore, require all sample points to be feasible, and we seek to avoid infeasible function evaluations.
Notice that this feasibility requirement means that if the algorithm is stopped at any time, it will still produce a feasible output.


An important consideration in fitting the model functions is the ``geometry'' of the sample set.
This will be discussed in more detail in \cref{geometry}, but the key point is that the relative positions of the sample points within the trust region have a significant effect on the accuracy of the model functions over the trust region.
When the geometry of the sample set is poor, it is sometimes necessary to evaluate the functions at new points within the trust region to improve the geometry of the sample set.
It is well understood how to do this for unconstrained problems, but for constrained problems,
our requirement that the sample points must be feasible poses some interesting challenges to maintaining good geometry.   

As a first step toward developing algorithms to solve such problems,  \cref{chap:linear}  considers a simplified problem where all of the constraints are linear; namely:
\begin{align*}
\begin{array}{ccl} \min_{x \in \Rn} & f(x) \\
& \lca x \le \lcb, & 
\end{array}
\end{align*}
where $\lca$ is an $m \times n$ matrix and $\lcb \in \Rm$.
We then handle general black-box constraints within \cref{chap:general}, with special attention to the  the case where the feasible set $\feasible$ is convex.

% \sbnote{Maybe move the following paragraph to \cref{chap:linear}.}
% 
% \begin{comment}
% 
% The central idea of the method described in \cref{chap:linear}
% is to construct a feasible ellipsoid at each iteration that lies within the intersection of the feasible region and the current trust region.
% We call this ellipsoid the {\em sample trust region}.
% To choose well-poised sample points for this ellipsoidal region,  we adapt a model improvement algorithm presented in \cite{introduction_book} 
% for spherical trust regions and establish error bounds on the accuracy of the model functions over this region.   
% 
% We present several variants of how to construct the feasible ellipsoid.
% We first show how to find the maximum volume ellipsoid contained within a polytope given a fixed center.
% We then explore several strategies for shifting the center of the ellipsoid.    
% Our convergence analysis is based on an algorithmic framework presented \cite{Conejo:2013:GCT:2620806.2621814},
% which describes a class of trust-region algorithms for convex constrained minimization without derivatives. 
% \end{comment}

\cref{chap:linear} considers the case of a black-box objective with linear 
%partially-quantifiable
constraints.  \sbnote{It doesn't make sense to call linear constraints partially-quantifiable.   The issue is that the objective function can't be evaluated outside the feasible region.}
Because the constraints are known, we present an algorithm that constructs sample points from the feasible region intersected with the current trust region.
This region is called the {\em sample region}.
The algorithm we present is an implementation of the algorithmic template described in \cite{Conejo:2013:GCT:2620806.2621814}.
As such, we can prove its convergence by showing that it satisfies the required assumptions presented in \cite{Conejo:2013:GCT:2620806.2621814}.
We then discuss several variants to improve on the base algorithm, as well as provide numerical results.

\cref{chap:general} considers the case of nonlinear black-box constraints.
In contrast to the case of linear constraints, we can no longer be assured that feasible points for the model sub-problems are feasible for the original problem.
We, therefore, propose a methodology that constructs second-order cones for each nearly-active constraint and then constructs an ellipsoidal sample trust region within the intersection of these cones.
We show that under reasonable assumptions, this sample trust region is guaranteed to be feasible when the trust region is sufficiently small.
Using this result,  we prove that the criticality measure for the iterates generated by the algorithm converges to zero.
Thus,  any accumulation point of the iterates satisfies the first-order optimality conditions.
Finally, numerical results are presented comparing the performance of our algorithm to several other algorithms for constrained derivative-fee optimization.

\section{Notation}  Throughout the thesis, we use the following notational conventions.
Any variables that depend on the iteration will be super-scripted by $k$.
For example, the $k$-th iterate is given by $\xk$, and iterate $k$'s model of the objective is given by $\mfk$.
Subscripts on vectors are used as an index into the vector, while vectors in a sequence of vectors use superscripts:
that is, $\xki$ is the $i$-th component of the $k$-th vector in the sequence $\{\xk\}_k$.
We use $\proj_X(x) = \argmin_{x' \in X}\left\|x' - x\right\|$ to denote the projection of $x$ onto a convex set $X$.

\paragraph*{Matrices}
Matrices are denoted with capital letters.
The $i$-th row of the matrix $A$ is denoted $A_i$.
For any index set $S \subseteq \{i \in \naturals | 1 \le i \le m\}$, the $|S| \times n$ matrix formed by only using
rows of the $m\times n$ matrix $A$ from the set $S$ is $A_S$.
Let the condition number of a matrix $Q$ be denoted $\condition(Q)$.
% , while the $i$-th column is denoted $A_{\bullet i}$.
We use $e_i$ to denote the $i$-th unit vector and $e$ to denote the vector of all ones.

$B_k\left(c; \Delta\right)$ is the ball of radius $\Delta$ in the $k$ norm, centered at point $c$.  That is,
\begin{align}
\label{define_ball}
B_k\left(c; \Delta\right) = \left\{x \in \Rn \bigg | \left\|x - c\right\|_k \le \Delta \right\}.
\end{align}
Note that some of the common norms are:
\begin{align*}
\|x\|_{\infty} = \max_{1\le i\le n}|x_i|, \quad
\|x\|_{2} = \sqrt{\sum_{i=1}^n x_i^2}, \quad \textrm{and} \quad
\|x\|_1 = \sum_{i = 1}^n |x_i|.
\end{align*}

\paragraph*{Sets}
The complement of a set $S$ is denoted as $\bar S$.
Define a point $x$ subtracted from a set $S$ as $S - x = \left\{s \in \Rn | s - x \in S\right\}$.
Set addition is $X + Y = \left\{x + y | x \in X, y \in Y\right\}$.

$\delta_{i,j}$ is the Kronecker delta, $\delta_{i, j} = \begin{cases} 1 & \textrm{if} \; i = j \\ 0 & \textrm{if} \; i \ne j \end{cases}$.
We define the positive part of a real number to be
\begin{align*}
a^+ = \begin{cases} a & \textrm{if} \quad a \ge 0 \\ 0 & \textrm{otherwise}. \end{cases}\\
\end{align*}
For any $m \in \naturals$, we define $[m] = \left\{i \in \naturals | 1 \le i \le m\right\}$.

% For any matrix $A$, we let $A^{\dagger}$ be the Moore-Penrose inverse.
