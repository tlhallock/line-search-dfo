@book{dennisschnabel1983,
  author={Dennis, J. E. and Schnabel,Robert B.},
  year={1983},
  title={Numerical methods for unconstrained optimization and nonlinear equations},
  publisher={Prentice-Hall},
  address={Englewood Cliffs, N.J},
  keywords={Equations; Mathematical optimization; Numerical solutions},
  isbn={0136272169;9780136272168;},
  language={English},
}

% 1
@article{AMAIOUA201813,
  abstract     = {The mesh adaptive direct search algorithm (MADS) is an iterative method for constrained blackbox optimization problems. One of the optional MADS features is a versatile search step in which quadratic models are built leading to a series of quadratically constrained quadratic subproblems. This work explores different algorithms that exploit the structure of the quadratic models: the first one applies an l1-exact penalty function, the second uses an augmented Lagrangian and the third one combines the former two, resulting in a new algorithm. It is notable that this latter approach is uniquely suitable for quadratically constrained quadratic problems. These methods are implemented within the NOMAD software package and their impact are assessed through computational experiments on 65 analytical test problems and 4 simulation-based engineering applications.},
  author       = {Amaioua, Nadir and Audet, Charles and Conn, Andrew R. and Le~Digabel, Sébastien},
  url          = {http://www.sciencedirect.com/science/article/pii/S0377221717309876},
  date         = {2018},
  doi          = {https://doi.org/10.1016/j.ejor.2017.10.058},
  issn         = {0377-2217},
  journal      = {European Journal of Operational Research},
  keywords     = {Nonlinear programming,Derivative-free optimization,Quadratic programming,Trust-region subproblem,Mesh adaptive direct search},
  year         = {2018},
  number       = {1},
  pages        = {13--24},
  title        = {Efficient solution of quadratically constrained quadratic subproblems within the mesh adaptive direct search algorithm},
  volume       = {268},
}

% 2
@article{Audet2002AnalysisOG,
  title={Analysis of Generalized Pattern Searches},
  author={Charles Audet and J. E. Dennis},
  journal={SIAM Journal on Optimimization},
  year={2002},
  volume={13},
  issue={3},
  pages={889-903}
}

% 3
@TechReport{NOWPAC2014,
   author      = {Augustin, F. and Marzouk, Y.~M.},
   title       = {{NOWPAC:} A provably convergent derivative-free
nonlinear optimizer with path-augmented constraints},
   institution = {ArXiv},
   number      = {1403.1931},
   year        = {2014},
   gurl        =
{https://scholar.google.com/scholar?cluster=16890935403623042623},
   url         = {https://arxiv.org/abs/1403.1931},
}

% 4
@article{BAJAJ2018306,
  abstract     = {This paper presents an algorithm for constrained black-box and grey-box optimization. It is based on surrogate models developed using input-output data in a trust-region framework. Unlike many current methods, the proposed approach does not require feasible initial point and can handle hard constraints via a novel optimization-based constrained sampling scheme. A two-phase strategy is employed, where the first phase involves finding feasible point through minimizing a smooth constraint violation function (feasibility phase). The second phase improves the objective in the feasible region using the solution of the feasibility phase as starting point (optimization phase). The method is applied to solve 92 test problems and the performance is compared with established derivative-free solvers. The two-phase algorithm outperforms these solvers in terms of number of problems solved and number of samples used. We also apply the algorithm to solve a chemical process design problem involving highly-coupled, nonlinear algebraic and partial differential equations.},
  author       = {Bajaj, Ishan and Iyer, Shachit S. and {Faruque Hasan}, M.M.},
  url          = {https://www.sciencedirect.com/science/article/pii/S0098135417304404},
  year         = {2018},
  doi          = {https://doi.org/10.1016/j.compchemeng.2017.12.011},
  issn         = {0098-1354},
  journal      = {Computers and Chemical Engineering},
  keywords     = {Constrained derivative-free optimization,Black-box optimization,Grey-box optimization,Simulation-based optimization,Surrogate model,Data-driven optimization},
  note         = {Multi-scale Systems Engineering – in memory and honor of Professor C.A. Floudas},
  pages        = {306--321},
  title        = {A trust region-based two phase algorithm for constrained black-box and grey-box optimization with infeasible initial point},
  volume       = {116},
}

@article{billups.larson.ea:derivative-free,
Author = {Billups, Stephen C. and Larson, Jeffrey and Graf, Peter},
Title = {Derivative-free optimization of expensive functions with computational error using weighted regressions},
Journal = {SIAM Journal on Optimization},
Year = {2013},
Volume = {23},
Number = {1},
Pages = {27-53},
DOI = {10.1137/100814688},
ISSN = {1052-6234},
EISSN = {1095-7189},
ResearcherID-Numbers = {Larson, Jeffrey/R-4839-2019
   Billups, Stephen/K-5074-2015},
ORCID-Numbers = {Larson, Jeffrey/0000-0001-9924-2082
   Billups, Stephen/0000-0003-3627-0793},
Unique-ID = {ISI:000316857500002},
}

% 5
@article{BMNORW2020,
   author        = {Raghu Bollapragada and Matt Menickelly and Witold
Nazarewicz and Jared O'Neal and Paul-Gerhard Reinhard and Stefan M. Wild},
   title         = {Optimization and Supervised Machine Learning Methods
for Fitting Numerical Physics Models without Derivatives},
   journal     = {Journal of Physics G: Nuclear and Particle Physics},
   volume        = {48},
   pages = {024001},
   number = {2},
   year        = {2021},
   doi         = {10.1088/1361-6471/abd009},
   aurl         = {https://arxiv.org/abs/2010.05668},
}

% 6
@TechReport{Brilli2021interior,
   title={An interior point method for nonlinear constrained derivative-free optimization},
   author={Andrea Brilli and Giampaolo Liuzzi and Stefano Lucidi},
   year={2021},
   number={2108.05157},
   institution = {ArXiv},
   url  = {https://arxiv.org/abs/2108.05157},
}

%7
@article{Cheng2017,
  abstract     = {We describe a parallel algorithmic framework for optimizing the shape of elements in a simplicial volume mesh. Using fine-grained parallelism and asymmetric multiprocessing on multi-core CPU and modern graphics processing unit hardware simultaneously, we achieve speedups of more than tenfold over current state-of-the-art serial methods. In addition, improved mesh quality is obtained by optimizing both the surface and the interior vertex positions in a single pass, using feature preservation to maintain fidelity to the original mesh geometry. The framework is flexible in terms of the core numerical optimization method employed, and we provide performance results for both gradient-based and derivative-free optimization methods.},
  author       = {Cheng, Zuofu and Shaffer, Eric and Yeh, Raine and Zagaris, George and Olson, Luke},
  url          = {https://doi.org/10.1007/s00366-014-0393-7},
  year         = {2017},
  date         = {2017-10-01},
  doi          = {10.1007/s00366-014-0393-7},
  issn         = {1435-5663},
  journal      = {Engineering with Computers},
  number       = {4},
  pages        = {717--726},
  title        = {Efficient parallel optimization of volume meshes on heterogeneous computing systems},
  volume       = {33},
}

%8
@article{Conejo:2013:GCT:2620806.2621814,
  author       = {Conejo, P. D. and Karas, E. W. and Pedroso, L. G. and Ribeiro, A. A. and Sachine, M.},
  location     = {New York, NY, USA},
  publisher    = {Elsevier Science Inc.},
  url          = {http://dx.doi.org/10.1016/j.amc.2013.06.041},
  year         = {2013},
  date         = {2013-09},
  doi          = {10.1016/j.amc.2013.06.041},
  issn         = {0096-3003},
  journal      = {Appl. Math. Comput.},
  keywords     = {Convex constrained optimization,Derivative-free optimization,Trust region},
  pages        = {324--330},
  title        = {Global Convergence of Trust-region Algorithms for Convex Constrained Minimization Without Derivatives},
  volume       = {220},
}

% doi:10.1080/10556788.2015.1026968
% 9
@article{Conejo2015,
  author       = {Conejo, P.D. and Karas, E.W. and Pedroso, L.G.},
  publisher    = {Taylor \& Francis},
  url          = {https://doi.org/10.1080/10556788.2015.1026968},
  year         = {2015},
  doi          = {10.1080/10556788.2015.1026968},
  eprint       = {https://doi.org/10.1080/10556788.2015.1026968},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1126--1145},
  title        = {A trust-region derivative-free algorithm for constrained optimization},
  volume       = {30},
}

% 10
@book{Conn:2000:TM:357813,
  author    = {Conn, Andrew R. and Gould, Nicholas I. M. and Toint, Philippe L.},
  location  = {Philadelphia, PA, USA},
  publisher = {Society for Industrial and Applied Mathematics},
  year      = {2000},
  isbn      = {0-89871-460-5},
  title     = {Trust-region Methods},
}

% 11
@book{introduction_book,
  author    = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
  location  = {Philadelphia, PA, USA},
  publisher = {Society for Industrial and Applied Mathematics},
  year      = {2009},
  isbn      = {0898716683, 9780898716689},
  title     = {Introduction to Derivative-Free Optimization},
}


@inbook{doi:10.1137/1.9781611974683.ch37,
  author    = {Custódio, Ana Luísa and Scheinberg, Katya and Vicente, Luis Nunes},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974683.ch37},
  booktitle = {Advances and Trends in Optimization with Engineering Applications},
  doi       = {10.1137/1.9781611974683.ch37},
  year      = {2017},
  eprint    = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974683.ch37},
  pages     = {495--506},
  title     = {Chapter 37: Methodologies and Software for Derivative-Free Optimization},
}















@article{More:2009:BDO:1654367.1654371,
  author       = {Mor{é}, Jorge J. and Wild, Stefan M.},
  location     = {Philadelphia, PA, USA},
  publisher    = {Society for Industrial and Applied Mathematics},
  url          = {http://dx.doi.org/10.1137/080724083},
  date         = {2009-03},
  year         = {2009},
  doi          = {10.1137/080724083},
  issn         = {1052-6234},
  journal      = {SIAM Journal on Optimization},
  keywords     = {benchmarking,computational budget,derivative-free optimization,deterministic simulations,performance evaluation},
  number       = {1},
  pages        = {172--191},
  title        = {Benchmarking Derivative-Free Optimization Algorithms},
  volume       = {20},
}



@book{Schittkowski1981MoreTE,
author = {Schittkowski, Klaus}, 
title = {More Test Examples for Nonlinear Programming Codes}, 
year = {1987}, 
isbn = {978-3-540-17182-9}, 
publisher = {Springer-Verlag}, 
address = {Berlin, Heidelberg}, 
doi={https://doi.org/10.100/978-3-642-61582-5},
series={Lecture Notes in Economics and Mathematical Systems},
number={282}
}


@article{Hock1980,
  abstract     = {The increasing importance of nonlinear programming software requires an enlarged set of test examples. The purpose of this note is to point out how an interested mathematical programmer could obtain computer programs of more than 120 constrained nonlinear programming problems which have been used in the past to test and compare optimization codes.},
  author       = {Hock, W. and Schittkowski, K.},
  url          = {https://doi.org/10.1007/BF00934594},
  date         = {1980-01-01},
  year         = {1980},
  doi          = {10.1007/BF00934594},
  issn         = {1573-2878},
  journal      = {Journal of Optimization Theory and Applications},
  number       = {1},
  pages        = {127--129},
  title        = {Test examples for nonlinear programming codes},
  volume       = {30},
}



@article{doi:10.1137/151005683,
  author       = {Garmanjani, R. and Júdice, D. and Vicente, L.},
  url          = {https://doi.org/10.1137/151005683},
  year         = {2016},
  doi          = {10.1137/151005683},
  eprint       = {https://doi.org/10.1137/151005683},
  journal      = {SIAM Journal on Optimization},
  number       = {4},
  pages        = {1987--2011},
  title        = {Trust-Region Methods Without Using Derivatives: Worst Case Complexity and the NonSmooth Case},
  volume       = {26},
}

@article{Ciarlet1971,
  author       = {Ciarlet, P. G. and Wagschal, C.},
  url          = {https://doi.org/10.1007/BF01395869},
  date         = {1971-02-01},
  year         = {1971},
  doi          = {10.1007/BF01395869},
  issn         = {0945-3245},
  journal      = {Numerische Mathematik},
  number       = {1},
  pages        = {84--100},
  title        = {Multipoint Taylor formulas and applications to the finite element method},
  volume       = {17},
}

@article{Ciarlet1972,
  author       = {Ciarlet, P. G. and Raviart, P. A.},
  url          = {https://doi.org/10.1007/BF00252458},
  date         = {1972-01-01},
  year         = {1972},
  doi          = {10.1007/BF00252458},
  issn         = {1432-0673},
  journal      = {Archive for Rational Mechanics and Analysis},
  number       = {3},
  pages        = {177--199},
  title        = {General lagrange and hermite interpolation in Rn with applications to finite element methods},
  volume       = {46},
}

@inproceedings{Audet2016APB,
  author = {Audet, Charles and Le~Digabel, S{é}bastien},
  year   = {2016},
  title  = {A progressive barrier derivative-free trust-region algorithm for constrained optimization},
}

@article{Troltzsch2016,
  abstract     = {In this paper, we present a new model-based trust-region derivative-free optimization algorithm which can handle nonlinear equality constraints by applying a sequential quadratic programming (SQP) approach. The SQP methodology is one of the best known and most efficient frameworks to solve equality-constrained optimization problems in gradient-based optimization [see e.g. Lalee et al. (SIAM J Optim 8:682–706, 1998), Schittkowski (Optim Lett 5:283–296, 2011), Schittkowski and Yuan (Wiley encyclopedia of operations research and management science, Wiley, New York, 2010)]. Our derivative-free optimization (DFO) algorithm constructs local polynomial interpolation-based models of the objective and constraint functions and computes steps by solving QP sub-problems inside a region using the standard trust-region methodology. As it is crucial for such model-based methods to maintain a good geometry of the set of interpolation points, our algorithm exploits a self-correcting property of the interpolation set geometry. To deal with the trust-region constraint which is intrinsic to the approach of self-correcting geometry, the method of Byrd and Omojokun is applied. Moreover, we will show how the implementation of such a method can be enhanced to outperform well-known DFO packages on smooth equality-constrained optimization problems. Numerical experiments are carried out on a set of test problems from the CUTEst library and on a simulation-based engineering design problem.},
  author       = {Tröltzsch, Anke},
  language     = {English},
  year         = {2016},
  doi          = {10.1007/s11590-014-0830-y},
  journal      = {Optimization Letters},
  keywords     = {Derivative-free optimization; Nonlinear optimization; Trust region; Equality constraints; SQP; Numerical experiments},
  number       = {2},
  pages        = {383--399},
  title        = {A sequential quadratic programming algorithm for equality-constrained optimization without derivatives},
  volume       = {10},
}

@article{doi:10.1080/02331934.2016.1263629,
  author       = {Ferreira, P. S. and Karas, E. W. and Sachine, M. and Sobral, F. N. C.},
  publisher    = {Taylor \& Francis},
  url          = {https://doi.org/10.1080/02331934.2016.1263629},
  year         = {2017},
  doi          = {10.1080/02331934.2016.1263629},
  eprint       = {https://doi.org/10.1080/02331934.2016.1263629},
  journal      = {Optimization},
  number       = {2},
  pages        = {271--292},
  title        = {Global convergence of a derivative-free inexact restoration filter algorithm for nonlinear programming},
  volume       = {66},
}

@article{doi:10.1080/10556780802409296,
  author       = {Fasano, Giovanni and Morales, José Luis and Nocedal, Jorge},
  publisher    = {Taylor \& Francis},
  url          = {https://doi.org/10.1080/10556780802409296},
  year         = {2009},
  doi          = {10.1080/10556780802409296},
  eprint       = {https://doi.org/10.1080/10556780802409296},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {145--154},
  title        = {On the geometry phase in model-based algorithms for derivative-free optimization},
  volume       = {24},
}

@article{doi:10.1093/imanum/drx043,
  author       = {Gratton, Serge and Royer, Cl\'{e}ment W and Vicente, Luís N and Zhang, Zaikun},
  url          = {http://dx.doi.org/10.1093/imanum/drx043},
  year         = {2018},
  doi          = {10.1093/imanum/drx043},
  eprint       = {/oup/backfile/content_public/journal/imajna/38/3/10.1093_imanum_drx043/3/drx043.pdf},
  journal      = {IMA Journal of Numerical Analysis},
  number       = {3},
  pages        = {1579--1597},
  title        = {Complexity and global rates of trust-region methods based on probabilistic models},
  volume       = {38},
}

@article{Beyhaghi2017,
  abstract     = {This paper introduces a modification of our original Delaunay-based optimization algorithm (developed in JOGO DOI:10.1007/s10898-015-0384-2) that reduces the number of function evaluations on the boundary of feasibility as compared with the original algorithm. A weaknesses we have identified with the original algorithm is the sometimes faulty behavior of the generated uncertainty function near the boundary of feasibility, which leads to more function evaluations along the boundary of feasibility than might otherwise be necessary. To address this issue, a second search function is introduced which has improved behavior near the boundary of the search domain. Additionally, the datapoints are quantized onto a Cartesian grid, which is successively refined, over the search domain. These two modifications lead to a significant reduction of datapoints accumulating on the boundary of feasibility, and faster overall convergence.},
  author       = {Beyhaghi, Pooriya and Bewley, Thomas},
  url          = {https://doi.org/10.1007/s10898-017-0548-3},
  date         = {2017-12-01},
  year         = {2017},
  doi          = {10.1007/s10898-017-0548-3},
  issn         = {1573-2916},
  journal      = {Journal of Global Optimization},
  number       = {4},
  pages        = {927--949},
  title        = {Implementation of Cartesian grids to accelerate Delaunay-based derivative-free optimization},
  volume       = {69},
}

@inproceedings{Golovin:2017:GVS:3097983.3098043,
  author    = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D.},
  location  = {Halifax, NS, Canada},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3097983.3098043},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2017},
  doi       = {10.1145/3097983.3098043},
  isbn      = {978-1-4503-4887-4},
  keywords  = {bayesian inference,black-box optimization,gaussian process,machine learning},
  pages     = {1487--1495},
  series    = {KDD '17},
  title     = {Google Vizier: A Service for Black-Box Optimization},
}

@article{Kamandi2017,
  abstract     = {This paper gives a variant trust-region method, where its radius is automatically adjusted by using the model information gathered at the current and preceding iterations. The primary aim is to decrease the number of function evaluations and solving subproblems, which increases the efficiency of the trust-region method. The next aim is to update the new radius for large-scale problems without imposing too much computational cost to the scheme. Global convergence to first-order stationary points is proved under classical assumptions. Preliminary numerical experiments on a set of test problems from the CUTEst collection show that the presented method is promising for solving unconstrained optimization problems.},
  author       = {Kamandi, Ahmad and Amini, Keyvan and Ahookhosh, Masoud},
  url          = {https://doi.org/10.1007/s11590-016-1018-4},
  date         = {2017-03-01},
  year         = {2017},
  doi          = {10.1007/s11590-016-1018-4},
  issn         = {1862-4480},
  journal      = {Optimization Letters},
  number       = {3},
  pages        = {555--569},
  title        = {An improved adaptive trust-region algorithm},
  volume       = {11},
}

@article{Neveu2017,
  abstract     = {Model-based, derivative-free, trust-region algorithms are increasingly popular for optimizing computationally expensive numerical simulations. A strength of such methods is their efficient use of function evaluations. In this paper, we use one such algorithm to optimize the beam dynamics in two cases of interest at the Argonne Wakefield Accelerator (AWA) facility. First, we minimize the emittance of a 1 nC electron bunch produced by the AWA rf photocathode gun by adjusting three parameters: rf gun phase, solenoid strength, and laser radius. The algorithm converges to a set of parameters that yield an emittance of 1.08 μm. Second, we expand the number of optimization parameters to model the complete AWA rf photoinjector (the gun and six accelerating cavities) at 40 nC. The optimization algorithm is used in a Pareto study that compares the trade-off between emittance and bunch length for the AWA 70MeV photoinjector.},
  author       = {Neveu, N and Larson, J and Power, J G and Spentzouris, L},
  url          = {http://stacks.iop.org/1742-6596/874/i=1/a=012062},
  year         = {2017},
  journal      = {Journal of Physics: Conference Series},
  number       = {1},
  pages        = {012062},
  title        = {Photoinjector optimization using a derivative-free, model-based trust-region algorithm for the {A}rgonne {W}akefield {A}ccelerator},
  volume       = {874},
}


@article{KS2018,
   title={Ensemble {K}alman inversion: a derivative-free technique for machine learning tasks},
   volume={35},
   ISSN={1361-6420},
   url={http://dx.doi.org/10.1088/1361-6420/ab1c3a},
   DOI={10.1088/1361-6420/ab1c3a},
   number={9},
   journal={Inverse Problems},
   publisher={IOP Publishing},
   author={Kovachki, Nikola B and Stuart, Andrew M},
   year={2019},
   month={Aug},
   pages={095005}
}

@inproceedings{8247938,
  abstract  = {We focus on simulation optimization algorithms that are designed to accommodate noisy black-box functions on mixed integer/continuous domains. There are several approaches used to account for noise which include aggregating multiple function replications from sample points and a newer method of aggregating single replications within a “shrinking ball.” We examine a range of algorithms, including, simulated annealing, interacting particle, covariance-matrix adaption evolutionary strategy, and particle swarm optimization to compare the effectiveness in generating optimal solutions using averaged function replications versus a shrinking ball approximation. We explore problems in mixed integer/continuous domains. Six test functions are examined with 10 and 20 dimensions, with integer restrictions enforced on 0%, 50%, and 100% of the dimensions, and with noise ranging from 10% to 20% of function output. This study demonstrates the relative effectiveness of using the shrinking ball approach, demonstrating that its use typically enhances solver performance for the tested optimization methods.},
  author    = {Linz, D. D. and Zabinsky, Z. B. and Kiatsupaibul, S. and Smith, R. L.},
  booktitle = {2017 Winter Simulation Conference (WSC)},
  date      = {2017-12},
  year      = {2017},
  doi       = {10.1109/WSC.2017.8247938},
  issn      = {1558-4305},
  keywords  = {approximation theory;covariance matrices;evolutionary computation;integer programming;particle swarm optimisation;simulated annealing;covariance-matrix adaption evolutionary strategy;particle swarm optimization;optimal solutions;averaged function replications;shrinking ball approximation;mixed integer/continuous domains;test functions;integer restrictions;function output;shrinking ball approach;tested optimization methods;computational comparison;simulation optimization methods;single observations;black-box functions;simulation optimization algorithms;multiple function replications;simulated annealing;particle interaction;Modeling;Simulated annealing;Benchmark testing;Estimation;Approximation algorithms;Particle swarm optimization},
  pages     = {2045--2056},
  title     = {A computational comparison of simulation optimization methods using single observations within a shrinking ball on noisy black-box functions with mixed integer and continuous domains},
}

@article{doi:10.1002/aic.16364,
  abstract     = {We present an improved trust region filter (TRF) method for optimization of combined glass box/black box systems. Glass box systems refer to models that are easily expressed in an algebraic modeling language, providing cheap and accurate derivative information. By contrast, black box systems may be computationally expensive and derivatives are unavailable. The TRF method, as first introduced in our previous work (Eason and Biegler, AIChE J. 2016; 62:3124–3136), is able to handle hybrid systems containing both glass and black box components, which can frequently arise in chemical engineering, for example, when a multiphase reactor model is included in a flow sheet optimization problem. We discuss several recent modifications in the algorithm such as the sampling region, which maintains the algorithm's global convergence properties without requiring the trust region to shrink to zero in the limit. To benchmark the development of this optimization method, a test set of problems is generated based on modified problems from the CUTEr and COPS sets. The modified algorithm demonstrates improved performance using the test problem set. Finally, the algorithm is implemented within the Pyomo environment and demonstrated on a rigorous process optimization case study for carbon capture. © 2018 American Institute of Chemical Engineers AIChE J, 64: 3934–3943, 2018},
  author       = {Eason, John P. and Biegler, Lorenz T.},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aic.16364},
  doi          = {10.1002/aic.16364},
  eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aic.16364},
  journal      = {AIChE Journal},
  keywords     = {Simulation,process,optimization,adsorption/gas,mathematical modeling},
  number       = {11},
  pages        = {3934--3943},
  title        = {Advanced trust region optimization strategies for glass box/black box models},
  volume       = {64},
  year         = {2018},
}

@book{AuHa2017,
  author    = {Audet, C. and Hare, W.},
  location  = {Berlin},
  publisher = {Springer International Publishing},
  year      = {2017},
  doi       = {10.1007/978-3-319-68913-5},
  pages     = {302},
  series    = {{Springer Series in Operations Research and Financial Engineering}},
  title     = {{Derivative-Free and Blackbox Optimization}},
}

@article{Verderio2017,
  abstract     = {We consider derivative-free trust-region algorithms based on sampling approaches for convex constrained problems and discuss two conditions on the quadratic models for ensuring their global convergence. The first condition requires the poisedness of the sample sets, as usual in this context, while the other one is related to the error between the model and the objective function at the sample points. Although the second condition trivially holds if the model is constructed by polynomial interpolation, since in this case the model coincides with the objective function at the sample set, we show that it also holds for models constructed by support vector regression. These two conditions imply that the error between the gradient of the trust-region model and the objective function is of the order of {\$}{\$}{\backslash}delta {\_}k{\$}{\$}$\delta$k, where {\$}{\$}{\backslash}delta {\_}k{\$}{\$}$\delta$kcontrols the diameter of the sample set. This allows proving the global convergence of a trust-region algorithm that uses two radii, {\$}{\$}{\backslash}delta {\_}k{\$}{\$}$\delta$kand the trust-region radius. Preliminary numerical experiments are presented for minimizing functions with and without noise.},
  author       = {Verd{é}rio, Adriano and Karas, Elizabeth W. and Pedroso, Lucas G. and Scheinberg, Katya},
  url          = {https://doi.org/10.1007/s13675-017-0081-7},
  date         = {2017-12-01},
  year         = {2017},
  doi          = {10.1007/s13675-017-0081-7},
  issn         = {2192-4414},
  journal      = {EURO Journal on Computational Optimization},
  number       = {4},
  pages        = {501--527},
  title        = {On the construction of quadratic models for derivative-free trust-region algorithms},
  volume       = {5},
}

@article{Gao2017,
  abstract     = {In this note, we present a derivative-free trust-region (TR) algorithm for reliability based optimization (RBO) problems. The proposed algorithm consists of solving a set of subproblems, in which simple surrogate models of the reliability constraints are constructed and used in solving the subproblems. Taking advantage of the special structure of the RBO problems, we employ a sample reweighting method to evaluate the failure probabilities, which constructs the surrogate for the reliability constraints by performing only a single full reliability evaluation in each iteration. With numerical experiments, we illustrate that the proposed algorithm is competitive against existing methods.},
  author       = {Gao, Tian and Li, Jinglai},
  url          = {https://doi.org/10.1007/s00158-016-1587-y},
  date         = {2017-04-01},
  year         = {2017},
  doi          = {10.1007/s00158-016-1587-y},
  issn         = {1615-1488},
  journal      = {Structural and Multidisciplinary Optimization},
  number       = {4},
  pages        = {1535--1539},
  title        = {A derivative-free trust-region algorithm for reliability-based optimization},
  volume       = {55},
}


@article{Martínez2013,
  abstract     = {Many derivative-free methods for constrained problems are not efficient for minimizing functions on ``thin'' domains. Other algorithms, like those based on Augmented Lagrangians, deal with thin constraints using penalty-like strategies. When the constraints are computationally inexpensive but highly nonlinear, these methods spend many potentially expensive objective function evaluations motivated by the difficulties in improving feasibility. An algorithm that handles this case efficiently is proposed in this paper. The main iteration is split into two steps: restoration and minimization. In the restoration step, the aim is to decrease infeasibility without evaluating the objective function. In the minimization step, the objective function f is minimized on a relaxed feasible set. A global minimization result will be proved and computational experiments showing the advantages of this approach will be presented.},
  author       = {Mart{í}nez, J. M. and Sobral, F. N. C.},
  url          = {https://doi.org/10.1007/s10898-012-9944-x},
  date         = {2013-07-01},
  date         = {2013},
  doi          = {10.1007/s10898-012-9944-x},
  issn         = {1573-2916},
  journal = {Journal of Global Optimization},
  number       = {3},
  pages        = {1217--1232},
  title        = {Constrained derivative-free optimization on thin domains},
  volume       = {56},
}

@article{Amaran2014,
  abstract     = {Simulation optimization refers to the optimization of an objective function subject to constraints, both of which can be evaluated through a stochastic simulation. To address specific features of a particular simulation---discrete or continuous decisions, expensive or cheap simulations, single or multiple outputs, homogeneous or heterogeneous noise---various algorithms have been proposed in the literature. As one can imagine, there exist several competing algorithms for each of these classes of problems. This document emphasizes the difficulties in simulation optimization as compared to algebraic model-based mathematical programming makes reference to state-of-the-art algorithms in the field, examines and contrasts the different approaches used, reviews some of the diverse applications that have been tackled by these methods, and speculates on future directions in the field.},
  author       = {Amaran, Satyajith and Sahinidis, Nikolaos V. and Sharda, Bikram and Bury, Scott J.},
  url          = {https://doi.org/10.1007/s10288-014-0275-2},
  date         = {2014-12-01},
  date         = {2014},
  doi          = {10.1007/s10288-014-0275-2},
  issn         = {1614-2411},
  journal      = {4OR},
  number       = {4},
  pages        = {301--333},
  title        = {Simulation optimization: a review of algorithms and applications},
  volume       = {12},
}

@inproceedings{Costa2014RBFOptA,
  author = {Costa, Alberto and Nannicini, Giacomo},
  year   = {2014},
  title  = {RBFOpt : an open-source library for black-box optimization with costly function evaluations},
}

@article{doi:10.1137/15M1031679,
  author       = {Maggiar, A. and Wächter, A. and Dolinskaya, I. and Staum, J.},
  url          = {https://doi.org/10.1137/15M1031679},
  year         = {2018},
  doi          = {10.1137/15M1031679},
  eprint       = {https://doi.org/10.1137/15M1031679},
  journal      = {SIAM Journal on Optimization},
  number       = {2},
  pages        = {1478--1507},
  title        = {A Derivative-Free Trust-Region Algorithm for the Optimization of Functions Smoothed via Gaussian Convolution Using Adaptive Multiple Importance Sampling},
  volume       = {28},
}

@article{Gao2018,
  abstract     = {This paper focuses on a class of nonlinear optimization subject to linear inequality constraints with unavailable-derivative objective functions. We propose a derivative-free trust-region methods with interior backtracking technique for this optimization. The proposed algorithm has four properties. Firstly, the derivative-free strategy is applied to reduce the algorithm's requirement for first- or second-order derivatives information. Secondly, an interior backtracking technique ensures not only to reduce the number of iterations for solving trust-region subproblem but also the global convergence to standard stationary points. Thirdly, the local convergence rate is analyzed under some reasonable assumptions. Finally, numerical experiments demonstrate that the new algorithm is effective.},
  author       = {Gao, Jing and Cao, Jian},
  url          = {https://doi.org/10.1186/s13660-018-1698-7},
  date         = {2018-05-09},
  year         = {2018},
  doi          = {10.1186/s13660-018-1698-7},
  issn         = {1029-242X},
  journal      = {Journal of Inequalities and Applications},
  number       = {1},
  pages        = {108},
  title        = {A class of derivative-free trust-region methods with interior backtracking technique for nonlinear optimization problems subject to linear inequality constraints},
  volume       = {2018},
}

@article{PLOSKAS201816,
  author       = {Ploskas, Nikolaos and Laughman, Christopher and Raghunathan, Arvind U. and Sahinidis, Nikolaos V.},
  url          = {http://www.sciencedirect.com/science/article/pii/S0263876217303027},
  year         = {2018},
  doi          = {https://doi.org/10.1016/j.cherd.2017.05.015},
  issn         = {0263-8762},
  journal      = {Chemical Engineering Research and Design},
  keywords     = {Heat exchanger design,Refrigerant circuitry,Optimization,Derivative-free algorithms},
  note         = {Energy Systems Engineering},
  pages        = {16--28},
  title        = {Optimization of circuitry arrangements for heat exchangers using derivative-free optimization},
  volume       = {131},
}


@article{Powell2015,
  abstract     = {Quadratic models {\$}{\$}Q{\_}k ( {\backslash}underline{\{}x\}), {\backslash}underline{\{}x\}{\backslash}in {\backslash}mathcal{\{}R\}^n{\$}{\$}Qk(x̲),x̲∈Rn, of the objective function {\$}{\$}F ( {\backslash}underline{\{}x\}), {\backslash}underline{\{}x\}{\backslash}in {\backslash}mathcal{\{}R\}^n{\$}{\$}F(x̲),x̲∈Rn, are used by many successful iterative algorithms for minimization, where k is the iteration number. Given the vector of variables {\$}{\$}{\backslash}underline{\{}x\}{\_}k {\backslash}in {\backslash}mathcal{\{}R\}^n{\$}{\$}x̲k∈Rn, a new vector {\$}{\$}{\backslash}underline{\{}x\}{\_}{\{}k+1\}{\$}{\$}x̲k+1may be calculated that satisfies {\$}{\$}Q{\_}k ( {\backslash}underline{\{}x\}{\_}{\{}k+1\} ) < Q{\_}k ( {\backslash}underline{\{}x\}{\_}k ){\$}{\$}Qk(x̲k+1)<Qk(x̲k), in the hope that it provides the reduction {\$}{\$}F ( {\backslash}underline{\{}x\}{\_}{\{}k+1\} ) < F ( {\backslash}underline{\{}x\}{\_}k ){\$}{\$}F(x̲k+1)<F(x̲k). Trust region methods include a bound of the form {\$}{\$}{\backslash}Vert {\backslash}underline{\{}x\}{\_}{\{}k+1\} - {\backslash}underline{\{}x\}{\_}k {\backslash}Vert {\backslash}le {\backslash}Delta {\_}k{\$}{\$}{\textbardbl}x̲k+1-x̲k{\textbardbl}≤$\Delta$k. Also we allow general linear constraints on the variables that have to hold at {\$}{\$}{\backslash}underline{\{}x\}{\_}k{\$}{\$}x̲kand at {\$}{\$}{\backslash}underline{\{}x\}{\_}{\{}k+1\}{\$}{\$}x̲k+1. We consider the construction of {\$}{\$}{\backslash}underline{\{}x\}{\_}{\{}k+1\}{\$}{\$}x̲k+1, using only of magnitude {\$}{\$}n^2{\$}{\$}n2operations on a typical iteration when n is large. The linear constraints are treated by active sets, which may be updated during an iteration, and which decrease the number of degrees of freedom in the variables temporarily, by restricting {\$}{\$}{\backslash}underline{\{}x\}{\$}{\$}x̲to an affine subset of {\$}{\$}{\backslash}mathcal{\{}R\}^n{\$}{\$}Rn. Conjugate gradient and Krylov subspace methods are addressed for adjusting the reduced variables, but the resultant steps are expressed in terms of the original variables. Termination conditions are given that are intended to combine suitable reductions in {\$}{\$}Q{\_}k ( {\backslash}cdot ){\$}{\$}Qk(·)with a sufficiently small number of steps. The reason for our work is that {\$}{\$}{\backslash}underline{\{}x\}{\_}{\{}k+1\}{\$}{\$}x̲k+1is required in the LINCOA software of the author, which is designed for linearly constrained optimization without derivatives when there are hundreds of variables. Our studies go beyond the final version of LINCOA, however, which employs conjugate gradients with termination at the trust region boundary. In particular, we find that, if an active set is updated at a point that is not the trust region centre, then the Krylov method may terminate too early due to a degeneracy. An extension to the conjugate gradient method for searching round the trust region boundary receives attention too, but it was also rejected from LINCOA, because of poor numerical results. The given techniques of LINCOA seem to be adequate in practice.},
  author       = {Powell, M. J. D.},
  url          = {https://doi.org/10.1007/s12532-015-0084-4},
  date         = {2015-09-01},
  year         = {2015},
  doi          = {10.1007/s12532-015-0084-4},
  issn         = {1867-2957},
  journal      = {Mathematical Programming Computation},
  number       = {3},
  pages        = {237--267},
  title        = {On fast trust region methods for quadratic models with linear constraints},
  volume       = {7},
}

@article{Kieslich2018,
  abstract     = {A surrogate-based optimization method is presented, which aims to locate the global optimum of box-constrained problems using input--output data. The method starts with a global search of the n-dimensional space, using a Smolyak (Sparse) grid which is constructed using Chebyshev extrema in the one-dimensional space. The collected samples are used to fit polynomial interpolants, which are used as surrogates towards the search for the global optimum. The proposed algorithm adaptively refines the grid by collecting new points in promising regions, and iteratively refines the search space around the incumbent sample until the search domain reaches a minimum hyper-volume and convergence has been attained. The algorithm is tested on a large set of benchmark problems with up to thirty dimensions and its performance is compared to a recent algorithm for global optimization of grey-box problems using quadratic, kriging and radial basis functions. It is shown that the proposed algorithm has a consistently reliable performance for the vast majority of test problems, and this is attributed to the use of Chebyshev-based Sparse Grids and polynomial interpolants, which have not gained significant attention in surrogate-based optimization thus far.},
  author       = {Kieslich, Chris A. and Boukouvala, Fani and Floudas, Christodoulos A.},
  url          = {https://doi.org/10.1007/s10898-018-0643-0},
  date         = {2018-08-01},
  year         = {2018},
  doi          = {10.1007/s10898-018-0643-0},
  issn         = {1573-2916},
  journal      = {Journal of Global Optimization},
  number       = {4},
  pages        = {845--869},
  title        = {Optimization of black-box problems using Smolyak grids and polynomial approximations},
  volume       = {71},
}


@TechReport{ledigabel2015taxonomy,
  title       = {A Taxonomy of Constraints in Simulation-Based Optimization},
  author      = {Le~Digabel, Sébastien and Wild, Stefan M.},
  year        = {2015},
  number={1505.07881},
  institution = {ArXiv},
}


@article{Khachiyan1993,
  abstract     = {We give a new polynomial bound on the complexity of approximating the maximal inscribed ellipsoid for a polytope.},
  author       = {Khachiyan, Leonid G. and Todd, Michael J.},
  url          = {https://doi.org/10.1007/BF01582144},
  date         = {1993-08-01},
  year         = {1993},
  doi          = {10.1007/BF01582144},
  issn         = {1436-4646},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {137--159},
  title        = {On the complexity of approximating the maximal inscribed ellipsoid for a polytope},
  volume       = {61},
}

@article{miguel_review,
  abstract     = {This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.},
  author       = {Rios, Luis Miguel and Sahinidis, Nikolaos V.},
  url          = {http://dx.doi.org/10.1007/s10898-012-9951-y},
  year         = {2013},
  doi          = {10.1007/s10898-012-9951-y},
  issn         = {1573-2916},
  journal      = {Journal of Global Optimization},
  number       = {3},
  pages        = {1247--1293},
  title        = {Derivative-free optimization: a review of algorithms and comparison of software implementations},
  volume       = {56},
}

@misc{custodio_review2,
  author = {Custodio, A.L. and Scheinberg, K. and Vicente, L.N.},
  url    = {https://www.mat.uc.pt/~lnv/papers/dfo-survey.pdf},
  year   = {2017},
  title  = {Methodologies and Software for Derivative-free Optimization},
}

@article{continuity_of_metric_projections,
  abstract     = {Let X be a Hilbert space, and consider the point x0 minimizing, for a given f in X, the distance ∥x − f∥ as x ranges over a polyhedral set C defined by a finite number of real-valued equalities and inequalities. We wish to see how x0 varies when f and C vary. It is easy to see that x0 is Hölder continuous with exponent 12 in its dependence on f and C; this estimate is in general sharp. We show, however, that in certain cases x0 is actually Lipschitz continuous in its dependence on the parameters which are used to define the set C.},
  author       = {Daniel, James W},
  url          = {http://www.sciencedirect.com/science/article/pii/0021904574900653},
  year         = {1974},
  doi          = {https://doi.org/10.1016/0021-9045(74)90065-3},
  issn         = {0021-9045},
  journal      = {Journal of Approximation Theory},
  number       = {3},
  pages        = {234--239},
  title        = {The continuity of metric projections as functions of the data},
  volume       = {12},
}

@article{perturbations_of_linear_inequalities,
  abstract     = {We consider what happens to sets defined by systems of linear inequalities when elements of the system are perturbed. If S = {x ∣ Gx ≤ g, Dx = d}, and if S' and S" are defined in the obvious manner by perturbed matrices G', G'', g', g'', D', D'', d', d", we show that, under certain hypotheses, to each element x' in S' there corresponds x" in S" with |x' - x"| ≤ c {|G' - G"| + |g' - g"| + |D' - D"| + |d' - d"|} (1 + |x'|) for some constant c depending on S.},
  author       = {Daniel, James W.},
  publisher    = {Society for Industrial and Applied Mathematics},
  url          = {http://www.jstor.org/stable/2156358},
  year         = {1973},
  issn         = {00361429},
  journal      = {SIAM Journal on Numerical Analysis},
  number       = {2},
  pages        = {299--307},
  title        = {On Perturbations in Systems of Linear Inequalities},
  volume       = {10},
}

@article{hoffman_theorem,
  abstract     = {Let Ax b be a consistent system of linear inequalities. The principal result is a quantitative formulation of the fact that if JC almostsatisfies the inequalities, then x is "close" to a solution. It is further shown how it is possible in certain cases to estimate the size of the vector joining JC to the nearest solution from the magnitude of the positive coordinates of Ax — b},
  author       = {Hoffman, A. I.},
  url          = {https://pdfs.semanticscholar.org/2c2f/f5a11a3c895e8ee403a2ad3f393509b86910.pdf},
  year         = {1952},
  journal      = {Nat. Bur. Standards},
  number       = {B 49},
  pages        = {263--265},
  title        = {On approximate solutions of systems of linear inequalities},
}

@article{continuity_of_inverse,
  abstract     = {We obtain the optimal perturbation bounds of the Moore–Penrose inverse under the Frobenius norm by using Singular Value Decomposition, which improved the results in the earlier paper [P.-Å. Wedin, Perturbation theory for pseudo-inverses, BIT 13 (1973) 217–232]. In addition, a perturbation bound of the Moore–Penrose inverse under the Frobenius norm in the case of the multiplicative perturbation model is also given.},
  author       = {Meng, Lingsheng and Zheng, Bing},
  url          = {http://www.sciencedirect.com/science/article/pii/S0024379509005230},
  year         = {2010},
  doi          = {https://doi.org/10.1016/j.laa.2009.10.009},
  issn         = {0024-3795},
  journal      = {Linear Algebra and its Applications},
  keywords     = {Singular Value Decomposition,Frobenius norm,Moore–Penrose inverse,Additive perturbation,Multiplicative perturbation},
  number       = {4},
  pages        = {956--963},
  title        = {The optimal perturbation bounds of the Moore–Penrose inverse under the Frobenius norm},
  volume       = {432},
}

@misc{pena2018algorithm,
  author      = {Pena, Javier and Vera, Juan and Zuluaga, Luis},
  year        = {2018},
  eprint      = {1804.08418},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  title       = {An algorithm to compute the Hoffman constant of a system of linear constraints},
}

@article{projecttoellipsoid,
  author       = {Yuri, Kiselev and Orlov, Michael and Fedotova, E.L.},
  date         = {1993-01},
  year         = {1993},
  journal      = {Moscow University Computational Mathematics and Cybernetics},
  title        = {Projecting a point onto an ellipsoid},
  volume       = {1993},
}

@misc{eberly_2013,
  author = {Eberly, David},
  title = {Perspective Projection of an Ellipsoid \hfill},
  year = {2013},
  howpublished = {\linebreak\url{https://www.geometrictools.com/Documentation/PerspectiveProjectionEllipsoid.pdf}},
  url = {\linebreak https://www.geometrictools.com/Documentation/PerspectiveProjectionEllipsoid.pdf},
  urldate = {2013-11-01}
}

@misc{pena2020new,
  author      = {Pena, Javier and Vera, Juan and Zuluaga, Luis},
  year        = {2021},
  eprint      = {1905.02894},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  title       = {New characterizations of Hoffman constants for systems of linear constraints},
}

@article{BillupsLarson2013,
  author       = {Billups, Stephen C. and Larson, Jeffrey and Graf, Peter A.},
  url          = {https://doi.org/10.1137/100814688},
  year         = {2013},
  doi          = {10.1137/100814688},
  journal      = {{SIAM} J. Optim.},
  number       = {1},
  pages        = {27--53},
  title        = {Derivative-Free Optimization of Expensive Functions with Computational Error Using Weighted Regression},
  volume       = {23},
}

@article{larson_menickelly_wild_2019,
  author       = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
  publisher    = {Cambridge University Press},
  year         = {2019},
  doi          = {10.1017/S0962492919000060},
  journal      = {Acta Numerica},
  pages        = {287--404},
  title        = {Derivative-free optimization methods},
  volume       = {28},
}


@article{Le2011a,
  author   = {S. {Le~Digabel}},
  title    = {Algorithm 909: {NOMAD}: Nonlinear Optimization with the {MADS} algorithm},
  journal  = {{ACM} Transactions on Mathematical Software},
  volume   = {37},
  number   = {4},
  pages    = {1--15},
  year     = {2011}
}

@article{AuDe2006,
  author   = {C. Audet and J.E. {Dennis, Jr.}},
  title    = {Mesh Adaptive Direct Search Algorithms for Constrained Optimization},
  journal  = {SIAM Journal on Optimization},
  volume   = {17},
  number   = {1},
  pages    = {188--217},
  doi      = {doi:10.1137/040603371},
  url      = {http://dx.doi.org/doi:10.1137/040603371},
  year     = {2006}
}

@article{10.1093/comjnl/7.4.308,
    author = {Nelder, J. A. and Mead, R.},
    title = {A Simplex Method for Function Minimization},
    journal = {The Computer Journal},
    volume = {7},
    number = {4},
    pages = {308-313},
    year = {1965},
    month = {01},
    abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
    issn = {0010-4620},
    doi = {10.1093/comjnl/7.4.308},
    url = {https://doi.org/10.1093/comjnl/7.4.308},
    eprint = {https://academic.oup.com/comjnl/article-pdf/7/4/308/1013182/7-4-308.pdf},
}



@inbook{pub.1046127469,
 author = {Powell, M. J. D.},
 booktitle = {Advances in Optimization and Numerical Analysis},
 doi = {10.1007/978-94-015-8330-5_4},
 keywords = {},
 pages = {51-67},
 publisher = {Springer},
 title = {A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation},
 url = {https://app.dimensions.ai/details/publication/pub.1046127469},
 year = {1994}
}


@misc{AlwaysFeasibleDFO,
  author = {Trever Hallock},
  title = {An Always Feasible Algorithm for Derivative Free Optimization},
  year = 2021,
  howpublished = {\url{https://github.com/iconocl4st/AlwaysFeasibleConvexDFO}},
  url = {https://github.com/iconocl4st/AlwaysFeasibleConvexDFO},
  urldate = {2021-08-13}
}


@misc{PDFO,
  author = {Powell, M. J. D. and Ragonneau, T. M. and Zhang, Z.},
  title = {{P}owell's Derivative-Free Optimization solvers},
  year = {2021},
  howpublished = {\url{https://www.pdfo.net/}},
  url = {https://www.pdfo.net/},
  urldate = {2021-08-13}
}



@article{Galvan2021,
   doi = {10.1007/s10589-021-00296-1},
   year = {2021},
   note = {To appear},
   author = {Giulio Galvan and Marco Sciandrone and Stefano Lucidi},
   title = {A parameter-free unconstrained reformulation for nonsmooth
problems with convex constraints},
   journal = {Computational Optimization and Applications}
}


@TechReport{Padidar2021,
	title={Derivative-Free Optimization of a Rapid-Cycling Synchrotron},
	author={Jeffrey S. Eldred and Jeffrey Larson and Misha Padidar and Eric Stern and Stefan M. Wild},
	year={2021},
	number={2108.04774},
	institution = {ArXiv},
	url         = {https://arxiv.org/abs/2108.04774},
}



@Article{CONORBIT15,
author = {Rommel G. Regis and Stefan M. Wild},
title = {{CONORBIT}: constrained optimization by radial basis function interpolation in trust regions},
journal = {Optimization Methods and Software},
volume = {32},
number = {3},
pages = {552-580},
year  = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/10556788.2016.1226305},
URL = { 
        https://doi.org/10.1080/10556788.2016.1226305  
},
eprint = { 
        https://doi.org/10.1080/10556788.2016.1226305
    }
}

@article{torczon:convergence,
  title={On the Convergence of Pattern Search Algorithms},
  author={Virginia Torczon},
  journal={SIAM J. Optim.},
  year={1997},
  volume={7},
  pages={1-25}
}

@article{kolda.lewis.ea:optimization,
  title={Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods},
  author={Tamara G. Kolda and Robert Michael Lewis and Virginia Torczon},
  journal={SIAM Rev.},
  year={2003},
  volume={45},
  pages={385-482}
}

@article{conn.scheinberg.ea:recent,
  title={Recent progress in unconstrained nonlinear optimization without derivatives},
  author={Andrew R. Conn and Katya Scheinberg and Philippe L. Toint},
  journal={Mathematical Programming},
  year={1997},
  volume={79},
  pages={397-414}
}


@article{powell:UOBYQA,
  title={{UOBYQA}: unconstrained optimization by quadratic approximation},
  author={M. J. D. Powell},
  journal={Mathematical Programming},
  year={2002},
  volume={92},
  pages={555-582}
}

@Incollection{powell:NEWUOA,
author="Powell, M. J. D.",
editor="Di Pillo, G.
and Roma, M.",
title="The {NEWUOA} software for unconstrained optimization without derivatives",
bookTitle="Large-Scale Nonlinear Optimization",
year="2006",
publisher="Springer US",
address="Boston, MA",
pages="255--297",
abstract="The NEWUOA software seeks the least value of a function F(x), x∈Rn, when F(x) can be calculated for any vector of variables x. The algorithm is iterative, a quadratic model Q≈F being required at the beginning of each iteration, which is used in a trust region procedure for adjusting the variables. When Q is revised, the new Q interpolates F at m points, the value m = 2n + 1 being recommended. The remaining freedom in the new Q is taken up by minimizing the Frobenius norm of the change to ∇2Q. Only one interpolation point is altered on each iteration. Thus, except for occasional origin shifts, the amount of work per iteration is only of order (m+n)2, which allows n to be quite large. Many questions were addressed during the development of NEWUOA, for the achievement of good accuracy and robustness. They include the choice of the initial quadratic model, the need to maintain enough linear independence in the interpolation conditions in the presence of computer rounding errors, and the stability of the updating of certain matrices that allow the fast revision of Q. Details are given of the techniques that answer all the questions that occurred. The software was tried on several test problems. Numerical results for nine of them are reported and discussed, in order to demonstrate the performance of the software for up to 160 variables.",
isbn="978-0-387-30065-8",
doi="10.1007/0-387-30065-1_16",
url="https://doi.org/10.1007/0-387-30065-1_16"
}

@article{oeuvray.bierlaire:boosters,
author = {R. Oeuvray and M. Bierlaire},
title = {Boosters: A Derivative-Free Algorithm Based on Radial Basis Functions},
journal = {International Journal of Modelling and Simulation},
volume = {29},
number = {1},
pages = {26-36},
year  = {2009},
publisher = {Taylor \& Francis},
doi = {10.1080/02286203.2009.11442507},
URL = {https://doi.org/10.1080/02286203.2009.11442507},
eprint = {https://doi.org/10.1080/02286203.2009.11442507}
}

@article{wild.regis.ea:orbit,
author = {Wild, Stefan and Regis, Rommel and Shoemaker, Christine},
year = {2008},
month = {01},
pages = {3197-3219},
title = {{ORBIT}: Optimization by Radial Basis Function Interpolation in Trust-Regions},
volume = {30},
journal = {SIAM J. Scientific Computing},
doi = {10.1137/070691814}
}

@article{wild.shoemaker:global,
author = {Wild, Stefan and Shoemaker, Christine},
year = {2011},
month = {07},
pages = {761-781},
title = {Global Convergence of Radial Basis Function Trust Region Derivative-Free Algorithms},
volume = {21},
journal = {SIAM Journal on Optimization},
doi = {10.1137/09074927X}
}

@Incollection{powell:direct,
author="Powell, M. J. D.",
editor="Gomez, Susana
and Hennart, Jean-Pierre",
title="A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation",
bookTitle="Advances in Optimization and Numerical Analysis",
year="1994",
publisher="Springer Netherlands",
address="Dordrecht",
pages="51--67",
abstract="An iterative algorithm is proposed for nonlinearly constrained optimization calculations when there are no derivatives. Each iteration forms linear approximations to the objective and constraint functions by interpolation at the vertices of a simplex and a trust region bound restricts each change to the variables. Thus a new vector of variables is calculated, which may replace one of the current vertices, either to improve the shape of the simplex or because it is the best vector that has been found so far, according to a merit function that gives attention to the greatest constraint violation. The trust region radius $\rho$ is never increased, and it is reduced when the approximations of a well-conditioned simplex fail to yield an improvement to the variables, until $\rho$ reaches a prescribed value that controls the final accuracy. Some convergence properties and several numerical results are given, but there are no more than 9 variables in these calculations because linear approximations can be highly inefficient. Nevertheless, the algorithm is easy to use for small numbers of variables.",
isbn="978-94-015-8330-5",
doi="10.1007/978-94-015-8330-5_4",
url="https://doi.org/10.1007/978-94-015-8330-5_4"
}

@article{audet.dennis:mesh,
  title={Mesh Adaptive Direct Search Algorithms for Constrained Optimization},
  author={Charles Audet and John E. Dennis},
  journal={SIAM J. Optim.},
  year={2006},
  volume={17},
  pages={188-217}
}

@article{abramson.audet:convergence,
author = {Abramson, Mark and Audet, Charles},
year = {2006},
month = {01},
pages = {606-619},
title = {Convergence of Mesh Adaptive Direct Search to Second‐Order Stationary Points},
volume = {17},
journal = {SIAM Journal on Optimization},
doi = {10.1137/050638382}
}

@article{booker.dennis.ea:rigorous,
author = {Booker, Andrew and Dennis, J. and Frank, Paul and Serafini, David and Torczon, Virginia and Trosset, Michael},
year = {1998},
month = {09},
pages = {},
title = {A Rigorous Framework for Optimization of Expensive Functions by Surrogates},
volume = {17},
journal = {Structural Optimization},
doi = {10.1007/BF01197708}
}

@article{thi.vaz.ea:optimizing,
author = {Le Thi, Hoai An and Vaz, Ismael and Vicente, L.},
year = {2012},
month = {04},
pages = {190-214},
title = {Optimizing radial basis functions by d.c. programming and its use in direct search for global derivative-free optimization},
volume = {20},
journal = {Top},
doi = {10.1007/s11750-011-0193-9}
}

@article{custodio.vicente:using,
author = {Custódio, A. and Vicente, Luís},
year = {2007},
month = {01},
pages = {537-555},
title = {Using Sampling and Simplex Derivatives in Pattern Search Methods},
volume = {18},
journal = {SIAM Journal on Optimization},
doi = {10.1137/050646706}
}

@article{hooke.jeeves:direct,
author = {Hooke, Robert and Jeeves, T. A.},
title = {``{D}irect Search''  Solution of Numerical and Statistical Problems},
year = {1961},
issue_date = {April 1961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}, volume = {8}, number = {2}, issn = {0004-5411},
url = {https://doi.org/10.1145/321062.321069},
doi = {10.1145/321062.321069},
journal = {J. ACM},
month = {April},
pages = {212–229},
numpages = {18} }

@techreport{fermi.metropolis:numerical,
author = {Fermi, E. and Metropolis, N.},
title = {Numerical solution of a minimum problem},
institution = {Los Alamos Scientific Laboratory of the University of California},
number = {LA-1492},
year = {1952},
url = {https://hdl.handle.net/2027/mdp.39015086487645}
}

@article{lewis.torczon:globally,
author = {Lewis, Robert Michael and Torczon, Virginia},
title = {A Globally Convergent Augmented {L}agrangian Pattern Search 	Algorithm for Optimization with General Constraints and 	Simple Bounds},
journal = {SIAM Journal on Optimization},
volume = {12},
number = {4},
pages = {1075-1089},
year = {2002},
doi = {10.1137/S1052623498339727},
URL = {         
https://doi.org/10.1137/S1052623498339727
},
eprint = { 
        https://doi.org/10.1137/S1052623498339727    
}
}

@techreport{lewis.torczon:direct,
  title={A Direct Search Approach to Nonlinear Programming Problems Using an Augmented {L}agrangian Method with Explicit Treatment of Linear Constraints},
  author={Robert Michael Lewis and Virginia Torczon},
  institution={Department of Computer Science, College of William and Mary},
  number={WM-CS-2010-01},
  url = {http://www.cs.wm.edu/~va/research/wm-cs-2010-01.pdf},
  year={2010}
}

@article{bueno.friedlander.ea:inexact,
  title={Inexact Restoration Method for Derivative-Free Optimization with Smooth Constraints},
  author={Luis Felipe Bueno and Ana Friedlander and Jos{\'e} Mario Mart{\'i}nez and F. N. C. Sobral},
  journal={SIAM J. Optim.},
  year={2013},
  volume={23},
  pages={1189-1213}
}

@article{brekelmans.driessen.ea:constrained,
  title={Constrained optimization involving expensive function evaluations: A sequential approach},
  author={R.C.M. Brekelmans and Lonneke Driessen and Herbert Hamers and Dick den Hertog},
  journal={Eur. J. Oper. Res.},
  year={2005},
  volume={160},
  pages={121-138}
}

@article{ferreira.karas.ea:global,
  title={Global convergence of a derivative-free inexact restoration filter algorithm for nonlinear programming},
  author={Priscila S. Ferreira and Elizabeth W. Karas and Mael Sachine and F. N. C. Sobral},
  journal={Optimization},
  year={2017},
  volume={66},
  pages={271 - 292}
}

@article{lewis.torczon:pattern1999,
  title={Pattern Search Algorithms for Bound Constrained Minimization},
  author={Robert Michael Lewis and Virginia Torczon},
  journal={SIAM J. Optim.},
  year={1999},
  volume={9},
  pages={1082-1099}
}

@article{lewis.torczon:pattern2000,
  title={Pattern Search Methods for Linearly Constrained Minimization},
  author={Robert Michael Lewis and Virginia Torczon},
  journal={SIAM J. Optim.},
  year={2000},
  volume={10},
  pages={917-941}
}

@misc{chandramouli.narayanan:scaled,
      title={A scaled conjugate gradient based direct search algorithm for high dimensional box constrained derivative free optimization}, 
      author={Gannavarapu Chandramouli and Vishnu Narayanan},
      year={2019},
      eprint={1901.05215},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{kolda.lewis.ea:stationarity,
author = { Kolda, Tamara G. and  Lewis, Robert Michael and  Torczon, Virginia},
title = {Stationarity Results for Generating Set Search for Linearly Constrained Optimization},
journal = {SIAM Journal on Optimization},
volume = {17},
number = {4},
pages = {943-968},
year = {2007},
doi = {10.1137/S1052623403433638},
URL = {https://doi.org/10.1137/S1052623403433638}
}

@article{lucidi.sciandrone:derivative-free,
author = {Lucidi, S.  and Sciandrone, M.},
year = {2002},
pages = {119-142},
title = {A derivative-free algorithm for bound constrained optimization},
volume = {21},
issue = {2},
journal = {Computational Optimization and Applications},
doi = {10.1023/A:1013735414984}
}

@article{lucidi.sciandrone.ea:objective,
author = {Lucidi, Stefano and Tseng, P.},
year = {2002},
month = {03},
pages = {37-59},
title = {Objective-derivative-free methods for constrained optimization},
volume = {92},
journal = {Mathematical Programming, Series B},
doi = {10.1007/s101070100266}
}

@article{gumma.mohsin.ea:derivative-free2014,
author = {Gumma, Elzain and Hashim, Mohsin and Ali, Montaz},
year = {2014},
month = {04},
pages = {599-621},
title = {A derivative-free algorithm for linearly constrained optimization problems},
volume = {57},
journal = {Computational Optimization and Applications},
doi = {10.1007/s10589-013-9607-y}
}

@article{gumma.mohsin.ea:derivative-free2020,
author = {E. A. E. Gumma and M. Montaz Ali and M. H. A. Hashim},
title = {A derivative-free algorithm for non-linear optimization with linear equality constraints},
journal = {Optimization},
volume = {69},
number = {6},
pages = {1361-1387},
year  = {2020},
publisher = {Taylor \& Francis},
doi = {10.1080/02331934.2019.1690491},
URL = { 
        https://doi.org/10.1080/02331934.2019.1690491    
},
eprint = { 
        https://doi.org/10.1080/02331934.2019.1690491    
}
}

@article{spendley.hext.ea:sequential,
author = { W.   Spendley  and  G. R.   Hext  and  F. R.   Himsworth },
title = {Sequential Application of Simplex Designs in Optimisation and Evolutionary Operation},
journal = {Technometrics},
volume = {4},
number = {4},
pages = {441-461},
year  = {1962},
publisher = {Taylor \& Francis},
doi = {10.1080/00401706.1962.10490033},
URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490033
  },
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/00401706.1962.10490033    
}
}

@PhdThesis{may:linearly,
author = {J. H. May},
title = {Linearly constrained nonlinear programming: a solution method that does not require analytic derivatives},
school = {Yale University},
year = {1974}
}

@article{may:solving,
author = {May, Jerrold H.},
title = {Solving Nonlinear Programs Without Using Analytic Derivatives},
journal = {Operations Research},
volume = {27},
number = {3},
pages = {457-484},
year = {1979},
doi = {10.1287/opre.27.3.457},
URL = { 
        https://doi.org/10.1287/opre.27.3.457    
},
eprint = { 
        https://doi.org/10.1287/opre.27.3.457    
}
}

@article{vandenberghen.bersini:condor,
title = {{CONDOR},  a new parallel, constrained extension of {P}owell's {UOBYQA} algorithm: Experimental results and comparison with the {DFO} algorithm},
journal = {Journal of Computational and Applied Mathematics},
volume = {181},
number = {1},
pages = {157-175},
year = {2005},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2004.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S037704270400559X},
author = {Frank {Vanden Berghen} and Hugues Bersini},
keywords = {Nonlinear, High-computing-load, Noisy optimization, Lagrange interpolation, Trust region method, Optimal shape design, Parallel computing},
abstract = {This paper presents an algorithmic extension of Powell's UOBYQA algorithm (Unconstrained Optimization BY Quadratical Approximation). We start by summarizing the original algorithm of Powell and by presenting it in a more comprehensible form. Thereafter, we report comparative numerical results between UOBYQA, DFO and a parallel, constrained extension of UOBYQA that will be called in the paper CONDOR (COnstrained, Non-linear, Direct, parallel Optimization using trust Region method for high-computing load function). The experimental results are very encouraging and validate the approach. They open wide possibilities in the field of noisy and high-computing-load objective functions optimization (from 2min to several days) like, for instance, industrial shape optimization based on computation fluid dynamic codes or partial differential equations solvers. Finally, we present a new, easily comprehensible and fully stand-alone implementation in C++ of the parallel algorithm.}
}

@PhdThesis{vandenberghen:condor,
author = {F.  Vanden Berghen},
title = {{CONDOR}:  A constrained, non-linear, derivative-free parallel optimizer for continuous, high computing load, noisy objective functions},
school = {Universit\'{e} Libre de Bruxelles},
year = {2004},
url = {http://www.applied-mathematics.net/optimization/thesis_optimization.pdf}
}

@article{box:new,
    author = {Box, M. J.},
    title = {A New Method of Constrained Optimization and a Comparison With Other Methods},
    journal = {The Computer Journal},
    volume = {8},
    number = {1},
    pages = {42-52},
    year = {1965},
    month = {04},
    abstract = {A new method for finding the maximum of a general non-linear function of several variables within a constrained region is described, and shown to be efficient compared with existing methods when the required optimum lies on one or more constraints. The efficacy of using effective constraints to eliminate variables is demonstrated, and a program to achieve this easily and automatically is described. Finally, the performance of the new method (the “Complex” method) with unconstrained problems, is compared with those of the Simplex method, from which it was evolved, and Rosenbrock's method.},
    issn = {0010-4620},
    doi = {10.1093/comjnl/8.1.42},
    url = {https://doi.org/10.1093/comjnl/8.1.42},
    eprint = {https://academic.oup.com/comjnl/article-pdf/8/1/42/1323328/8-1-42.pdf},
}

@Article{audet.ledigabel.ea:linear,
author = {Audet, C. and Le~Digabel, S. and Peyrega, M.},
title = {Linear equalities in blackbox optimization},
journal = {Computational Optimization and Applications},
year = {2015},
volume = {61},
issue = {1},
pages = {1--23},
doi = {10.1007/s10589-014-9708-2},
}

@Article{gratton.royer.ea:direct2019,
author={Gratton, S.
and Royer, C. W.
and Vicente, L. N.
and Zhang, Z.},
title={Direct search based on probabilistic feasible descent for bound and linearly constrained problems},
journal={Computational Optimization and Applications},
year={2019},
month={Apr},
day={01},
volume={72},
number={3},
pages={525-559},
abstract={Direct search is a methodology for derivative-free optimization whose iterations are characterized by evaluating the objective function using a set of polling directions. In deterministic direct search applied to smooth objectives, these directions must somehow conform to the geometry of the feasible region, and typically consist of positive generators of approximate tangent cones (which then renders the corresponding methods globally convergent in the linearly constrained case). One knows however from the unconstrained case that randomly generating the polling directions leads to better complexity bounds as well as to gains in numerical efficiency, and it becomes then natural to consider random generation also in the presence of constraints. In this paper, we study a class of direct-search methods based on sufficient decrease for solving smooth linearly constrained problems where the polling directions are randomly generated (in approximate tangent cones). The random polling directions must satisfy probabilistic feasible descent, a concept which reduces to probabilistic descent in the absence of constraints. Such a property is instrumental in establishing almost-sure global convergence and worst-case complexity bounds with overwhelming probability. Numerical results show that the randomization of the polling directions can be beneficial over standard approaches with deterministic guarantees, as it is suggested by the respective worst-case complexity bounds.},
issn={1573-2894},
doi={10.1007/s10589-019-00062-4},
url={https://doi.org/10.1007/s10589-019-00062-4}
}

@TechReport{powell:bobyqa,
author = {M.J.D. Powell},
title={The {BOBYQA} algorithm for bound constrained optimization without derivatives},
year={2009},
institution = {University of Cambridge},
type = {DAMTP},
number = {2009/NA06},
url = {http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2009_06.pdf}
}

@article{gratton.royer.ea:direct2015,
author = {Gratton, S. and Royer, C. W. and Vicente, L. N. and Zhang, Z.},
title = {Direct Search Based on Probabilistic Descent},
journal = {SIAM Journal on Optimization},
volume = {25},
number = {3},
pages = {1515-1541},
year = {2015},
doi = {10.1137/140961602},
URL = { 
        https://doi.org/10.1137/140961602    
},
eprint = { 
        https://doi.org/10.1137/140961602    
}
}

@article{arouxet.echebest.ea:active-set,
author = {Arouxet, Belen and Echebest, N. and Pilotta, Elvio},
year = {2016},
month = {07},
pages = {171-196},
title = {Active-set strategy in {P}owell's method for optimization without derivatives},
volume = {30},
journal = {Computational and Applied Mathematics},
doi = {10.1590/S1807-03022011000100009}
}

@Article{gumma.hashim.ea:derivative-free,
author={Gumma, E. A. E.
and Hashim, M. H. A.
and Ali, M. Montaz},
title={A derivative-free algorithm for linearly constrained optimization problems},
journal={Computational Optimization and Applications},
year={2014},
month={Apr},
day={01},
volume={57},
number={3},
pages={599-621},
abstract={Based on the NEWUOA algorithm, a new derivative-free algorithm is developed, named LCOBYQA. The main aim of the algorithm is to find a minimizer {\$}x^{\{}*{\}} {\backslash}in{\backslash}mathbb{\{}R{\}}^{\{}n{\}}{\$}of a non-linear function, whose derivatives are unavailable, subject to linear inequality constraints. The algorithm is based on the model of the given function constructed from a set of interpolation points. LCOBYQA is iterative, at each iteration it constructs a quadratic approximation (model) of the objective function that satisfies interpolation conditions, and leaves some freedom in the model. The remaining freedom is resolved by minimizing the Frobenius norm of the change to the second derivative matrix of the model. The model is then minimized by a trust-region subproblem using the conjugate gradient method for a new iterate. At times the new iterate is found from a model iteration, designed to improve the geometry of the interpolation points. Numerical results are presented which show that LCOBYQA works well and is very competing against available model-based derivative-free algorithms.},
issn={1573-2894},
doi={10.1007/s10589-013-9607-y},
url={https://doi.org/10.1007/s10589-013-9607-y}
}

@PhdThesis{wild:derivative-free,
author = {S. M. Wild},
title = {Derivative-free optimization algorithms for computationally expensive functions},
school = {Cornell University},
year = {2008},
url = {http://ecommons.cornell.edu/handle/1813/11248}
}

@article{gratton.toint.ea:active-set,
author = { Serge   Gratton  and  Philippe   L.   Toint  and  Anke   Tröltzsch },
title = {An active-set trust-region method for derivative-free nonlinear bound-constrained optimization},
journal = {Optimization Methods and Software},
volume = {26},
number = {4-5},
pages = {873-894},
year  = {2011},
publisher = {Taylor \& Francis},
doi = {10.1080/10556788.2010.549231},
URL = { 
        https://doi.org/10.1080/10556788.2010.549231   
},
eprint = { 
        https://doi.org/10.1080/10556788.2010.549231   
}
}

@article{audet.dennis:progressive,
author = {Audet, Charles and Dennis, J. E.},
title = {A Progressive Barrier for Derivative-Free Nonlinear Programming},
journal = {SIAM Journal on Optimization},
volume = {20},
number = {1},
pages = {445-472},
year = {2009},
doi = {10.1137/070692662},
URL = { 
        https://doi.org/10.1137/070692662
 },
eprint = { 
        https://doi.org/10.1137/070692662
}
}

@article{liuzzi.lucidi:derivative-free,
author = {Liuzzi, G. and Lucidi, S.},
title = {A Derivative-Free Algorithm for Inequality Constrained Nonlinear Programming via Smoothing of an $\ell_\infty$ Penalty Function},
journal = {SIAM Journal on Optimization},
volume = {20},
number = {1},
pages = {1-29},
year = {2009},
doi = {10.1137/070711451},
URL = { 
        https://doi.org/10.1137/070711451    
},
eprint = { 
        https://doi.org/10.1137/070711451
    }
}



@article{liuzzi.lucidi.ea:sequential,
author = {Liuzzi, Giampaolo and Lucidi, Stefano and Sciandrone, Marco},
title = {Sequential Penalty Derivative-Free Methods for Nonlinear Constrained Optimization},
journal = {SIAM Journal on Optimization},
volume = {20},
number = {5},
pages = {2614-2635},
year = {2010},
doi = {10.1137/090750639},
URL = { 
        https://doi.org/10.1137/090750639
  },
eprint = { 
        https://doi.org/10.1137/090750639    
}
}


@article{fasano.liuzzi.ea:linesearch,
author = {Fasano, G. and Liuzzi, G. and Lucidi, S. and Rinaldi, F.},
title = {A Linesearch-Based Derivative-Free Approach for Nonsmooth Constrained Optimization},
journal = {SIAM Journal on Optimization},
volume = {24},
number = {3},
pages = {959-992},
year = {2014},
doi = {10.1137/130940037},
URL = { 
        https://doi.org/10.1137/130940037
    },
eprint = { 
        https://doi.org/10.1137/130940037    
}
}

@inproceedings{picheny2016bayesian,
 title={Bayesian optimization under mixed constraints with a slack-variable augmented {L}agrangian}, 
      author={Victor Picheny and Robert B. Gramacy and Stefan M. Wild and Sebastien Le~Digabel},
      booktitle={Advances in Neural Information Processing Systems 29},
      eventtitletitle={30th Annual Conference on Neural Information Processing Systems},
      eventdate = {December 5--10,  2016},
      editors={D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
         year={2016},
      pages={1435--1443},
      publisher={Curran Associates, Inc.},     
     location = {Barcelona, Spain},   
     url={https://papers.nips.cc/paper/6439-bayesian-optimization-under-mixed-constraints-with-a-slack-variable-agumented-lagrangian.pdf}
     }
     
  @PhdThesis{Pourmohamad:combining,
  author = {Pourmohamad, Tony},
  title  = {Combining Multivariate Stochastic Process Models with Filter Methods for Constrained Optimization},
  school = {UC Santa Cruz: Statistics and Applied Mathematics},
  year   = {2016},
  url   = {https://scholar.google.com/scholar?cluster=3680794014816203402},
}

@Article{echebest.schuverdt.ea:inexact,
author={Echebest, N.
and Schuverdt, M. L.
and Vignau, R. P.},
title={An inexact restoration derivative-free filter method for nonlinear programming},
journal={Computational and Applied Mathematics},
year={2017},
month={Mar},
day={01},
volume={36},
number={1},
pages={693-718},
abstract={An inexact restoration derivative-free filter method for nonlinear programming is introduced in this paper. Each iteration is composed of a restoration phase, which reduces a measure of infeasibility, and an optimization phase, which reduces the objective function. The restoration phase is solved using a derivative-free method for solving underdetermined nonlinear systems with bound constraints, developed previously by the authors. An alternative for solving the optimization phase is considered. Theoretical convergence results and some preliminary numerical experiments are presented.},
issn={1807-0302},
doi={10.1007/s40314-015-0253-0},
url={https://doi.org/10.1007/s40314-015-0253-0}
}

@article{sampaio.toint:numerical,
author = {Ph. R. Sampaio and Ph. L. Toint},
title = {Numerical experience with a derivative-free trust-funnel method for nonlinear optimization problems with general nonlinear constraints},
journal = {Optimization Methods and Software},
volume = {31},
number = {3},
pages = {511-534},
year  = {2016},
publisher = {Taylor \& Francis},
doi = {10.1080/10556788.2015.1135919},
URL = { 
        https://doi.org/10.1080/10556788.2015.1135919   
},
eprint = { 
        https://doi.org/10.1080/10556788.2015.1135919    
}
}

@article{glass.cooper:sequential,
author = {Glass, H. and Cooper, L.},
title = {Sequential Search: A Method for Solving Constrained Optimization Problems},
year = {1965},
issue_date = {Jan. 1965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/321250.321256},
doi = {10.1145/321250.321256},
journal = {J. ACM},
month = {jan},
pages = {71–82},
numpages = {12}
}
   
   
@Article{burmen.olensek.ea:mesh,
author={B{\H{u}}rmen, {\'A}rp{\'a}d
and Olen{\v{s}}ek, Jernej
and Tuma, Tadej},
title={Mesh adaptive direct search with second directional derivative-based {H}essian update},
journal={Computational Optimization and Applications},
year={2015},
month={Dec},
day={01},
volume={62},
number={3},
pages={693-715},
abstract={The subject of this paper is inequality constrained black-box optimization with mesh adaptive direct search (MADS). The MADS search step can include additional strategies for accelerating the convergence and improving the accuracy of the solution. The strategy proposed in this paper involves building a quadratic model of the function and linear models of the constraints. The quadratic model is built by means of a second directional derivative-based Hessian update. The linear terms are obtained by linear regression. The resulting quadratic programming (QP) problem is solved with a dedicated solver and the original functions are evaluated at the QP solution. The proposed search strategy is computationally less expensive than the quadratically constrained QP strategy in the state of the art MADS implementation (NOMAD). The proposed MADS variant (QPMADS) and NOMAD are compared on four sets of test problems. QPMADS outperforms NOMAD on all four of them for all but the smallest computational budgets.},
issn={1573-2894},
doi={10.1007/s10589-015-9753-5},
url={https://doi.org/10.1007/s10589-015-9753-5}
}
     
     
@article{diniz-ehrhardt.martinez.ea:derivative-free,
author = {Diniz-Ehrhardt, Maria and Martínez, José Mario and Pedroso, L.},
year = {2016},
month = {07},
pages = {19-52},
title = {Derivative-free methods for nonlinear programming with general lower-level constraints},
volume = {30},
journal = {Computational \& Applied Mathematics},
doi = {10.1590/S1807-03022011000100003}
}     
 
 
@article{audet.dennis:pattern,
author = {Audet, Charles and Dennis, J. E.},
title = {Pattern Search Algorithms for Mixed Variable Programming},
journal = {SIAM Journal on Optimization},
volume = {11},
number = {3},
pages = {573-594},
year = {2001},
doi = {10.1137/S1052623499352024},
URL = { 
        https://doi.org/10.1137/S1052623499352024    
},
eprint = { 
        https://doi.org/10.1137/S1052623499352024    
}
}


@Article{sampaio.toint:derivative-free,
author={Sampaio, Ph. R.
and Toint, Ph. L.},
title={A derivative-free trust-funnel method for equality-constrained nonlinear optimization},
journal={Computational Optimization and Applications},
year={2015},
month={May},
day={01},
volume={61},
number={1},
pages={25-49},
abstract={A new derivative-free method is proposed for solving equality-constrained nonlinear optimization problems. This method is of the trust-funnel variety and is also based on the use of polynomial interpolation models. In addition, it uses a self-correcting geometry procedure in order to ensure that the interpolation problem is well defined in the sense that the geometry of the set of interpolation points does not differ too much from the ideal one. The algorithm is described in detail and some encouraging numerical results are presented.},
issn={1573-2894},
doi={10.1007/s10589-014-9715-3},
url={https://doi.org/10.1007/s10589-014-9715-3}
}



@article{regis:constrained,
author = {Rommel G. Regis},
title = {Constrained optimization by radial basis function interpolation for high-dimensional expensive black-box problems with infeasible initial points},
journal = {Engineering Optimization},
volume = {46},
number = {2},
pages = {218-243},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/0305215X.2013.765000},
URL = { 
        https://doi.org/10.1080/0305215X.2013.765000    
},
eprint = { 
        https://doi.org/10.1080/0305215X.2013.765000    
}
}

@misc{augustin.marzouk:trust-region,
      title={A trust-region method for derivative-free nonlinear constrained stochastic optimization}, 
      author={F. Augustin and Y. M. Marzouk},
      year={2017},
      eprint={1703.04156},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}





