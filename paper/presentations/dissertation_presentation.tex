% This text is proprietary.
% It's a part of presentation made by myself.
% It may not used commercial.
% The noncommercial use such as private and study is free
% Sep. 2005 
% Author: Sascha Frank 
% University Freiburg 
% www.informatik.uni-freiburg.de/~frank/


\documentclass{beamer}


\newcommand{\xk}{{{x}^{(k)}}}
\newcommand{\dk}{{\Delta_k}}
\newcommand{\mk}{{m_f}}
\newcommand{\fk}{{f_k}}
\newcommand{\fgk}{{g^{(k)}_f}}
\newcommand{\fhk}{{H^{(k)}_f}}
\newcommand{\ck}{{c^{(k)}_{i}(\xk)}}
\newcommand{\cgk}{{g^{(k)}_{c_i}}}
\newcommand{\mck}{{m_{c_i}}}
\newcommand{\bk}{{B_{\infty}(\xk, \dk)}}



\usepackage{color}


\begin{document}


\title{Master's Presentation}   
\author{Trever Hallock} 
\date{\today} 

\frame{\titlepage} 

\section{Introduction}

\begin{frame}{Introduction}
    \begin{itemize}
        \item We develop an algorithm to find local optima of convex constrained problems
        \item This algorithm is useful for problems where function values are not provided for infeasible points
        \item The focus is on narrow feasible regions
    \end{itemize}
\end{frame}

\section{Derivative Free Background}

\begin{frame}{Derivative Free Problem Formulation}
\begin{center}
\label{Problem}
\begin{align*}
\min_x & \quad f(x) \\
  c_i(x) \le 0   & \quad \forall \; 1 \le i \le m \\
\end{align*}
\end{center}
    \begin{itemize}
        \item All functions are black-box functions, meaning that we have no information about their derivatives
        \item For example, optimization problems where the objective or some of the constraints depend on an expensive simulation
        \item Function values may not be available outside of the feasible region, we call this \emph{partially-quantifiable}
%        \item $S(x)$ is a black-box function, meaning that we have no information about its derivatives
%        \item We assume that $f$ and $c$ are apriori functions
%         \item We assume the level sets of $f$ are bounded
%         \item We assume $f$, $c$ are continuously twice differentiable
%         \item The goal of my research is to develop algorithms for this problem
    \end{itemize}
\end{frame}


\begin{frame}{Strategy}
    \begin{itemize}
        \item Extend our current algorithm for solving linear partially-quantifiable constraints to convex constraints
        \item Account for uncertainty in constraint boundaries
        \item Ensure that no infeasible points must be evaluated
    \end{itemize}
\end{frame}


\frame{\frametitle{Table of Contents}\tableofcontents} 



\begin{frame}{Model Based Trust Region Methods}
    \begin{itemize}
        \setlength\itemsep{2em}
    	\item Approximate derivatives of $f$ and $c$ with model functions created from a sample set
    	\item We approximate using a second order model, meaning that we approximate:
    	\begin{itemize}
            \item $\fhk \approx \nabla ^2 f(\xk)$
            \item $\fgk \approx \nabla f(\xk)$
            \item $\mk(x) = \fk + \left(x - \xk \right)^T\fgk + \left(x - \xk \right)^T\fhk\left(x - \xk \right) \approx f(x)$
            \item $\cgk \approx c_i(\xk)$
            \item $\mck(x) = \ck + \left(x - \xk\right)^T\cgk \approx c_i(x)$
    	\end{itemize}
	    \item Choose next iterate by minimizing a model over a trust region
	\end{itemize}
\end{frame}


\begin{frame}{Model Fitting}
% 	\begin{align*}
% 		l_1(x) = -(x-1)(x+1), \;
% 		l_2(x) = \frac 1 2 x(x+1), \;
% 		l_3(x) = \frac 1 2 x(x-1) \\
% 		f(x) = 3 + \sin(x) \quad
% 		m_f(x) = f(0)l_1(x) + f(1)l_2(x) + f(-1)l_3(x)
% 	\end{align*}
	\begin{center}
		\includegraphics[width=250px]{images/lagrange_polynomials.png}
	\end{center}
\end{frame}


% 12
\begin{frame}{Trust Region Subproblem}
    \begin{itemize}
        \item In each iteration, we attempt to solve the trust region subproblem to compute a step direction $s$

        \begin{displaymath}
\begin{array}{lrcc}
min_s & \mk(s)   &     &            \\
s.t.  &  \mck(x) & \le & 0   \quad \forall \; 1 \le i \le m       \\
      &  s & \in & \bk.  \\
\end{array}
        \end{displaymath}
        \item Replaced true functions with model functions
        \item Added trust region constraint
        \item The solution is then used as a trial point for the next iterate
    \end{itemize}
\end{frame}





%\begin{frame}{Trust Region Management}
%    \setlength\itemsep{2em}
%    \begin{itemize}
%		\item $\rho_k = \frac{f(x^{(k)}) - f(x^{(k)}+s^{(k)})}{m_k(x^{(k)}) - m_k(x^{(k)}+s^{(k)})}$
%        \item This is the actual decrease over the predicted decrease
%        \begin{itemize}
%            \item measures accuracy of model functions
%            \item ensures reduction in the objective
%        \end{itemize}
%        \item Helps determine new trust region radius
%		\begin{itemize}
%		    \item If $\rho_k$ is small, $x^{(k+1)}=x^{(k)}$ (reject) and decrease radius
%		    \item If $\rho_k$ is intermediate, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and decrease radius
%		    \item If $\rho_k$ is large, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and increase radius
%		\end{itemize}
%        \item There are several potential approaches for incorporating constraints
%    \end{itemize}
%\end{frame}


\begin{frame}{Geometry}
    \begin{itemize}
        \item<1, 2, 3> Geometry refers to the shape of the set of sample points
        \item<1, 2, 3> When the points are not well poised, the constructed model can be innacurate \\
        \only<2>{
            \includegraphics[width=120px]{images/poised_good.png} \includegraphics[width=120px]{images/poised_bad.png}
        }
        \item<3> Constructing poised sets over ellipses is well known
        \item<3> Constraints limit what points are available for the sample set
        \item<3> With narrow constraints, this means well poised sets may not exist
        \only<3>{
			\begin{center}
				\includegraphics[width=180px]{images/impossible_poised.png}
			\end{center}
        }
    \end{itemize}
\end{frame}


% \begin{frame}{Problem with choice of Poised points}
% \begin{center}
% \includegraphics[width=120px]{images/nearly_poised.png} \includegraphics[width=120px]{images/nearly_poised_for_subset.png}  
% \end{center}
% \end{frame}


%         \item<3> The geometry can be measured by its $\Lambda$-poisedness

\section{Convex Always Feasible Algorithm}


\begin{frame}{Feasible Derivative Free Algorithm}
    \begin{itemize}
        \item Our algorithm is an instance of a general algorithmic framework analyzed by \cite{CONEJO2013324}
        \item This paper provides convergence analysis, without depending on implementation details
		\item Algorithm assumes known constraints
        \item This framework assumes the following properties
            \begin{itemize}
				\item Quadratic or linear model functions
                \item Ability to satisfy an efficiency condition
                \item Ability to satisfy an accuracy condition
                \item A method for projecting points onto the feasible set
            \end{itemize}
%         \item This algorithm was promising after bench-marking our algorithms on the Hott-Schittowski problem set.
    \end{itemize}
\end{frame}


\begin{frame}{Algorithm outline}
	\begin{itemize}
		\item Construct a model for the current point
		\item Check criticality
		\item Compute step
		\item Check reduction
		\item Either accept or decrease radius
	\end{itemize}
\end{frame}

% 
% \begin{frame}{Feasible Derivative Free Algorithm Part 2}
%     \begin{itemize}
%         \item The efficiency condition requires: $$m_f^{(k)}(x^{(k)}) - m_f^{(k)}(x^{(k+1)}) \ge c_1 \xi^{(k)} \min \{\frac{\xi^{(k)}}{1 + \|\nabla^2 m_f^{(k)}(x^{(k)})\|}, \Delta_k, 1\}$$
%         \item This can be satisfied using the Generalized Cauchy Point
%         \item The accuracy condition requires: $$\|\nabla m_f^{(k)}(x^{(k)}) - \nabla f (x^{(k)})\| \le c_2 \Delta_k$$
%         \item To satisfy this, we require the ellipsoid we find to have bounded condition number
%     \end{itemize}
% \end{frame}


\begin{frame}{Criticality Measure}
	\begin{itemize}
		\item We had to modify the convergence analysis in a few significant ways
		\item One of these was within the criticality measure
		\item $\xi(x)$ measures how close to being ``critical" a point $x$ is
		\item If $x$ satisfies first order optimality conditions, $\xi(x) = 0$
		\item $\xi(x)$ is large if there it is possible to move far along a feasible descent direction
	\end{itemize}
	\begin{center}
		\includegraphics[width=150px]{images/criticality.png}
	\end{center}
\end{frame}



\begin{frame}{Criticality Measure}
	\begin{itemize}
		\item The algorithm's criticality measure used the projection operator onto the true constraints
		\item We only use the projection onto the linearization of the true constraints
	\end{itemize}
	\begin{center}
		\includegraphics[width=150px]{images/criticality_measure.png}
	\end{center}
\end{frame}

\begin{frame}{Shrinking Trust Regions}

\begin{itemize}
    \item Another difficulty is that the original algorithm does not decrease the trust region radius when accepting the trial point
    \includegraphics[width=300px]{images/decrease_required.png}
\end{itemize}
\end{frame}



\begin{frame}{Feasible set}
\begin{itemize}
	\item Given a single feasible point, in general, it can be difficult to find even a second feasible point.
	\item This means that we must assume a feasible starting set.
	\begin{center}
		\includegraphics[width=300px]{images/only_one_feasible_point.png}
	\end{center}
\end{itemize}
\end{frame}




\begin{frame}{Feasible Ellipsoid}
	\begin{itemize}
		\item Also, the paper does not provide a mechanism for satisfying the accuracy condition
		\[\|\nabla m_f(x) - \nabla f(x)\| \le \epsilon_g \dk \]
		\item This usually not an issue, as any point can be added to the sample set to replace ill-poised points
		\begin{center}
			\includegraphics[width=120px]{images/impossible_poised.png}
		\end{center}
		\item We had to show that we can still find a ``large" enough region to choose sample points
	\end{itemize}
\end{frame}


\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/explanation_1.png}
	\end{center}
\end{frame}


\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/explanation_2.png}
	\end{center}
\end{frame}


\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/explanation_3.png}
	\end{center}
\end{frame}


\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/completed_1.png}
	\end{center}
\end{frame}


\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/completed_2.png}
	\end{center}
\end{frame}

\begin{frame}{Feasible Ellipsoid}
	\begin{center}
		\includegraphics[width=300px]{images/feasible_direction.png}
	\end{center}
\end{frame}


\begin{frame}{Feasible Sphere}

\begin{itemize}
	\item Ideally, we would not have to choose the ellipsoid within such a small cone.
	\item It would be better to choose the ellipsoid that maximizes the volume within all constraints.
	\item However, we are still formulating the optimization problem for this.
	\item A simpler problem is to choose the center of the maximum volume sphere.
\end{itemize}
\end{frame}

\begin{frame}{Feasible Sphere}
\begin{itemize}
	\item We have formulated a program to find this sphere
		\begin{align*}
\max_{r \ge 0, c, t^j}	& r & \\
					&  e_1^T t^j - \left\|t^j - e_1^T t^j\right\| \ge r			& \quad 1 \le j \le m \\
		\end{align*}
	\item This is a non-convex problem
\end{itemize}
\begin{center}
\includegraphics[width=150px]{images/feasible_sphere.png}
\end{center}
\end{frame}


\begin{frame}{Feasible Derivative Free Trust regions}
    \begin{itemize}
        \item We define two trust regions
        \item The inner trust region:
            \begin{itemize}
                \item Has an ellipsoidal shape
                \item Is assumed to be feasible, with respect to true constraints
                \item Is used for constructing sample points
            \end{itemize}
        \item The outer trust region:
            \begin{itemize}
                \item Is an $L_{\infty}$ ball
                \item Can contain infeasible points, even for the linearization of the constraints
                \item Is used as the search space for computing the next iterate
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}{Example trust region}
\begin{center}
    \includegraphics[width=300px]{images/trust_regions.png}
\end{center}
% \includegraphics[width=300px]{images/ellipse_at_current_iterate.png}
\end{frame}



%         \item Use the previous iterations
\begin{frame}{The Algorithm}
    \begin{itemize}
        \item Begin with a feasible set of points
        \item Construct the model functions by evaluating sample points
            \begin{itemize}
                \item Use the previous iteration's models to find a feasible inner trust region
                \item If a point in this ellipsoid is found to be infeasible, decrease the radius
            \end{itemize}
        \item Solve the trust region subproblem
            \begin{itemize}
                \item Add linear cuts to remove infeasible points
            \end{itemize}
        \item Test for sufficient reduction
    \end{itemize}
\end{frame}



\begin{frame}{Results}
	\begin{itemize}
		\item coming soon
		\item depends on good heuristic for ellipsoid
	\end{itemize}
\end{frame}


\begin{frame}{Contributions}
	\begin{itemize}
		\item Showing that the projection onto the linear approximation is bounded
		\item Finding the feasible ellipsoid
		\item Solving the trust region subproblem with linear cuts
	\end{itemize}
\end{frame}



\section{Future Work}


\frame {
\frametitle{Extensions to the Algorithm}
\begin{itemize}
    \item The feasible ellipsoid calculation is much more conservative than needed.
    \item We are defining the optimization problem of computing the maximum volume ellipse
    \item There may be simple heuristics that find better ellipsoids
    \item There are different strategies for dealing with infeasible trial points
\end{itemize}

}


\frame {
    \frametitle{Questions?}
}

\begin{frame}[allowframebreaks]
    \frametitle{References}
    \bibliographystyle{apalike}
    \bibliography{presentation}
\end{frame}

% 
% \frame {
% \frametitle{Adding Linear Cuts to Feasible Region}
% \begin{itemize}
%     \item For efficiency, we do not limit the trial point to the inner trust region
%     \item It is then possible that the trial point found by the trust region subproblem is infeasible
%     \item If so, we add a constraint that removes this infeasible sample point
%     \item The program is:
% %         \begin{itemize}
% %             \item Start with a polynomial $p$, a tolerance $\epsilon \in (0, 1)$, an set of infeasible points $u_{\text{infe}}^{(j)}$, and a set of feasible points $u_{\text{fe}}^{(i)}$.
%             \item Compute
%             \begin{displaymath}
%                 \begin{array}{ccccc}
%     \min_{s, v^{(j)}, b^{(j)}}   & p(s)            &       &                            \\
%                                 & \left(u_{\text{fe}}^{(i)}\right)^T v^{(j)}     & \le   & b^{(j)}                     \\
%                                 & \left(u_{\text{infe}}^{(j)}\right)^T v^{(i)}      & \ge   & b^{(j)} + \epsilon \Delta_k       \\
%                                 & \|v^{(i)}\|^2   & =     & 1                           \\
%                                 & s          & \in   & B_{\infty}(x^{(k)}, \Delta_k)    \\
%                 \end{array}
%             \end{displaymath}
% %         \end{itemize}
% \end{itemize}
% }
% 
% 
% %                                 & I[i]^T v^{(i)}      & \ge   & \epsilon                \\
% 
% 
% \begin{frame}{Visualizing Linear Cuts}
% \begin{center}
%     \includegraphics[width=300px]{images/cut_infeasible_points.png}
% \end{center}
% % \includegraphics[width=300px]{images/ellipse_at_current_iterate.png}
% \end{frame}


% 
% 
% \frame{
% \frametitle{Using Fewer Sample Points}
%     \begin{itemize}
%         \item With general convex constraints, sufficiently poised sets within the feasible region may not exist.
%         \item To illustrate, consider finding a fully quadratic model in 2-dimensions.
%         \item As the constraints become more thin,
%             \begin{itemize}
%                 \item The model becomes less poised
%                 \item There is less need for requiring the model to be quadratic in $y$.
%             \end{itemize}
%         \item We would prefer to use a subset of all quadratics, with fewer points to approximate them.
%     \end{itemize}
% }
% 
% \frame {
% \frametitle{2D Illustration}
%     \includegraphics[width=150px]{images/2_2_4_68.png}
%     \includegraphics[width=150px]{images/2_3_5_1.png}
% }
% 
% \frame {
% \frametitle{Full pivoting}
%     \begin{itemize}
%         \item One approach is to use full pivoting within the LU factorization used compute the Lagrange polynomials.
% 
%         \item When there are no more pivots greater than a threshold, the LU factorization terminates, and only uses the points and polynomials already computed after zeroing the remaining entries.
%         
%         \item This method will provide the next point to use as well as the next polynomial to include.
%         
%     \end{itemize}
% }
% 


% Circle





% \begin{frame}{Orders of models}
% 	\begin{itemize}
% 		\item Our algorithm assumes linear models for constraints, and a quadratic model for the objective
% 		\item This is not required, but does introduce the question of how to choose points poised for different orders of models
% 		\item We could select separate points for each different order
% 		\item We could create points for for a quadratic model, and only add points for a linear model if required
% 		\item We could choose points that are best for both models
% 		\item We currently use quadratic models for both
% 	\end{itemize}
% \end{frame}


\end{document}
